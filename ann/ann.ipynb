{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 6735 ANN - Intro**\n",
    "\n",
    "This is the notebook for the development of an artificial neural network from scratch for CS 6735. The purpose of this network is to perform a binary classification between malignant and benign tumours within a dataset of medical data about breast cancer.\n",
    "\n",
    "This ANN is to be developed from scratch, and must include:\n",
    "\n",
    "* a typical ANN structure (input, some number of hidden, output)\n",
    "* back propagation as the method of updating weights\n",
    "\n",
    "This code was developed by Matthew Tidd.\n",
    "\n",
    "**DATE CREATED: 13/11/2024**\n",
    "\n",
    "**DATE MODIFIED: 14/11/2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Packages**\n",
    "\n",
    "Must first import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the Dataset**\n",
    "\n",
    "Need to load the dataset. This dataset can be found on the UC Irvine Machine Learning Repository, and was donated from the University of Wisconsin-Madison in 1995. It is considered a multivariate dataset, and consists of several features pertaining to breast mass that were computed from digitized images of fine needle aspirate. These features describe the characteristics of the cell nuclei present in the image. \n",
    "\n",
    "There are 10 real-valued features that are of importance for this dataset:\n",
    "\n",
    "* radius\n",
    "* texture\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness\n",
    "* compactness\n",
    "* concavity\n",
    "* concave points\n",
    "* symmetry\n",
    "* fractal dimension\n",
    "\n",
    "For each of these real-valued features there are 3 versions, representing the mean, standard deviation, and worst case value. \n",
    "\n",
    "In total there are 30 features and 569 instances. The target variable in this dataset is a diagnosis, either malignant or benign. This dataset can be found at:\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the dataset is from the UCI ML repo, we can use their functions to fetch from their website:\n",
    "\n",
    "breast_data = fetch_ucirepo(id = 17)\n",
    "\n",
    "# can now access the data:\n",
    "\n",
    "breast_x = breast_data.data.features\n",
    "breast_y = breast_data.data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Dataset Properties**:\n",
    "\n",
    "Going to examine the properties of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 569 examples in the dataset\n",
      "there are 30 distinct features to train on\n",
      "the available diagnoses are: ['M' 'B']\n",
      "the available features are: \n",
      "\n",
      "radius1\n",
      "texture1\n",
      "perimeter1\n",
      "area1\n",
      "smoothness1\n",
      "compactness1\n",
      "concavity1\n",
      "concave_points1\n",
      "symmetry1\n",
      "fractal_dimension1\n",
      "radius2\n",
      "texture2\n",
      "perimeter2\n",
      "area2\n",
      "smoothness2\n",
      "compactness2\n",
      "concavity2\n",
      "concave_points2\n",
      "symmetry2\n",
      "fractal_dimension2\n",
      "radius3\n",
      "texture3\n",
      "perimeter3\n",
      "area3\n",
      "smoothness3\n",
      "compactness3\n",
      "concavity3\n",
      "concave_points3\n",
      "symmetry3\n",
      "fractal_dimension3\n"
     ]
    }
   ],
   "source": [
    "# get the total number of instances:\n",
    "print(f\"there are {breast_x.shape[0]} examples in the dataset\")\n",
    "\n",
    "# get number of features:\n",
    "print(f\"there are {breast_x.shape[1]} distinct features to train on\")\n",
    "\n",
    "# get the number of unique target variables:\n",
    "print(f\"the available diagnoses are: {breast_y['Diagnosis'].unique()}\")\n",
    "\n",
    "# get the names of the features:\n",
    "print(f\"the available features are: \\n\")\n",
    "for col in breast_x.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Pre-Processing**:\n",
    "\n",
    "Going to pre-process the dataset by performing the following:\n",
    "\n",
    "* remove the null values\n",
    "* scale the feature values to within a common range\n",
    "* encode the categorical labels into binary 0 or 1\n",
    "* split into training, validation, and testing data\n",
    "\n",
    "This is done to help realize the best model performance given the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no null values detected in features\n",
      "no null values detected in labels\n"
     ]
    }
   ],
   "source": [
    "# check for null values in the features:\n",
    "\n",
    "null = breast_x.isnull().values.any()\n",
    "if null == True:\n",
    "    breast_x = breast_x.dropna()\n",
    "    print(f'null values removed from features')\n",
    "else:\n",
    "    print(f'no null values detected in features')\n",
    "\n",
    "# check for null values in the labels:\n",
    "\n",
    "null = breast_y.isnull().values.any()\n",
    "if null == True:\n",
    "    breast_y = breast_y.dropna()\n",
    "    print(f'null values removed from labels')\n",
    "else:\n",
    "    print(f'no null values detected in labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize values to improve ANN performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features scaled\n"
     ]
    }
   ],
   "source": [
    "# standardize values:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_scaled = pd.DataFrame(scaler.fit_transform(breast_x))\n",
    "print('features scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to encode the categorical variables into binary values to be used in the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels encoded: M = 1, B = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 894,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode benign to 0, and malignant to 1\n",
    "\n",
    "breast_y =  pd.DataFrame(breast_y['Diagnosis'].map(lambda row: 1 if row == 'M' else 0))\n",
    "print('labels encoded: M = 1, B = 0')\n",
    "breast_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally need to create train/validation/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has form: (398, 30), labels are: (398, 1)\n",
      "validation data has form: (85, 30), labels are: (85, 1)\n",
      "test data has form: (86, 30), labels are: (86, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition data -> want 80% train, 10% validation, 10% testing\n",
    "\n",
    "x_train, dummy_x, y_train, dummy_y = train_test_split(x_scaled, breast_y, train_size = 0.7, test_size = 0.3)\n",
    "x_val, x_test, y_val, y_test = train_test_split(dummy_x, dummy_y, train_size = 0.5, test_size = 0.5)\n",
    "\n",
    "print(f\"training data has form: {x_train.shape}, labels are: {y_train.shape}\")\n",
    "print(f\"validation data has form: {x_val.shape}, labels are: {y_val.shape}\")\n",
    "print(f\"test data has form: {x_test.shape}, labels are: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANN Function Definition**\n",
    "\n",
    "Going to be using a class-based approach to defining the ANN, which will allow for instantiating, training, and querying of the network. Need to define the class itself as well as functions that will be used often, like the \n",
    "sigmoid function, relu, and their derivatives:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define useful functions:\n",
    "\n",
    "# logistic sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# rectified linear unit (basically a straight line)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# derivative function of relu (accepts np.arrays)\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# binary cross entropy -> for binary classification between 0 & 1\n",
    "def binary_crossentropy_loss(target, output):\n",
    "    output = np.clip(output, 1e-10, 1 - 1e-10)                              # prevent log 0\n",
    "    loss = - (target * np.log(output) + (1 - target) * np.log(1 - output))  # BCE formula\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network class:\n",
    "\n",
    "class NeuralNetwork:\n",
    "    # constructor:\n",
    "    def __init__(self, input_size, hidden_size, output_size, load):\n",
    "        # assign function inputs as instance variables:\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # need to randomly initialize weights and biases for the layers:\n",
    "\n",
    "        if load == True:\n",
    "            # implement loading of initial weights here!\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # from input to hidden:\n",
    "            self.w1 = np.random.randn(hidden_size, input_size)\n",
    "            self.b1 = np.random.randn(hidden_size, 1)\n",
    "\n",
    "            # from hidden to output:\n",
    "            self.w2 = np.random.randn(output_size, hidden_size)\n",
    "            self.b2 = np.random.randn(output_size, 1)\n",
    "\n",
    "            # save the initial weights:\n",
    "\n",
    "            self.initial = [self.w1, self.b1, self.w2, self.b2]\n",
    "\n",
    "    # feedforward function:\n",
    "    def forward_pass(self, x):\n",
    "        # make sure x is a column vector:\n",
    "        x = x.reshape((self.input_size, 1))\n",
    "\n",
    "        # from input to hidden:\n",
    "        self.net1 = np.dot(self.w1, x) + self.b1    # calculate net1\n",
    "        self.h1 = relu(self.net1)                   # calculate output of hidden layer\n",
    "\n",
    "        # from hidden to output:\n",
    "        self.net2 = np.dot(self.w2, self.h1) + self.b2  # calculate net2\n",
    "        self.o = sigmoid(self.net2)                     # calculate output of network\n",
    "\n",
    "        return self.o       # return value to user\n",
    "    \n",
    "    # backpropagation:\n",
    "    def backward_pass(self, x, y, learning_rate):\n",
    "        # make sure x is a column vector:\n",
    "        x = x.reshape((self.input_size, 1))\n",
    "\n",
    "        # get o - t:\n",
    "        o_error = self.o - y\n",
    "\n",
    "        # get gradients for weights and biases at the output layer:\n",
    "        de_dw2 = np.dot((o_error * sigmoid_derivative(self.net2)), self.h1.T)   # this is the partial derivative of E wrt. W2\n",
    "        de_db2 = o_error * sigmoid_derivative(self.net2)                        # this is the partial derivative of E wrt. B2\n",
    "\n",
    "        # get gradients for weights and biases at the input layer:\n",
    "\n",
    "        # this is an intermediary value because I was getting lost in the matrix dimensions\n",
    "        delta_1 = np.dot(self.w2.T, o_error * sigmoid_derivative(self.net2)) * relu_derivative(self.net1)  \n",
    "\n",
    "        de_dw1 = np.dot(delta_1, x.T)   # this is the partial derivative of E wrt. W1\n",
    "        de_db1 = delta_1                # this is the partial derivative of E wrt. B1\n",
    "\n",
    "        # update weights and biases:\n",
    "\n",
    "        self.w1 -= learning_rate  * de_dw1  # update w1\n",
    "        self.b1 -= learning_rate  * de_db1  # update b1\n",
    "        self.w2 -= learning_rate  * de_dw2  # update w2\n",
    "        self.b2 -= learning_rate  * de_db2  # update b2\n",
    "\n",
    "    # training:\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate):\n",
    "        # used in the plotting\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # initialize lists for appending train and val history to:\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        # for every epoch:\n",
    "        for epoch in range(epochs):\n",
    "            total_train_loss = 0    # reset train loss for new epoch\n",
    "            total_val_loss = 0      # reset val loss for new epoch\n",
    "\n",
    "            # training loop:\n",
    "            for i in range(x_train.shape[0]):\n",
    "                # extract example:\n",
    "                x = x_train.iloc[i].values\n",
    "\n",
    "                # get target for that example:\n",
    "                target = y_train.iloc[i].values\n",
    "\n",
    "                # compute forward pass:\n",
    "                output = self.forward_pass(x)\n",
    "\n",
    "                # backpropagate\n",
    "                self.backward_pass(x, target, learning_rate)\n",
    "\n",
    "                # get loss:\n",
    "                loss = binary_crossentropy_loss(target, output)\n",
    "                total_train_loss += loss    # add to total loss  \n",
    "\n",
    "            # get average BCE for train:\n",
    "            average_train_loss_per_epoch = total_train_loss / x_train.shape[0]\n",
    "            train_loss_history.append(average_train_loss_per_epoch)\n",
    "\n",
    "            # validation loop:\n",
    "            for i in range(x_val.shape[0]):\n",
    "                # extract example:\n",
    "                x = x_val.iloc[i].values\n",
    "\n",
    "                # get target for that example:\n",
    "                target = y_val.iloc[i].values\n",
    "\n",
    "                # compute forward pass:\n",
    "                output = self.forward_pass(x)\n",
    "\n",
    "                # get loss:\n",
    "                loss = binary_crossentropy_loss(target, output)\n",
    "                total_val_loss += loss    # add to total loss \n",
    "            \n",
    "            # get average BCE for val:\n",
    "            average_val_loss_per_epoch = total_val_loss / x_val.shape[0]\n",
    "            val_loss_history.append(average_val_loss_per_epoch)\n",
    "            \n",
    "            print(f\"epoch: {epoch + 1}/{epochs} | train loss was: {round(float(average_train_loss_per_epoch), 6)} | val loss was: {round(float(average_val_loss_per_epoch), 6)}\")  \n",
    "\n",
    "        return np.array(train_loss_history).reshape(-1, 1), np.array(val_loss_history).reshape(-1, 1)\n",
    "\n",
    "    # testing:\n",
    "    def test(self, x_test, y_test):\n",
    "        # need to get the output for each value of x_test, compare against y_test:\n",
    "        correct_predictions = 0\n",
    "\n",
    "        targets = []\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(x_test.shape[0]):\n",
    "            # extract example:\n",
    "            x = x_test.iloc[i].values\n",
    "\n",
    "            # extract target:\n",
    "            target = y_test.iloc[i].values\n",
    "\n",
    "            # compute forward pass:\n",
    "            output = self.forward_pass(x)\n",
    "\n",
    "            # get class value from sigmoid value:\n",
    "            prediction = 1 if output >= 0.5 else 0\n",
    "\n",
    "            print(f\"predicted: {prediction} | true: {target}\")\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            targets.append(target)\n",
    "\n",
    "            # if correct, add to successes\n",
    "            if prediction == target:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        # return accuracy \n",
    "        accuracy = round((correct_predictions / x_test.shape[0]) * 100, 3)\n",
    "        print(f\"accuracy of model is: {accuracy}\")\n",
    "\n",
    "        return predictions, targets, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training & Testing the Model:**\n",
    "\n",
    "Following the definition of the neural network class and functions, a network can be instantiated and trained, and following this, tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_size = 30, hidden_size = 15, output_size = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1500 | train loss was: 5.070786 | val loss was: 5.890921\n",
      "epoch: 2/1500 | train loss was: 4.996438 | val loss was: 5.852855\n",
      "epoch: 3/1500 | train loss was: 4.934012 | val loss was: 5.821726\n",
      "epoch: 4/1500 | train loss was: 4.882258 | val loss was: 5.791235\n",
      "epoch: 5/1500 | train loss was: 4.833881 | val loss was: 5.758593\n",
      "epoch: 6/1500 | train loss was: 4.784504 | val loss was: 5.720684\n",
      "epoch: 7/1500 | train loss was: 4.732358 | val loss was: 5.683335\n",
      "epoch: 8/1500 | train loss was: 4.682518 | val loss was: 5.651056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtidd2\\AppData\\Local\\Temp\\ipykernel_8680\\1912620221.py:122: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"epoch: {epoch + 1}/{epochs} | train loss was: {round(float(average_train_loss_per_epoch), 6)} | val loss was: {round(float(average_val_loss_per_epoch), 6)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/1500 | train loss was: 4.638615 | val loss was: 5.622681\n",
      "epoch: 10/1500 | train loss was: 4.5995 | val loss was: 5.596275\n",
      "epoch: 11/1500 | train loss was: 4.562563 | val loss was: 5.570469\n",
      "epoch: 12/1500 | train loss was: 4.525828 | val loss was: 5.54402\n",
      "epoch: 13/1500 | train loss was: 4.486998 | val loss was: 5.516164\n",
      "epoch: 14/1500 | train loss was: 4.443972 | val loss was: 5.485935\n",
      "epoch: 15/1500 | train loss was: 4.395013 | val loss was: 5.45249\n",
      "epoch: 16/1500 | train loss was: 4.340195 | val loss was: 5.402617\n",
      "epoch: 17/1500 | train loss was: 4.278847 | val loss was: 5.339916\n",
      "epoch: 18/1500 | train loss was: 4.213648 | val loss was: 5.270447\n",
      "epoch: 19/1500 | train loss was: 4.151013 | val loss was: 5.201957\n",
      "epoch: 20/1500 | train loss was: 4.092032 | val loss was: 5.130394\n",
      "epoch: 21/1500 | train loss was: 4.027365 | val loss was: 5.054116\n",
      "epoch: 22/1500 | train loss was: 3.954928 | val loss was: 4.968388\n",
      "epoch: 23/1500 | train loss was: 3.873313 | val loss was: 4.885541\n",
      "epoch: 24/1500 | train loss was: 3.785166 | val loss was: 4.790173\n",
      "epoch: 25/1500 | train loss was: 3.685055 | val loss was: 4.685947\n",
      "epoch: 26/1500 | train loss was: 3.58352 | val loss was: 4.590273\n",
      "epoch: 27/1500 | train loss was: 3.487239 | val loss was: 4.4903\n",
      "epoch: 28/1500 | train loss was: 3.382269 | val loss was: 4.383167\n",
      "epoch: 29/1500 | train loss was: 3.272208 | val loss was: 4.278602\n",
      "epoch: 30/1500 | train loss was: 3.168514 | val loss was: 4.181608\n",
      "epoch: 31/1500 | train loss was: 3.06129 | val loss was: 4.081779\n",
      "epoch: 32/1500 | train loss was: 2.932509 | val loss was: 3.942757\n",
      "epoch: 33/1500 | train loss was: 2.819117 | val loss was: 3.811737\n",
      "epoch: 34/1500 | train loss was: 2.740577 | val loss was: 3.695841\n",
      "epoch: 35/1500 | train loss was: 2.676957 | val loss was: 3.593184\n",
      "epoch: 36/1500 | train loss was: 2.614895 | val loss was: 3.485881\n",
      "epoch: 37/1500 | train loss was: 2.546139 | val loss was: 3.356758\n",
      "epoch: 38/1500 | train loss was: 2.454334 | val loss was: 3.181691\n",
      "epoch: 39/1500 | train loss was: 2.323386 | val loss was: 2.983442\n",
      "epoch: 40/1500 | train loss was: 2.202865 | val loss was: 2.820353\n",
      "epoch: 41/1500 | train loss was: 2.083975 | val loss was: 2.677703\n",
      "epoch: 42/1500 | train loss was: 1.988896 | val loss was: 2.535113\n",
      "epoch: 43/1500 | train loss was: 1.903791 | val loss was: 2.402883\n",
      "epoch: 44/1500 | train loss was: 1.825455 | val loss was: 2.289743\n",
      "epoch: 45/1500 | train loss was: 1.753066 | val loss was: 2.186058\n",
      "epoch: 46/1500 | train loss was: 1.680279 | val loss was: 2.103264\n",
      "epoch: 47/1500 | train loss was: 1.617809 | val loss was: 2.024865\n",
      "epoch: 48/1500 | train loss was: 1.564552 | val loss was: 1.955212\n",
      "epoch: 49/1500 | train loss was: 1.515244 | val loss was: 1.892641\n",
      "epoch: 50/1500 | train loss was: 1.469324 | val loss was: 1.832872\n",
      "epoch: 51/1500 | train loss was: 1.424286 | val loss was: 1.774489\n",
      "epoch: 52/1500 | train loss was: 1.379682 | val loss was: 1.717364\n",
      "epoch: 53/1500 | train loss was: 1.335233 | val loss was: 1.659079\n",
      "epoch: 54/1500 | train loss was: 1.289791 | val loss was: 1.601958\n",
      "epoch: 55/1500 | train loss was: 1.245293 | val loss was: 1.541613\n",
      "epoch: 56/1500 | train loss was: 1.206281 | val loss was: 1.495073\n",
      "epoch: 57/1500 | train loss was: 1.176796 | val loss was: 1.456504\n",
      "epoch: 58/1500 | train loss was: 1.149815 | val loss was: 1.416819\n",
      "epoch: 59/1500 | train loss was: 1.120479 | val loss was: 1.373909\n",
      "epoch: 60/1500 | train loss was: 1.089194 | val loss was: 1.332442\n",
      "epoch: 61/1500 | train loss was: 1.063833 | val loss was: 1.300315\n",
      "epoch: 62/1500 | train loss was: 1.045945 | val loss was: 1.275961\n",
      "epoch: 63/1500 | train loss was: 1.031272 | val loss was: 1.252743\n",
      "epoch: 64/1500 | train loss was: 1.017114 | val loss was: 1.228883\n",
      "epoch: 65/1500 | train loss was: 1.002945 | val loss was: 1.204356\n",
      "epoch: 66/1500 | train loss was: 0.988918 | val loss was: 1.179865\n",
      "epoch: 67/1500 | train loss was: 0.975355 | val loss was: 1.156263\n",
      "epoch: 68/1500 | train loss was: 0.962522 | val loss was: 1.134222\n",
      "epoch: 69/1500 | train loss was: 0.950561 | val loss was: 1.114141\n",
      "epoch: 70/1500 | train loss was: 0.93949 | val loss was: 1.096207\n",
      "epoch: 71/1500 | train loss was: 0.929262 | val loss was: 1.080455\n",
      "epoch: 72/1500 | train loss was: 0.919855 | val loss was: 1.066738\n",
      "epoch: 73/1500 | train loss was: 0.911187 | val loss was: 1.05489\n",
      "epoch: 74/1500 | train loss was: 0.90317 | val loss was: 1.044609\n",
      "epoch: 75/1500 | train loss was: 0.895725 | val loss was: 1.035562\n",
      "epoch: 76/1500 | train loss was: 0.888766 | val loss was: 1.027447\n",
      "epoch: 77/1500 | train loss was: 0.882204 | val loss was: 1.0201\n",
      "epoch: 78/1500 | train loss was: 0.875952 | val loss was: 1.013348\n",
      "epoch: 79/1500 | train loss was: 0.869932 | val loss was: 1.007051\n",
      "epoch: 80/1500 | train loss was: 0.864073 | val loss was: 1.001104\n",
      "epoch: 81/1500 | train loss was: 0.858309 | val loss was: 0.995418\n",
      "epoch: 82/1500 | train loss was: 0.852581 | val loss was: 0.989921\n",
      "epoch: 83/1500 | train loss was: 0.84683 | val loss was: 0.984551\n",
      "epoch: 84/1500 | train loss was: 0.840998 | val loss was: 0.979252\n",
      "epoch: 85/1500 | train loss was: 0.835029 | val loss was: 0.973972\n",
      "epoch: 86/1500 | train loss was: 0.82885 | val loss was: 0.968654\n",
      "epoch: 87/1500 | train loss was: 0.822377 | val loss was: 0.963236\n",
      "epoch: 88/1500 | train loss was: 0.815323 | val loss was: 0.957649\n",
      "epoch: 89/1500 | train loss was: 0.807594 | val loss was: 0.951808\n",
      "epoch: 90/1500 | train loss was: 0.799129 | val loss was: 0.945603\n",
      "epoch: 91/1500 | train loss was: 0.789685 | val loss was: 0.938894\n",
      "epoch: 92/1500 | train loss was: 0.778932 | val loss was: 0.931503\n",
      "epoch: 93/1500 | train loss was: 0.766461 | val loss was: 0.923239\n",
      "epoch: 94/1500 | train loss was: 0.751896 | val loss was: 0.914031\n",
      "epoch: 95/1500 | train loss was: 0.73532 | val loss was: 0.904208\n",
      "epoch: 96/1500 | train loss was: 0.717892 | val loss was: 0.894567\n",
      "epoch: 97/1500 | train loss was: 0.701523 | val loss was: 0.885792\n",
      "epoch: 98/1500 | train loss was: 0.686607 | val loss was: 0.87808\n",
      "epoch: 99/1500 | train loss was: 0.673506 | val loss was: 0.871303\n",
      "epoch: 100/1500 | train loss was: 0.662266 | val loss was: 0.865144\n",
      "epoch: 101/1500 | train loss was: 0.652379 | val loss was: 0.856181\n",
      "epoch: 102/1500 | train loss was: 0.642776 | val loss was: 0.847656\n",
      "epoch: 103/1500 | train loss was: 0.632155 | val loss was: 0.83931\n",
      "epoch: 104/1500 | train loss was: 0.620127 | val loss was: 0.830951\n",
      "epoch: 105/1500 | train loss was: 0.608182 | val loss was: 0.822472\n",
      "epoch: 106/1500 | train loss was: 0.596204 | val loss was: 0.813878\n",
      "epoch: 107/1500 | train loss was: 0.58427 | val loss was: 0.80531\n",
      "epoch: 108/1500 | train loss was: 0.57265 | val loss was: 0.79702\n",
      "epoch: 109/1500 | train loss was: 0.561724 | val loss was: 0.789274\n",
      "epoch: 110/1500 | train loss was: 0.551809 | val loss was: 0.782243\n",
      "epoch: 111/1500 | train loss was: 0.54304 | val loss was: 0.775966\n",
      "epoch: 112/1500 | train loss was: 0.535369 | val loss was: 0.77038\n",
      "epoch: 113/1500 | train loss was: 0.528648 | val loss was: 0.76538\n",
      "epoch: 114/1500 | train loss was: 0.522705 | val loss was: 0.760855\n",
      "epoch: 115/1500 | train loss was: 0.517377 | val loss was: 0.756702\n",
      "epoch: 116/1500 | train loss was: 0.512534 | val loss was: 0.752849\n",
      "epoch: 117/1500 | train loss was: 0.508077 | val loss was: 0.749233\n",
      "epoch: 118/1500 | train loss was: 0.503926 | val loss was: 0.745802\n",
      "epoch: 119/1500 | train loss was: 0.500015 | val loss was: 0.742516\n",
      "epoch: 120/1500 | train loss was: 0.496273 | val loss was: 0.739335\n",
      "epoch: 121/1500 | train loss was: 0.492629 | val loss was: 0.736234\n",
      "epoch: 122/1500 | train loss was: 0.489079 | val loss was: 0.733186\n",
      "epoch: 123/1500 | train loss was: 0.485582 | val loss was: 0.730163\n",
      "epoch: 124/1500 | train loss was: 0.482096 | val loss was: 0.727135\n",
      "epoch: 125/1500 | train loss was: 0.478569 | val loss was: 0.724066\n",
      "epoch: 126/1500 | train loss was: 0.474935 | val loss was: 0.720883\n",
      "epoch: 127/1500 | train loss was: 0.471093 | val loss was: 0.717515\n",
      "epoch: 128/1500 | train loss was: 0.466878 | val loss was: 0.713846\n",
      "epoch: 129/1500 | train loss was: 0.461976 | val loss was: 0.70962\n",
      "epoch: 130/1500 | train loss was: 0.45568 | val loss was: 0.704231\n",
      "epoch: 131/1500 | train loss was: 0.446144 | val loss was: 0.696046\n",
      "epoch: 132/1500 | train loss was: 0.431993 | val loss was: 0.684532\n",
      "epoch: 133/1500 | train loss was: 0.425441 | val loss was: 0.681152\n",
      "epoch: 134/1500 | train loss was: 0.422669 | val loss was: 0.678032\n",
      "epoch: 135/1500 | train loss was: 0.420054 | val loss was: 0.674973\n",
      "epoch: 136/1500 | train loss was: 0.417472 | val loss was: 0.671907\n",
      "epoch: 137/1500 | train loss was: 0.414867 | val loss was: 0.668806\n",
      "epoch: 138/1500 | train loss was: 0.41223 | val loss was: 0.66564\n",
      "epoch: 139/1500 | train loss was: 0.409557 | val loss was: 0.66176\n",
      "epoch: 140/1500 | train loss was: 0.406854 | val loss was: 0.657854\n",
      "epoch: 141/1500 | train loss was: 0.404134 | val loss was: 0.653951\n",
      "epoch: 142/1500 | train loss was: 0.40141 | val loss was: 0.650082\n",
      "epoch: 143/1500 | train loss was: 0.398697 | val loss was: 0.646276\n",
      "epoch: 144/1500 | train loss was: 0.396007 | val loss was: 0.642553\n",
      "epoch: 145/1500 | train loss was: 0.393344 | val loss was: 0.638926\n",
      "epoch: 146/1500 | train loss was: 0.390708 | val loss was: 0.635397\n",
      "epoch: 147/1500 | train loss was: 0.388098 | val loss was: 0.631964\n",
      "epoch: 148/1500 | train loss was: 0.385505 | val loss was: 0.628619\n",
      "epoch: 149/1500 | train loss was: 0.382924 | val loss was: 0.625356\n",
      "epoch: 150/1500 | train loss was: 0.380347 | val loss was: 0.622164\n",
      "epoch: 151/1500 | train loss was: 0.377768 | val loss was: 0.619036\n",
      "epoch: 152/1500 | train loss was: 0.375179 | val loss was: 0.615965\n",
      "epoch: 153/1500 | train loss was: 0.372575 | val loss was: 0.612943\n",
      "epoch: 154/1500 | train loss was: 0.36995 | val loss was: 0.609964\n",
      "epoch: 155/1500 | train loss was: 0.367299 | val loss was: 0.607022\n",
      "epoch: 156/1500 | train loss was: 0.364616 | val loss was: 0.604113\n",
      "epoch: 157/1500 | train loss was: 0.361896 | val loss was: 0.601231\n",
      "epoch: 158/1500 | train loss was: 0.359134 | val loss was: 0.598372\n",
      "epoch: 159/1500 | train loss was: 0.356322 | val loss was: 0.595532\n",
      "epoch: 160/1500 | train loss was: 0.353457 | val loss was: 0.592707\n",
      "epoch: 161/1500 | train loss was: 0.35053 | val loss was: 0.589891\n",
      "epoch: 162/1500 | train loss was: 0.347535 | val loss was: 0.587081\n",
      "epoch: 163/1500 | train loss was: 0.344465 | val loss was: 0.584271\n",
      "epoch: 164/1500 | train loss was: 0.341311 | val loss was: 0.581458\n",
      "epoch: 165/1500 | train loss was: 0.338065 | val loss was: 0.578635\n",
      "epoch: 166/1500 | train loss was: 0.334717 | val loss was: 0.575796\n",
      "epoch: 167/1500 | train loss was: 0.331255 | val loss was: 0.572935\n",
      "epoch: 168/1500 | train loss was: 0.327669 | val loss was: 0.570044\n",
      "epoch: 169/1500 | train loss was: 0.323947 | val loss was: 0.567116\n",
      "epoch: 170/1500 | train loss was: 0.320075 | val loss was: 0.564142\n",
      "epoch: 171/1500 | train loss was: 0.316042 | val loss was: 0.561112\n",
      "epoch: 172/1500 | train loss was: 0.311837 | val loss was: 0.558018\n",
      "epoch: 173/1500 | train loss was: 0.307451 | val loss was: 0.554852\n",
      "epoch: 174/1500 | train loss was: 0.302885 | val loss was: 0.551608\n",
      "epoch: 175/1500 | train loss was: 0.298126 | val loss was: 0.548344\n",
      "epoch: 176/1500 | train loss was: 0.293181 | val loss was: 0.545006\n",
      "epoch: 177/1500 | train loss was: 0.288133 | val loss was: 0.541611\n",
      "epoch: 178/1500 | train loss was: 0.283048 | val loss was: 0.538189\n",
      "epoch: 179/1500 | train loss was: 0.278003 | val loss was: 0.534774\n",
      "epoch: 180/1500 | train loss was: 0.273084 | val loss was: 0.531405\n",
      "epoch: 181/1500 | train loss was: 0.268363 | val loss was: 0.528112\n",
      "epoch: 182/1500 | train loss was: 0.263895 | val loss was: 0.524916\n",
      "epoch: 183/1500 | train loss was: 0.25971 | val loss was: 0.521825\n",
      "epoch: 184/1500 | train loss was: 0.255758 | val loss was: 0.518756\n",
      "epoch: 185/1500 | train loss was: 0.252075 | val loss was: 0.515784\n",
      "epoch: 186/1500 | train loss was: 0.248635 | val loss was: 0.512898\n",
      "epoch: 187/1500 | train loss was: 0.245418 | val loss was: 0.51008\n",
      "epoch: 188/1500 | train loss was: 0.242395 | val loss was: 0.507318\n",
      "epoch: 189/1500 | train loss was: 0.239536 | val loss was: 0.504596\n",
      "epoch: 190/1500 | train loss was: 0.236812 | val loss was: 0.501901\n",
      "epoch: 191/1500 | train loss was: 0.234197 | val loss was: 0.499219\n",
      "epoch: 192/1500 | train loss was: 0.231666 | val loss was: 0.496538\n",
      "epoch: 193/1500 | train loss was: 0.229195 | val loss was: 0.493845\n",
      "epoch: 194/1500 | train loss was: 0.226763 | val loss was: 0.491125\n",
      "epoch: 195/1500 | train loss was: 0.22434 | val loss was: 0.488351\n",
      "epoch: 196/1500 | train loss was: 0.221909 | val loss was: 0.485519\n",
      "epoch: 197/1500 | train loss was: 0.219449 | val loss was: 0.482608\n",
      "epoch: 198/1500 | train loss was: 0.216932 | val loss was: 0.479593\n",
      "epoch: 199/1500 | train loss was: 0.21433 | val loss was: 0.476445\n",
      "epoch: 200/1500 | train loss was: 0.211606 | val loss was: 0.473126\n",
      "epoch: 201/1500 | train loss was: 0.208723 | val loss was: 0.469593\n",
      "epoch: 202/1500 | train loss was: 0.205643 | val loss was: 0.465801\n",
      "epoch: 203/1500 | train loss was: 0.202343 | val loss was: 0.46172\n",
      "epoch: 204/1500 | train loss was: 0.198847 | val loss was: 0.457369\n",
      "epoch: 205/1500 | train loss was: 0.195272 | val loss was: 0.452865\n",
      "epoch: 206/1500 | train loss was: 0.191817 | val loss was: 0.448433\n",
      "epoch: 207/1500 | train loss was: 0.188734 | val loss was: 0.444282\n",
      "epoch: 208/1500 | train loss was: 0.186124 | val loss was: 0.440482\n",
      "epoch: 209/1500 | train loss was: 0.183935 | val loss was: 0.436989\n",
      "epoch: 210/1500 | train loss was: 0.182063 | val loss was: 0.433731\n",
      "epoch: 211/1500 | train loss was: 0.180417 | val loss was: 0.430646\n",
      "epoch: 212/1500 | train loss was: 0.178931 | val loss was: 0.427691\n",
      "epoch: 213/1500 | train loss was: 0.177562 | val loss was: 0.424836\n",
      "epoch: 214/1500 | train loss was: 0.17628 | val loss was: 0.42206\n",
      "epoch: 215/1500 | train loss was: 0.175064 | val loss was: 0.41935\n",
      "epoch: 216/1500 | train loss was: 0.173902 | val loss was: 0.416697\n",
      "epoch: 217/1500 | train loss was: 0.172784 | val loss was: 0.414097\n",
      "epoch: 218/1500 | train loss was: 0.171703 | val loss was: 0.411546\n",
      "epoch: 219/1500 | train loss was: 0.170654 | val loss was: 0.409042\n",
      "epoch: 220/1500 | train loss was: 0.169627 | val loss was: 0.406587\n",
      "epoch: 221/1500 | train loss was: 0.168604 | val loss was: 0.40418\n",
      "epoch: 222/1500 | train loss was: 0.167608 | val loss was: 0.401824\n",
      "epoch: 223/1500 | train loss was: 0.166636 | val loss was: 0.39952\n",
      "epoch: 224/1500 | train loss was: 0.165686 | val loss was: 0.39727\n",
      "epoch: 225/1500 | train loss was: 0.164756 | val loss was: 0.395075\n",
      "epoch: 226/1500 | train loss was: 0.163844 | val loss was: 0.392938\n",
      "epoch: 227/1500 | train loss was: 0.162947 | val loss was: 0.390858\n",
      "epoch: 228/1500 | train loss was: 0.16206 | val loss was: 0.388837\n",
      "epoch: 229/1500 | train loss was: 0.16118 | val loss was: 0.386874\n",
      "epoch: 230/1500 | train loss was: 0.160301 | val loss was: 0.38497\n",
      "epoch: 231/1500 | train loss was: 0.159415 | val loss was: 0.383122\n",
      "epoch: 232/1500 | train loss was: 0.158519 | val loss was: 0.381306\n",
      "epoch: 233/1500 | train loss was: 0.157565 | val loss was: 0.379544\n",
      "epoch: 234/1500 | train loss was: 0.156571 | val loss was: 0.377834\n",
      "epoch: 235/1500 | train loss was: 0.155519 | val loss was: 0.376171\n",
      "epoch: 236/1500 | train loss was: 0.154386 | val loss was: 0.374552\n",
      "epoch: 237/1500 | train loss was: 0.153141 | val loss was: 0.372972\n",
      "epoch: 238/1500 | train loss was: 0.151746 | val loss was: 0.371429\n",
      "epoch: 239/1500 | train loss was: 0.150163 | val loss was: 0.369927\n",
      "epoch: 240/1500 | train loss was: 0.148374 | val loss was: 0.368489\n",
      "epoch: 241/1500 | train loss was: 0.146457 | val loss was: 0.367166\n",
      "epoch: 242/1500 | train loss was: 0.14467 | val loss was: 0.365998\n",
      "epoch: 243/1500 | train loss was: 0.143328 | val loss was: 0.364961\n",
      "epoch: 244/1500 | train loss was: 0.142458 | val loss was: 0.364007\n",
      "epoch: 245/1500 | train loss was: 0.141892 | val loss was: 0.363107\n",
      "epoch: 246/1500 | train loss was: 0.141501 | val loss was: 0.362245\n",
      "epoch: 247/1500 | train loss was: 0.141214 | val loss was: 0.361414\n",
      "epoch: 248/1500 | train loss was: 0.140993 | val loss was: 0.360609\n",
      "epoch: 249/1500 | train loss was: 0.140819 | val loss was: 0.359827\n",
      "epoch: 250/1500 | train loss was: 0.140676 | val loss was: 0.359065\n",
      "epoch: 251/1500 | train loss was: 0.140556 | val loss was: 0.358323\n",
      "epoch: 252/1500 | train loss was: 0.140455 | val loss was: 0.357598\n",
      "epoch: 253/1500 | train loss was: 0.140366 | val loss was: 0.35689\n",
      "epoch: 254/1500 | train loss was: 0.140289 | val loss was: 0.356198\n",
      "epoch: 255/1500 | train loss was: 0.140219 | val loss was: 0.355521\n",
      "epoch: 256/1500 | train loss was: 0.140156 | val loss was: 0.354859\n",
      "epoch: 257/1500 | train loss was: 0.140098 | val loss was: 0.35421\n",
      "epoch: 258/1500 | train loss was: 0.140045 | val loss was: 0.353575\n",
      "epoch: 259/1500 | train loss was: 0.139994 | val loss was: 0.352952\n",
      "epoch: 260/1500 | train loss was: 0.139947 | val loss was: 0.352342\n",
      "epoch: 261/1500 | train loss was: 0.139901 | val loss was: 0.351743\n",
      "epoch: 262/1500 | train loss was: 0.139858 | val loss was: 0.351155\n",
      "epoch: 263/1500 | train loss was: 0.139815 | val loss was: 0.350578\n",
      "epoch: 264/1500 | train loss was: 0.139774 | val loss was: 0.350012\n",
      "epoch: 265/1500 | train loss was: 0.139733 | val loss was: 0.349456\n",
      "epoch: 266/1500 | train loss was: 0.139693 | val loss was: 0.348909\n",
      "epoch: 267/1500 | train loss was: 0.139654 | val loss was: 0.348372\n",
      "epoch: 268/1500 | train loss was: 0.139614 | val loss was: 0.347844\n",
      "epoch: 269/1500 | train loss was: 0.139575 | val loss was: 0.347324\n",
      "epoch: 270/1500 | train loss was: 0.139536 | val loss was: 0.346813\n",
      "epoch: 271/1500 | train loss was: 0.139498 | val loss was: 0.34631\n",
      "epoch: 272/1500 | train loss was: 0.139459 | val loss was: 0.345815\n",
      "epoch: 273/1500 | train loss was: 0.13942 | val loss was: 0.345328\n",
      "epoch: 274/1500 | train loss was: 0.139381 | val loss was: 0.344848\n",
      "epoch: 275/1500 | train loss was: 0.139342 | val loss was: 0.344375\n",
      "epoch: 276/1500 | train loss was: 0.139303 | val loss was: 0.343909\n",
      "epoch: 277/1500 | train loss was: 0.139263 | val loss was: 0.34345\n",
      "epoch: 278/1500 | train loss was: 0.139224 | val loss was: 0.342997\n",
      "epoch: 279/1500 | train loss was: 0.139184 | val loss was: 0.342551\n",
      "epoch: 280/1500 | train loss was: 0.139144 | val loss was: 0.34211\n",
      "epoch: 281/1500 | train loss was: 0.139093 | val loss was: 0.341676\n",
      "epoch: 282/1500 | train loss was: 0.139041 | val loss was: 0.341247\n",
      "epoch: 283/1500 | train loss was: 0.13899 | val loss was: 0.340824\n",
      "epoch: 284/1500 | train loss was: 0.138939 | val loss was: 0.340407\n",
      "epoch: 285/1500 | train loss was: 0.138889 | val loss was: 0.339994\n",
      "epoch: 286/1500 | train loss was: 0.138838 | val loss was: 0.339587\n",
      "epoch: 287/1500 | train loss was: 0.138788 | val loss was: 0.339185\n",
      "epoch: 288/1500 | train loss was: 0.138738 | val loss was: 0.338787\n",
      "epoch: 289/1500 | train loss was: 0.138688 | val loss was: 0.338395\n",
      "epoch: 290/1500 | train loss was: 0.138638 | val loss was: 0.338007\n",
      "epoch: 291/1500 | train loss was: 0.138588 | val loss was: 0.337623\n",
      "epoch: 292/1500 | train loss was: 0.138539 | val loss was: 0.337244\n",
      "epoch: 293/1500 | train loss was: 0.13849 | val loss was: 0.336869\n",
      "epoch: 294/1500 | train loss was: 0.138441 | val loss was: 0.336498\n",
      "epoch: 295/1500 | train loss was: 0.138392 | val loss was: 0.336131\n",
      "epoch: 296/1500 | train loss was: 0.138343 | val loss was: 0.335769\n",
      "epoch: 297/1500 | train loss was: 0.138295 | val loss was: 0.33541\n",
      "epoch: 298/1500 | train loss was: 0.138247 | val loss was: 0.335054\n",
      "epoch: 299/1500 | train loss was: 0.138199 | val loss was: 0.334703\n",
      "epoch: 300/1500 | train loss was: 0.138151 | val loss was: 0.334355\n",
      "epoch: 301/1500 | train loss was: 0.138103 | val loss was: 0.33401\n",
      "epoch: 302/1500 | train loss was: 0.138055 | val loss was: 0.333669\n",
      "epoch: 303/1500 | train loss was: 0.138008 | val loss was: 0.333331\n",
      "epoch: 304/1500 | train loss was: 0.137961 | val loss was: 0.332997\n",
      "epoch: 305/1500 | train loss was: 0.137914 | val loss was: 0.332665\n",
      "epoch: 306/1500 | train loss was: 0.137867 | val loss was: 0.332337\n",
      "epoch: 307/1500 | train loss was: 0.137821 | val loss was: 0.332012\n",
      "epoch: 308/1500 | train loss was: 0.137774 | val loss was: 0.331689\n",
      "epoch: 309/1500 | train loss was: 0.137728 | val loss was: 0.33137\n",
      "epoch: 310/1500 | train loss was: 0.137682 | val loss was: 0.331053\n",
      "epoch: 311/1500 | train loss was: 0.137637 | val loss was: 0.33074\n",
      "epoch: 312/1500 | train loss was: 0.137591 | val loss was: 0.330429\n",
      "epoch: 313/1500 | train loss was: 0.137546 | val loss was: 0.33012\n",
      "epoch: 314/1500 | train loss was: 0.1375 | val loss was: 0.329815\n",
      "epoch: 315/1500 | train loss was: 0.137455 | val loss was: 0.329511\n",
      "epoch: 316/1500 | train loss was: 0.137411 | val loss was: 0.329211\n",
      "epoch: 317/1500 | train loss was: 0.137366 | val loss was: 0.328913\n",
      "epoch: 318/1500 | train loss was: 0.137322 | val loss was: 0.328617\n",
      "epoch: 319/1500 | train loss was: 0.137277 | val loss was: 0.328323\n",
      "epoch: 320/1500 | train loss was: 0.137233 | val loss was: 0.328032\n",
      "epoch: 321/1500 | train loss was: 0.137189 | val loss was: 0.327743\n",
      "epoch: 322/1500 | train loss was: 0.137146 | val loss was: 0.327457\n",
      "epoch: 323/1500 | train loss was: 0.137102 | val loss was: 0.327172\n",
      "epoch: 324/1500 | train loss was: 0.137059 | val loss was: 0.32689\n",
      "epoch: 325/1500 | train loss was: 0.137016 | val loss was: 0.32661\n",
      "epoch: 326/1500 | train loss was: 0.136973 | val loss was: 0.326332\n",
      "epoch: 327/1500 | train loss was: 0.13693 | val loss was: 0.326056\n",
      "epoch: 328/1500 | train loss was: 0.136887 | val loss was: 0.325782\n",
      "epoch: 329/1500 | train loss was: 0.136845 | val loss was: 0.32551\n",
      "epoch: 330/1500 | train loss was: 0.136803 | val loss was: 0.32524\n",
      "epoch: 331/1500 | train loss was: 0.136761 | val loss was: 0.324971\n",
      "epoch: 332/1500 | train loss was: 0.136719 | val loss was: 0.324705\n",
      "epoch: 333/1500 | train loss was: 0.136677 | val loss was: 0.324441\n",
      "epoch: 334/1500 | train loss was: 0.136635 | val loss was: 0.324178\n",
      "epoch: 335/1500 | train loss was: 0.136594 | val loss was: 0.323916\n",
      "epoch: 336/1500 | train loss was: 0.136552 | val loss was: 0.323657\n",
      "epoch: 337/1500 | train loss was: 0.136511 | val loss was: 0.323399\n",
      "epoch: 338/1500 | train loss was: 0.13647 | val loss was: 0.323143\n",
      "epoch: 339/1500 | train loss was: 0.136429 | val loss was: 0.322888\n",
      "epoch: 340/1500 | train loss was: 0.136389 | val loss was: 0.322635\n",
      "epoch: 341/1500 | train loss was: 0.136348 | val loss was: 0.322384\n",
      "epoch: 342/1500 | train loss was: 0.136308 | val loss was: 0.322134\n",
      "epoch: 343/1500 | train loss was: 0.136268 | val loss was: 0.321886\n",
      "epoch: 344/1500 | train loss was: 0.136228 | val loss was: 0.321639\n",
      "epoch: 345/1500 | train loss was: 0.136188 | val loss was: 0.321394\n",
      "epoch: 346/1500 | train loss was: 0.136148 | val loss was: 0.321151\n",
      "epoch: 347/1500 | train loss was: 0.136109 | val loss was: 0.320909\n",
      "epoch: 348/1500 | train loss was: 0.136069 | val loss was: 0.320668\n",
      "epoch: 349/1500 | train loss was: 0.13603 | val loss was: 0.320429\n",
      "epoch: 350/1500 | train loss was: 0.135991 | val loss was: 0.320191\n",
      "epoch: 351/1500 | train loss was: 0.135952 | val loss was: 0.319955\n",
      "epoch: 352/1500 | train loss was: 0.135913 | val loss was: 0.31972\n",
      "epoch: 353/1500 | train loss was: 0.135875 | val loss was: 0.319486\n",
      "epoch: 354/1500 | train loss was: 0.135836 | val loss was: 0.319254\n",
      "epoch: 355/1500 | train loss was: 0.135798 | val loss was: 0.319023\n",
      "epoch: 356/1500 | train loss was: 0.13576 | val loss was: 0.318793\n",
      "epoch: 357/1500 | train loss was: 0.135722 | val loss was: 0.318564\n",
      "epoch: 358/1500 | train loss was: 0.135684 | val loss was: 0.318337\n",
      "epoch: 359/1500 | train loss was: 0.135646 | val loss was: 0.318111\n",
      "epoch: 360/1500 | train loss was: 0.135609 | val loss was: 0.317887\n",
      "epoch: 361/1500 | train loss was: 0.135571 | val loss was: 0.317663\n",
      "epoch: 362/1500 | train loss was: 0.135534 | val loss was: 0.317441\n",
      "epoch: 363/1500 | train loss was: 0.135497 | val loss was: 0.31722\n",
      "epoch: 364/1500 | train loss was: 0.13546 | val loss was: 0.317\n",
      "epoch: 365/1500 | train loss was: 0.135423 | val loss was: 0.316781\n",
      "epoch: 366/1500 | train loss was: 0.135387 | val loss was: 0.316564\n",
      "epoch: 367/1500 | train loss was: 0.13535 | val loss was: 0.316347\n",
      "epoch: 368/1500 | train loss was: 0.135314 | val loss was: 0.316129\n",
      "epoch: 369/1500 | train loss was: 0.135278 | val loss was: 0.315911\n",
      "epoch: 370/1500 | train loss was: 0.135242 | val loss was: 0.315694\n",
      "epoch: 371/1500 | train loss was: 0.135206 | val loss was: 0.315478\n",
      "epoch: 372/1500 | train loss was: 0.13517 | val loss was: 0.315264\n",
      "epoch: 373/1500 | train loss was: 0.135134 | val loss was: 0.31505\n",
      "epoch: 374/1500 | train loss was: 0.135099 | val loss was: 0.314837\n",
      "epoch: 375/1500 | train loss was: 0.135064 | val loss was: 0.314626\n",
      "epoch: 376/1500 | train loss was: 0.135029 | val loss was: 0.314415\n",
      "epoch: 377/1500 | train loss was: 0.134994 | val loss was: 0.314205\n",
      "epoch: 378/1500 | train loss was: 0.134959 | val loss was: 0.313996\n",
      "epoch: 379/1500 | train loss was: 0.134924 | val loss was: 0.313788\n",
      "epoch: 380/1500 | train loss was: 0.13489 | val loss was: 0.313581\n",
      "epoch: 381/1500 | train loss was: 0.134855 | val loss was: 0.313375\n",
      "epoch: 382/1500 | train loss was: 0.134821 | val loss was: 0.313169\n",
      "epoch: 383/1500 | train loss was: 0.134787 | val loss was: 0.312965\n",
      "epoch: 384/1500 | train loss was: 0.134753 | val loss was: 0.312761\n",
      "epoch: 385/1500 | train loss was: 0.13472 | val loss was: 0.312559\n",
      "epoch: 386/1500 | train loss was: 0.134686 | val loss was: 0.312357\n",
      "epoch: 387/1500 | train loss was: 0.134653 | val loss was: 0.312156\n",
      "epoch: 388/1500 | train loss was: 0.134619 | val loss was: 0.311956\n",
      "epoch: 389/1500 | train loss was: 0.134586 | val loss was: 0.311757\n",
      "epoch: 390/1500 | train loss was: 0.134553 | val loss was: 0.311558\n",
      "epoch: 391/1500 | train loss was: 0.134519 | val loss was: 0.31136\n",
      "epoch: 392/1500 | train loss was: 0.134479 | val loss was: 0.311163\n",
      "epoch: 393/1500 | train loss was: 0.134439 | val loss was: 0.310967\n",
      "epoch: 394/1500 | train loss was: 0.134399 | val loss was: 0.310772\n",
      "epoch: 395/1500 | train loss was: 0.134359 | val loss was: 0.310577\n",
      "epoch: 396/1500 | train loss was: 0.134319 | val loss was: 0.310383\n",
      "epoch: 397/1500 | train loss was: 0.134279 | val loss was: 0.31019\n",
      "epoch: 398/1500 | train loss was: 0.13424 | val loss was: 0.309998\n",
      "epoch: 399/1500 | train loss was: 0.1342 | val loss was: 0.309806\n",
      "epoch: 400/1500 | train loss was: 0.134161 | val loss was: 0.309615\n",
      "epoch: 401/1500 | train loss was: 0.134121 | val loss was: 0.309425\n",
      "epoch: 402/1500 | train loss was: 0.134082 | val loss was: 0.309236\n",
      "epoch: 403/1500 | train loss was: 0.134043 | val loss was: 0.309047\n",
      "epoch: 404/1500 | train loss was: 0.134004 | val loss was: 0.308859\n",
      "epoch: 405/1500 | train loss was: 0.133965 | val loss was: 0.308671\n",
      "epoch: 406/1500 | train loss was: 0.133927 | val loss was: 0.308485\n",
      "epoch: 407/1500 | train loss was: 0.133888 | val loss was: 0.308299\n",
      "epoch: 408/1500 | train loss was: 0.13385 | val loss was: 0.308113\n",
      "epoch: 409/1500 | train loss was: 0.133811 | val loss was: 0.307929\n",
      "epoch: 410/1500 | train loss was: 0.133773 | val loss was: 0.307744\n",
      "epoch: 411/1500 | train loss was: 0.133735 | val loss was: 0.307561\n",
      "epoch: 412/1500 | train loss was: 0.133697 | val loss was: 0.307378\n",
      "epoch: 413/1500 | train loss was: 0.13366 | val loss was: 0.307196\n",
      "epoch: 414/1500 | train loss was: 0.133622 | val loss was: 0.307014\n",
      "epoch: 415/1500 | train loss was: 0.133585 | val loss was: 0.306833\n",
      "epoch: 416/1500 | train loss was: 0.133547 | val loss was: 0.306653\n",
      "epoch: 417/1500 | train loss was: 0.13351 | val loss was: 0.306473\n",
      "epoch: 418/1500 | train loss was: 0.133474 | val loss was: 0.306293\n",
      "epoch: 419/1500 | train loss was: 0.133437 | val loss was: 0.306115\n",
      "epoch: 420/1500 | train loss was: 0.1334 | val loss was: 0.305937\n",
      "epoch: 421/1500 | train loss was: 0.133364 | val loss was: 0.305759\n",
      "epoch: 422/1500 | train loss was: 0.133328 | val loss was: 0.305582\n",
      "epoch: 423/1500 | train loss was: 0.133292 | val loss was: 0.305405\n",
      "epoch: 424/1500 | train loss was: 0.133256 | val loss was: 0.30523\n",
      "epoch: 425/1500 | train loss was: 0.133221 | val loss was: 0.305054\n",
      "epoch: 426/1500 | train loss was: 0.133186 | val loss was: 0.304879\n",
      "epoch: 427/1500 | train loss was: 0.133151 | val loss was: 0.304705\n",
      "epoch: 428/1500 | train loss was: 0.133116 | val loss was: 0.304531\n",
      "epoch: 429/1500 | train loss was: 0.133081 | val loss was: 0.304357\n",
      "epoch: 430/1500 | train loss was: 0.133047 | val loss was: 0.304184\n",
      "epoch: 431/1500 | train loss was: 0.133012 | val loss was: 0.304012\n",
      "epoch: 432/1500 | train loss was: 0.132979 | val loss was: 0.30384\n",
      "epoch: 433/1500 | train loss was: 0.132945 | val loss was: 0.303668\n",
      "epoch: 434/1500 | train loss was: 0.132911 | val loss was: 0.303497\n",
      "epoch: 435/1500 | train loss was: 0.132878 | val loss was: 0.303327\n",
      "epoch: 436/1500 | train loss was: 0.132845 | val loss was: 0.303157\n",
      "epoch: 437/1500 | train loss was: 0.132812 | val loss was: 0.302987\n",
      "epoch: 438/1500 | train loss was: 0.13278 | val loss was: 0.302818\n",
      "epoch: 439/1500 | train loss was: 0.132748 | val loss was: 0.302649\n",
      "epoch: 440/1500 | train loss was: 0.132716 | val loss was: 0.30248\n",
      "epoch: 441/1500 | train loss was: 0.132684 | val loss was: 0.302312\n",
      "epoch: 442/1500 | train loss was: 0.132653 | val loss was: 0.302144\n",
      "epoch: 443/1500 | train loss was: 0.132621 | val loss was: 0.301977\n",
      "epoch: 444/1500 | train loss was: 0.13259 | val loss was: 0.30181\n",
      "epoch: 445/1500 | train loss was: 0.13256 | val loss was: 0.301644\n",
      "epoch: 446/1500 | train loss was: 0.132529 | val loss was: 0.301478\n",
      "epoch: 447/1500 | train loss was: 0.132499 | val loss was: 0.301312\n",
      "epoch: 448/1500 | train loss was: 0.132469 | val loss was: 0.301146\n",
      "epoch: 449/1500 | train loss was: 0.13244 | val loss was: 0.300981\n",
      "epoch: 450/1500 | train loss was: 0.13241 | val loss was: 0.300817\n",
      "epoch: 451/1500 | train loss was: 0.132381 | val loss was: 0.300652\n",
      "epoch: 452/1500 | train loss was: 0.132352 | val loss was: 0.300488\n",
      "epoch: 453/1500 | train loss was: 0.132324 | val loss was: 0.300325\n",
      "epoch: 454/1500 | train loss was: 0.132295 | val loss was: 0.300161\n",
      "epoch: 455/1500 | train loss was: 0.132267 | val loss was: 0.299998\n",
      "epoch: 456/1500 | train loss was: 0.13224 | val loss was: 0.299836\n",
      "epoch: 457/1500 | train loss was: 0.132212 | val loss was: 0.299673\n",
      "epoch: 458/1500 | train loss was: 0.132185 | val loss was: 0.299511\n",
      "epoch: 459/1500 | train loss was: 0.132158 | val loss was: 0.299349\n",
      "epoch: 460/1500 | train loss was: 0.132131 | val loss was: 0.299188\n",
      "epoch: 461/1500 | train loss was: 0.132105 | val loss was: 0.299027\n",
      "epoch: 462/1500 | train loss was: 0.132079 | val loss was: 0.298866\n",
      "epoch: 463/1500 | train loss was: 0.132053 | val loss was: 0.298705\n",
      "epoch: 464/1500 | train loss was: 0.132027 | val loss was: 0.298545\n",
      "epoch: 465/1500 | train loss was: 0.132002 | val loss was: 0.298385\n",
      "epoch: 466/1500 | train loss was: 0.131977 | val loss was: 0.298225\n",
      "epoch: 467/1500 | train loss was: 0.131952 | val loss was: 0.298066\n",
      "epoch: 468/1500 | train loss was: 0.131927 | val loss was: 0.297906\n",
      "epoch: 469/1500 | train loss was: 0.131903 | val loss was: 0.297748\n",
      "epoch: 470/1500 | train loss was: 0.131879 | val loss was: 0.297589\n",
      "epoch: 471/1500 | train loss was: 0.131855 | val loss was: 0.29743\n",
      "epoch: 472/1500 | train loss was: 0.131832 | val loss was: 0.297272\n",
      "epoch: 473/1500 | train loss was: 0.131808 | val loss was: 0.297114\n",
      "epoch: 474/1500 | train loss was: 0.131785 | val loss was: 0.296957\n",
      "epoch: 475/1500 | train loss was: 0.131762 | val loss was: 0.296799\n",
      "epoch: 476/1500 | train loss was: 0.13174 | val loss was: 0.296642\n",
      "epoch: 477/1500 | train loss was: 0.131717 | val loss was: 0.296485\n",
      "epoch: 478/1500 | train loss was: 0.131695 | val loss was: 0.296328\n",
      "epoch: 479/1500 | train loss was: 0.131673 | val loss was: 0.296172\n",
      "epoch: 480/1500 | train loss was: 0.131651 | val loss was: 0.296016\n",
      "epoch: 481/1500 | train loss was: 0.13163 | val loss was: 0.29586\n",
      "epoch: 482/1500 | train loss was: 0.131608 | val loss was: 0.295704\n",
      "epoch: 483/1500 | train loss was: 0.131587 | val loss was: 0.295549\n",
      "epoch: 484/1500 | train loss was: 0.131567 | val loss was: 0.295393\n",
      "epoch: 485/1500 | train loss was: 0.131546 | val loss was: 0.295238\n",
      "epoch: 486/1500 | train loss was: 0.131525 | val loss was: 0.295084\n",
      "epoch: 487/1500 | train loss was: 0.131505 | val loss was: 0.29493\n",
      "epoch: 488/1500 | train loss was: 0.131485 | val loss was: 0.294775\n",
      "epoch: 489/1500 | train loss was: 0.131465 | val loss was: 0.294622\n",
      "epoch: 490/1500 | train loss was: 0.131446 | val loss was: 0.294469\n",
      "epoch: 491/1500 | train loss was: 0.131426 | val loss was: 0.294317\n",
      "epoch: 492/1500 | train loss was: 0.131407 | val loss was: 0.294165\n",
      "epoch: 493/1500 | train loss was: 0.131388 | val loss was: 0.294013\n",
      "epoch: 494/1500 | train loss was: 0.131369 | val loss was: 0.293861\n",
      "epoch: 495/1500 | train loss was: 0.131351 | val loss was: 0.29371\n",
      "epoch: 496/1500 | train loss was: 0.131332 | val loss was: 0.293559\n",
      "epoch: 497/1500 | train loss was: 0.131314 | val loss was: 0.293408\n",
      "epoch: 498/1500 | train loss was: 0.131296 | val loss was: 0.293257\n",
      "epoch: 499/1500 | train loss was: 0.131278 | val loss was: 0.293107\n",
      "epoch: 500/1500 | train loss was: 0.13126 | val loss was: 0.292957\n",
      "epoch: 501/1500 | train loss was: 0.131242 | val loss was: 0.292807\n",
      "epoch: 502/1500 | train loss was: 0.131225 | val loss was: 0.292657\n",
      "epoch: 503/1500 | train loss was: 0.131207 | val loss was: 0.292508\n",
      "epoch: 504/1500 | train loss was: 0.13119 | val loss was: 0.292359\n",
      "epoch: 505/1500 | train loss was: 0.131173 | val loss was: 0.29221\n",
      "epoch: 506/1500 | train loss was: 0.131156 | val loss was: 0.292062\n",
      "epoch: 507/1500 | train loss was: 0.131139 | val loss was: 0.291913\n",
      "epoch: 508/1500 | train loss was: 0.131123 | val loss was: 0.291766\n",
      "epoch: 509/1500 | train loss was: 0.131106 | val loss was: 0.291618\n",
      "epoch: 510/1500 | train loss was: 0.13109 | val loss was: 0.291471\n",
      "epoch: 511/1500 | train loss was: 0.131074 | val loss was: 0.291324\n",
      "epoch: 512/1500 | train loss was: 0.131058 | val loss was: 0.291177\n",
      "epoch: 513/1500 | train loss was: 0.131042 | val loss was: 0.291031\n",
      "epoch: 514/1500 | train loss was: 0.131026 | val loss was: 0.290885\n",
      "epoch: 515/1500 | train loss was: 0.13101 | val loss was: 0.290739\n",
      "epoch: 516/1500 | train loss was: 0.130995 | val loss was: 0.290594\n",
      "epoch: 517/1500 | train loss was: 0.130979 | val loss was: 0.290449\n",
      "epoch: 518/1500 | train loss was: 0.130964 | val loss was: 0.290304\n",
      "epoch: 519/1500 | train loss was: 0.130949 | val loss was: 0.290159\n",
      "epoch: 520/1500 | train loss was: 0.130934 | val loss was: 0.290015\n",
      "epoch: 521/1500 | train loss was: 0.130919 | val loss was: 0.289872\n",
      "epoch: 522/1500 | train loss was: 0.130904 | val loss was: 0.289728\n",
      "epoch: 523/1500 | train loss was: 0.130889 | val loss was: 0.289585\n",
      "epoch: 524/1500 | train loss was: 0.130875 | val loss was: 0.289443\n",
      "epoch: 525/1500 | train loss was: 0.13086 | val loss was: 0.289301\n",
      "epoch: 526/1500 | train loss was: 0.130846 | val loss was: 0.289158\n",
      "epoch: 527/1500 | train loss was: 0.130831 | val loss was: 0.289016\n",
      "epoch: 528/1500 | train loss was: 0.130817 | val loss was: 0.288874\n",
      "epoch: 529/1500 | train loss was: 0.130802 | val loss was: 0.288733\n",
      "epoch: 530/1500 | train loss was: 0.130788 | val loss was: 0.288592\n",
      "epoch: 531/1500 | train loss was: 0.130774 | val loss was: 0.288451\n",
      "epoch: 532/1500 | train loss was: 0.13076 | val loss was: 0.288311\n",
      "epoch: 533/1500 | train loss was: 0.130746 | val loss was: 0.288171\n",
      "epoch: 534/1500 | train loss was: 0.130732 | val loss was: 0.288032\n",
      "epoch: 535/1500 | train loss was: 0.130719 | val loss was: 0.287893\n",
      "epoch: 536/1500 | train loss was: 0.130705 | val loss was: 0.287755\n",
      "epoch: 537/1500 | train loss was: 0.130692 | val loss was: 0.287617\n",
      "epoch: 538/1500 | train loss was: 0.130678 | val loss was: 0.287479\n",
      "epoch: 539/1500 | train loss was: 0.130665 | val loss was: 0.287342\n",
      "epoch: 540/1500 | train loss was: 0.130652 | val loss was: 0.287206\n",
      "epoch: 541/1500 | train loss was: 0.130638 | val loss was: 0.287069\n",
      "epoch: 542/1500 | train loss was: 0.130625 | val loss was: 0.286934\n",
      "epoch: 543/1500 | train loss was: 0.130612 | val loss was: 0.286798\n",
      "epoch: 544/1500 | train loss was: 0.1306 | val loss was: 0.286664\n",
      "epoch: 545/1500 | train loss was: 0.130587 | val loss was: 0.286529\n",
      "epoch: 546/1500 | train loss was: 0.130574 | val loss was: 0.286395\n",
      "epoch: 547/1500 | train loss was: 0.130561 | val loss was: 0.286262\n",
      "epoch: 548/1500 | train loss was: 0.130549 | val loss was: 0.286129\n",
      "epoch: 549/1500 | train loss was: 0.130537 | val loss was: 0.285996\n",
      "epoch: 550/1500 | train loss was: 0.130524 | val loss was: 0.285864\n",
      "epoch: 551/1500 | train loss was: 0.130512 | val loss was: 0.285732\n",
      "epoch: 552/1500 | train loss was: 0.1305 | val loss was: 0.285601\n",
      "epoch: 553/1500 | train loss was: 0.130488 | val loss was: 0.28547\n",
      "epoch: 554/1500 | train loss was: 0.130476 | val loss was: 0.28534\n",
      "epoch: 555/1500 | train loss was: 0.130464 | val loss was: 0.28521\n",
      "epoch: 556/1500 | train loss was: 0.130452 | val loss was: 0.285081\n",
      "epoch: 557/1500 | train loss was: 0.13044 | val loss was: 0.284952\n",
      "epoch: 558/1500 | train loss was: 0.130429 | val loss was: 0.284823\n",
      "epoch: 559/1500 | train loss was: 0.130417 | val loss was: 0.284695\n",
      "epoch: 560/1500 | train loss was: 0.130406 | val loss was: 0.284568\n",
      "epoch: 561/1500 | train loss was: 0.130394 | val loss was: 0.28444\n",
      "epoch: 562/1500 | train loss was: 0.130383 | val loss was: 0.284314\n",
      "epoch: 563/1500 | train loss was: 0.130372 | val loss was: 0.284188\n",
      "epoch: 564/1500 | train loss was: 0.130361 | val loss was: 0.284062\n",
      "epoch: 565/1500 | train loss was: 0.13035 | val loss was: 0.283937\n",
      "epoch: 566/1500 | train loss was: 0.130339 | val loss was: 0.283812\n",
      "epoch: 567/1500 | train loss was: 0.130328 | val loss was: 0.283688\n",
      "epoch: 568/1500 | train loss was: 0.130317 | val loss was: 0.283564\n",
      "epoch: 569/1500 | train loss was: 0.130307 | val loss was: 0.28344\n",
      "epoch: 570/1500 | train loss was: 0.130296 | val loss was: 0.283317\n",
      "epoch: 571/1500 | train loss was: 0.130286 | val loss was: 0.283195\n",
      "epoch: 572/1500 | train loss was: 0.130275 | val loss was: 0.283073\n",
      "epoch: 573/1500 | train loss was: 0.130265 | val loss was: 0.282951\n",
      "epoch: 574/1500 | train loss was: 0.130255 | val loss was: 0.28283\n",
      "epoch: 575/1500 | train loss was: 0.130244 | val loss was: 0.282709\n",
      "epoch: 576/1500 | train loss was: 0.130234 | val loss was: 0.282589\n",
      "epoch: 577/1500 | train loss was: 0.130224 | val loss was: 0.282469\n",
      "epoch: 578/1500 | train loss was: 0.130214 | val loss was: 0.28235\n",
      "epoch: 579/1500 | train loss was: 0.130205 | val loss was: 0.282231\n",
      "epoch: 580/1500 | train loss was: 0.130195 | val loss was: 0.282113\n",
      "epoch: 581/1500 | train loss was: 0.130185 | val loss was: 0.281995\n",
      "epoch: 582/1500 | train loss was: 0.130176 | val loss was: 0.281877\n",
      "epoch: 583/1500 | train loss was: 0.130166 | val loss was: 0.28176\n",
      "epoch: 584/1500 | train loss was: 0.130156 | val loss was: 0.281643\n",
      "epoch: 585/1500 | train loss was: 0.130146 | val loss was: 0.281527\n",
      "epoch: 586/1500 | train loss was: 0.130137 | val loss was: 0.281411\n",
      "epoch: 587/1500 | train loss was: 0.130127 | val loss was: 0.281295\n",
      "epoch: 588/1500 | train loss was: 0.130118 | val loss was: 0.28118\n",
      "epoch: 589/1500 | train loss was: 0.130109 | val loss was: 0.281066\n",
      "epoch: 590/1500 | train loss was: 0.130099 | val loss was: 0.280951\n",
      "epoch: 591/1500 | train loss was: 0.13009 | val loss was: 0.280838\n",
      "epoch: 592/1500 | train loss was: 0.130081 | val loss was: 0.280726\n",
      "epoch: 593/1500 | train loss was: 0.130073 | val loss was: 0.280613\n",
      "epoch: 594/1500 | train loss was: 0.130063 | val loss was: 0.2805\n",
      "epoch: 595/1500 | train loss was: 0.130055 | val loss was: 0.280389\n",
      "epoch: 596/1500 | train loss was: 0.130046 | val loss was: 0.280278\n",
      "epoch: 597/1500 | train loss was: 0.130037 | val loss was: 0.280167\n",
      "epoch: 598/1500 | train loss was: 0.130029 | val loss was: 0.280057\n",
      "epoch: 599/1500 | train loss was: 0.130021 | val loss was: 0.279948\n",
      "epoch: 600/1500 | train loss was: 0.130012 | val loss was: 0.279838\n",
      "epoch: 601/1500 | train loss was: 0.130004 | val loss was: 0.279729\n",
      "epoch: 602/1500 | train loss was: 0.129996 | val loss was: 0.279621\n",
      "epoch: 603/1500 | train loss was: 0.129987 | val loss was: 0.279512\n",
      "epoch: 604/1500 | train loss was: 0.12998 | val loss was: 0.279406\n",
      "epoch: 605/1500 | train loss was: 0.129972 | val loss was: 0.279299\n",
      "epoch: 606/1500 | train loss was: 0.129964 | val loss was: 0.279192\n",
      "epoch: 607/1500 | train loss was: 0.129956 | val loss was: 0.279086\n",
      "epoch: 608/1500 | train loss was: 0.129949 | val loss was: 0.278981\n",
      "epoch: 609/1500 | train loss was: 0.129941 | val loss was: 0.278875\n",
      "epoch: 610/1500 | train loss was: 0.129933 | val loss was: 0.27877\n",
      "epoch: 611/1500 | train loss was: 0.129926 | val loss was: 0.278667\n",
      "epoch: 612/1500 | train loss was: 0.129918 | val loss was: 0.278562\n",
      "epoch: 613/1500 | train loss was: 0.129912 | val loss was: 0.278459\n",
      "epoch: 614/1500 | train loss was: 0.129905 | val loss was: 0.278356\n",
      "epoch: 615/1500 | train loss was: 0.129897 | val loss was: 0.278253\n",
      "epoch: 616/1500 | train loss was: 0.129891 | val loss was: 0.278151\n",
      "epoch: 617/1500 | train loss was: 0.129884 | val loss was: 0.27805\n",
      "epoch: 618/1500 | train loss was: 0.129877 | val loss was: 0.277948\n",
      "epoch: 619/1500 | train loss was: 0.12987 | val loss was: 0.277848\n",
      "epoch: 620/1500 | train loss was: 0.129864 | val loss was: 0.277748\n",
      "epoch: 621/1500 | train loss was: 0.129857 | val loss was: 0.277647\n",
      "epoch: 622/1500 | train loss was: 0.129851 | val loss was: 0.277548\n",
      "epoch: 623/1500 | train loss was: 0.129845 | val loss was: 0.277449\n",
      "epoch: 624/1500 | train loss was: 0.129838 | val loss was: 0.277349\n",
      "epoch: 625/1500 | train loss was: 0.129832 | val loss was: 0.277252\n",
      "epoch: 626/1500 | train loss was: 0.129827 | val loss was: 0.277154\n",
      "epoch: 627/1500 | train loss was: 0.12982 | val loss was: 0.277056\n",
      "epoch: 628/1500 | train loss was: 0.129815 | val loss was: 0.276959\n",
      "epoch: 629/1500 | train loss was: 0.129809 | val loss was: 0.276863\n",
      "epoch: 630/1500 | train loss was: 0.129803 | val loss was: 0.276766\n",
      "epoch: 631/1500 | train loss was: 0.129798 | val loss was: 0.27667\n",
      "epoch: 632/1500 | train loss was: 0.129793 | val loss was: 0.276575\n",
      "epoch: 633/1500 | train loss was: 0.129786 | val loss was: 0.276479\n",
      "epoch: 634/1500 | train loss was: 0.129781 | val loss was: 0.276385\n",
      "epoch: 635/1500 | train loss was: 0.129776 | val loss was: 0.27629\n",
      "epoch: 636/1500 | train loss was: 0.129771 | val loss was: 0.276197\n",
      "epoch: 637/1500 | train loss was: 0.129766 | val loss was: 0.276104\n",
      "epoch: 638/1500 | train loss was: 0.12976 | val loss was: 0.27601\n",
      "epoch: 639/1500 | train loss was: 0.129756 | val loss was: 0.275918\n",
      "epoch: 640/1500 | train loss was: 0.129752 | val loss was: 0.275826\n",
      "epoch: 641/1500 | train loss was: 0.129746 | val loss was: 0.275734\n",
      "epoch: 642/1500 | train loss was: 0.129742 | val loss was: 0.275643\n",
      "epoch: 643/1500 | train loss was: 0.129738 | val loss was: 0.275552\n",
      "epoch: 644/1500 | train loss was: 0.129733 | val loss was: 0.27546\n",
      "epoch: 645/1500 | train loss was: 0.129729 | val loss was: 0.275371\n",
      "epoch: 646/1500 | train loss was: 0.129725 | val loss was: 0.275281\n",
      "epoch: 647/1500 | train loss was: 0.12972 | val loss was: 0.27519\n",
      "epoch: 648/1500 | train loss was: 0.129716 | val loss was: 0.275102\n",
      "epoch: 649/1500 | train loss was: 0.129712 | val loss was: 0.275012\n",
      "epoch: 650/1500 | train loss was: 0.129708 | val loss was: 0.274924\n",
      "epoch: 651/1500 | train loss was: 0.129705 | val loss was: 0.274836\n",
      "epoch: 652/1500 | train loss was: 0.129701 | val loss was: 0.274747\n",
      "epoch: 653/1500 | train loss was: 0.129697 | val loss was: 0.27466\n",
      "epoch: 654/1500 | train loss was: 0.129694 | val loss was: 0.274573\n",
      "epoch: 655/1500 | train loss was: 0.12969 | val loss was: 0.274486\n",
      "epoch: 656/1500 | train loss was: 0.129687 | val loss was: 0.2744\n",
      "epoch: 657/1500 | train loss was: 0.129683 | val loss was: 0.274313\n",
      "epoch: 658/1500 | train loss was: 0.129681 | val loss was: 0.274227\n",
      "epoch: 659/1500 | train loss was: 0.129678 | val loss was: 0.274142\n",
      "epoch: 660/1500 | train loss was: 0.129675 | val loss was: 0.274056\n",
      "epoch: 661/1500 | train loss was: 0.129672 | val loss was: 0.273972\n",
      "epoch: 662/1500 | train loss was: 0.12967 | val loss was: 0.273888\n",
      "epoch: 663/1500 | train loss was: 0.129666 | val loss was: 0.273803\n",
      "epoch: 664/1500 | train loss was: 0.129664 | val loss was: 0.27372\n",
      "epoch: 665/1500 | train loss was: 0.129661 | val loss was: 0.273635\n",
      "epoch: 666/1500 | train loss was: 0.129659 | val loss was: 0.273553\n",
      "epoch: 667/1500 | train loss was: 0.129657 | val loss was: 0.27347\n",
      "epoch: 668/1500 | train loss was: 0.129654 | val loss was: 0.273387\n",
      "epoch: 669/1500 | train loss was: 0.129653 | val loss was: 0.273305\n",
      "epoch: 670/1500 | train loss was: 0.129651 | val loss was: 0.273224\n",
      "epoch: 671/1500 | train loss was: 0.129649 | val loss was: 0.273141\n",
      "epoch: 672/1500 | train loss was: 0.129647 | val loss was: 0.27306\n",
      "epoch: 673/1500 | train loss was: 0.129645 | val loss was: 0.272978\n",
      "epoch: 674/1500 | train loss was: 0.129644 | val loss was: 0.272898\n",
      "epoch: 675/1500 | train loss was: 0.129642 | val loss was: 0.272818\n",
      "epoch: 676/1500 | train loss was: 0.12964 | val loss was: 0.272737\n",
      "epoch: 677/1500 | train loss was: 0.129639 | val loss was: 0.272658\n",
      "epoch: 678/1500 | train loss was: 0.129637 | val loss was: 0.272578\n",
      "epoch: 679/1500 | train loss was: 0.129637 | val loss was: 0.272499\n",
      "epoch: 680/1500 | train loss was: 0.129636 | val loss was: 0.272421\n",
      "epoch: 681/1500 | train loss was: 0.129634 | val loss was: 0.272341\n",
      "epoch: 682/1500 | train loss was: 0.129634 | val loss was: 0.272264\n",
      "epoch: 683/1500 | train loss was: 0.129634 | val loss was: 0.272186\n",
      "epoch: 684/1500 | train loss was: 0.129632 | val loss was: 0.272108\n",
      "epoch: 685/1500 | train loss was: 0.129632 | val loss was: 0.272031\n",
      "epoch: 686/1500 | train loss was: 0.129631 | val loss was: 0.271953\n",
      "epoch: 687/1500 | train loss was: 0.129631 | val loss was: 0.271876\n",
      "epoch: 688/1500 | train loss was: 0.129631 | val loss was: 0.2718\n",
      "epoch: 689/1500 | train loss was: 0.12963 | val loss was: 0.271723\n",
      "epoch: 690/1500 | train loss was: 0.12963 | val loss was: 0.271648\n",
      "epoch: 691/1500 | train loss was: 0.12963 | val loss was: 0.271571\n",
      "epoch: 692/1500 | train loss was: 0.12963 | val loss was: 0.271496\n",
      "epoch: 693/1500 | train loss was: 0.129631 | val loss was: 0.271421\n",
      "epoch: 694/1500 | train loss was: 0.12963 | val loss was: 0.271346\n",
      "epoch: 695/1500 | train loss was: 0.129631 | val loss was: 0.271272\n",
      "epoch: 696/1500 | train loss was: 0.129631 | val loss was: 0.271197\n",
      "epoch: 697/1500 | train loss was: 0.129632 | val loss was: 0.271123\n",
      "epoch: 698/1500 | train loss was: 0.129632 | val loss was: 0.271048\n",
      "epoch: 699/1500 | train loss was: 0.129633 | val loss was: 0.270975\n",
      "epoch: 700/1500 | train loss was: 0.129634 | val loss was: 0.270903\n",
      "epoch: 701/1500 | train loss was: 0.129635 | val loss was: 0.270829\n",
      "epoch: 702/1500 | train loss was: 0.129636 | val loss was: 0.270756\n",
      "epoch: 703/1500 | train loss was: 0.129636 | val loss was: 0.270683\n",
      "epoch: 704/1500 | train loss was: 0.129638 | val loss was: 0.270611\n",
      "epoch: 705/1500 | train loss was: 0.12964 | val loss was: 0.27054\n",
      "epoch: 706/1500 | train loss was: 0.129641 | val loss was: 0.270467\n",
      "epoch: 707/1500 | train loss was: 0.129643 | val loss was: 0.270396\n",
      "epoch: 708/1500 | train loss was: 0.129643 | val loss was: 0.270324\n",
      "epoch: 709/1500 | train loss was: 0.129646 | val loss was: 0.270254\n",
      "epoch: 710/1500 | train loss was: 0.129647 | val loss was: 0.270182\n",
      "epoch: 711/1500 | train loss was: 0.129649 | val loss was: 0.270112\n",
      "epoch: 712/1500 | train loss was: 0.129651 | val loss was: 0.270043\n",
      "epoch: 713/1500 | train loss was: 0.129653 | val loss was: 0.269972\n",
      "epoch: 714/1500 | train loss was: 0.129655 | val loss was: 0.269903\n",
      "epoch: 715/1500 | train loss was: 0.129657 | val loss was: 0.269832\n",
      "epoch: 716/1500 | train loss was: 0.12966 | val loss was: 0.269764\n",
      "epoch: 717/1500 | train loss was: 0.129663 | val loss was: 0.269695\n",
      "epoch: 718/1500 | train loss was: 0.129665 | val loss was: 0.269625\n",
      "epoch: 719/1500 | train loss was: 0.129668 | val loss was: 0.269557\n",
      "epoch: 720/1500 | train loss was: 0.12967 | val loss was: 0.269488\n",
      "epoch: 721/1500 | train loss was: 0.129673 | val loss was: 0.269421\n",
      "epoch: 722/1500 | train loss was: 0.129676 | val loss was: 0.269352\n",
      "epoch: 723/1500 | train loss was: 0.129679 | val loss was: 0.269285\n",
      "epoch: 724/1500 | train loss was: 0.129683 | val loss was: 0.269218\n",
      "epoch: 725/1500 | train loss was: 0.129685 | val loss was: 0.26915\n",
      "epoch: 726/1500 | train loss was: 0.129689 | val loss was: 0.269084\n",
      "epoch: 727/1500 | train loss was: 0.129692 | val loss was: 0.269016\n",
      "epoch: 728/1500 | train loss was: 0.129696 | val loss was: 0.26895\n",
      "epoch: 729/1500 | train loss was: 0.129699 | val loss was: 0.268883\n",
      "epoch: 730/1500 | train loss was: 0.129703 | val loss was: 0.268817\n",
      "epoch: 731/1500 | train loss was: 0.129706 | val loss was: 0.268751\n",
      "epoch: 732/1500 | train loss was: 0.129711 | val loss was: 0.268686\n",
      "epoch: 733/1500 | train loss was: 0.129715 | val loss was: 0.268621\n",
      "epoch: 734/1500 | train loss was: 0.129719 | val loss was: 0.268555\n",
      "epoch: 735/1500 | train loss was: 0.129723 | val loss was: 0.26849\n",
      "epoch: 736/1500 | train loss was: 0.129727 | val loss was: 0.268425\n",
      "epoch: 737/1500 | train loss was: 0.129732 | val loss was: 0.268361\n",
      "epoch: 738/1500 | train loss was: 0.129736 | val loss was: 0.268296\n",
      "epoch: 739/1500 | train loss was: 0.129741 | val loss was: 0.268232\n",
      "epoch: 740/1500 | train loss was: 0.129745 | val loss was: 0.268167\n",
      "epoch: 741/1500 | train loss was: 0.12975 | val loss was: 0.268104\n",
      "epoch: 742/1500 | train loss was: 0.129756 | val loss was: 0.268041\n",
      "epoch: 743/1500 | train loss was: 0.12976 | val loss was: 0.267977\n",
      "epoch: 744/1500 | train loss was: 0.129766 | val loss was: 0.267914\n",
      "epoch: 745/1500 | train loss was: 0.12977 | val loss was: 0.267851\n",
      "epoch: 746/1500 | train loss was: 0.129776 | val loss was: 0.26779\n",
      "epoch: 747/1500 | train loss was: 0.129781 | val loss was: 0.267727\n",
      "epoch: 748/1500 | train loss was: 0.129787 | val loss was: 0.267665\n",
      "epoch: 749/1500 | train loss was: 0.129792 | val loss was: 0.267603\n",
      "epoch: 750/1500 | train loss was: 0.129798 | val loss was: 0.267542\n",
      "epoch: 751/1500 | train loss was: 0.129804 | val loss was: 0.26748\n",
      "epoch: 752/1500 | train loss was: 0.12981 | val loss was: 0.26742\n",
      "epoch: 753/1500 | train loss was: 0.129816 | val loss was: 0.267359\n",
      "epoch: 754/1500 | train loss was: 0.129822 | val loss was: 0.267298\n",
      "epoch: 755/1500 | train loss was: 0.129829 | val loss was: 0.267238\n",
      "epoch: 756/1500 | train loss was: 0.129835 | val loss was: 0.267177\n",
      "epoch: 757/1500 | train loss was: 0.129842 | val loss was: 0.267117\n",
      "epoch: 758/1500 | train loss was: 0.129848 | val loss was: 0.267057\n",
      "epoch: 759/1500 | train loss was: 0.129855 | val loss was: 0.266997\n",
      "epoch: 760/1500 | train loss was: 0.129861 | val loss was: 0.266937\n",
      "epoch: 761/1500 | train loss was: 0.129868 | val loss was: 0.266878\n",
      "epoch: 762/1500 | train loss was: 0.129875 | val loss was: 0.266818\n",
      "epoch: 763/1500 | train loss was: 0.129882 | val loss was: 0.26676\n",
      "epoch: 764/1500 | train loss was: 0.129889 | val loss was: 0.2667\n",
      "epoch: 765/1500 | train loss was: 0.129896 | val loss was: 0.266642\n",
      "epoch: 766/1500 | train loss was: 0.129903 | val loss was: 0.266583\n",
      "epoch: 767/1500 | train loss was: 0.129911 | val loss was: 0.266525\n",
      "epoch: 768/1500 | train loss was: 0.129918 | val loss was: 0.266467\n",
      "epoch: 769/1500 | train loss was: 0.129926 | val loss was: 0.266409\n",
      "epoch: 770/1500 | train loss was: 0.129934 | val loss was: 0.266351\n",
      "epoch: 771/1500 | train loss was: 0.129942 | val loss was: 0.266294\n",
      "epoch: 772/1500 | train loss was: 0.12995 | val loss was: 0.266237\n",
      "epoch: 773/1500 | train loss was: 0.129957 | val loss was: 0.266179\n",
      "epoch: 774/1500 | train loss was: 0.12996 | val loss was: 0.266122\n",
      "epoch: 775/1500 | train loss was: 0.129962 | val loss was: 0.266065\n",
      "epoch: 776/1500 | train loss was: 0.129964 | val loss was: 0.266008\n",
      "epoch: 777/1500 | train loss was: 0.129966 | val loss was: 0.265951\n",
      "epoch: 778/1500 | train loss was: 0.129969 | val loss was: 0.265895\n",
      "epoch: 779/1500 | train loss was: 0.129971 | val loss was: 0.265838\n",
      "epoch: 780/1500 | train loss was: 0.129974 | val loss was: 0.265783\n",
      "epoch: 781/1500 | train loss was: 0.129976 | val loss was: 0.265726\n",
      "epoch: 782/1500 | train loss was: 0.129979 | val loss was: 0.265671\n",
      "epoch: 783/1500 | train loss was: 0.129982 | val loss was: 0.265615\n",
      "epoch: 784/1500 | train loss was: 0.129985 | val loss was: 0.26556\n",
      "epoch: 785/1500 | train loss was: 0.129988 | val loss was: 0.265504\n",
      "epoch: 786/1500 | train loss was: 0.129991 | val loss was: 0.265449\n",
      "epoch: 787/1500 | train loss was: 0.129995 | val loss was: 0.265394\n",
      "epoch: 788/1500 | train loss was: 0.129998 | val loss was: 0.265339\n",
      "epoch: 789/1500 | train loss was: 0.130001 | val loss was: 0.265284\n",
      "epoch: 790/1500 | train loss was: 0.130005 | val loss was: 0.26523\n",
      "epoch: 791/1500 | train loss was: 0.130008 | val loss was: 0.265175\n",
      "epoch: 792/1500 | train loss was: 0.130012 | val loss was: 0.265121\n",
      "epoch: 793/1500 | train loss was: 0.130016 | val loss was: 0.265067\n",
      "epoch: 794/1500 | train loss was: 0.13002 | val loss was: 0.265014\n",
      "epoch: 795/1500 | train loss was: 0.130024 | val loss was: 0.264959\n",
      "epoch: 796/1500 | train loss was: 0.130028 | val loss was: 0.264906\n",
      "epoch: 797/1500 | train loss was: 0.130032 | val loss was: 0.264852\n",
      "epoch: 798/1500 | train loss was: 0.130036 | val loss was: 0.2648\n",
      "epoch: 799/1500 | train loss was: 0.130041 | val loss was: 0.264746\n",
      "epoch: 800/1500 | train loss was: 0.130045 | val loss was: 0.264693\n",
      "epoch: 801/1500 | train loss was: 0.130049 | val loss was: 0.26464\n",
      "epoch: 802/1500 | train loss was: 0.130054 | val loss was: 0.264588\n",
      "epoch: 803/1500 | train loss was: 0.130059 | val loss was: 0.264535\n",
      "epoch: 804/1500 | train loss was: 0.130063 | val loss was: 0.264482\n",
      "epoch: 805/1500 | train loss was: 0.130068 | val loss was: 0.26443\n",
      "epoch: 806/1500 | train loss was: 0.130073 | val loss was: 0.264378\n",
      "epoch: 807/1500 | train loss was: 0.130078 | val loss was: 0.264326\n",
      "epoch: 808/1500 | train loss was: 0.130083 | val loss was: 0.264274\n",
      "epoch: 809/1500 | train loss was: 0.130089 | val loss was: 0.264223\n",
      "epoch: 810/1500 | train loss was: 0.130094 | val loss was: 0.264171\n",
      "epoch: 811/1500 | train loss was: 0.130099 | val loss was: 0.26412\n",
      "epoch: 812/1500 | train loss was: 0.130105 | val loss was: 0.264069\n",
      "epoch: 813/1500 | train loss was: 0.130111 | val loss was: 0.264018\n",
      "epoch: 814/1500 | train loss was: 0.130116 | val loss was: 0.263967\n",
      "epoch: 815/1500 | train loss was: 0.130122 | val loss was: 0.263917\n",
      "epoch: 816/1500 | train loss was: 0.130128 | val loss was: 0.263866\n",
      "epoch: 817/1500 | train loss was: 0.130134 | val loss was: 0.263816\n",
      "epoch: 818/1500 | train loss was: 0.13014 | val loss was: 0.263765\n",
      "epoch: 819/1500 | train loss was: 0.130146 | val loss was: 0.263715\n",
      "epoch: 820/1500 | train loss was: 0.130152 | val loss was: 0.263665\n",
      "epoch: 821/1500 | train loss was: 0.130158 | val loss was: 0.263614\n",
      "epoch: 822/1500 | train loss was: 0.130164 | val loss was: 0.263565\n",
      "epoch: 823/1500 | train loss was: 0.130171 | val loss was: 0.263515\n",
      "epoch: 824/1500 | train loss was: 0.130177 | val loss was: 0.263466\n",
      "epoch: 825/1500 | train loss was: 0.130184 | val loss was: 0.263416\n",
      "epoch: 826/1500 | train loss was: 0.130191 | val loss was: 0.263368\n",
      "epoch: 827/1500 | train loss was: 0.130197 | val loss was: 0.263318\n",
      "epoch: 828/1500 | train loss was: 0.130204 | val loss was: 0.26327\n",
      "epoch: 829/1500 | train loss was: 0.130211 | val loss was: 0.26322\n",
      "epoch: 830/1500 | train loss was: 0.130218 | val loss was: 0.263172\n",
      "epoch: 831/1500 | train loss was: 0.130225 | val loss was: 0.263123\n",
      "epoch: 832/1500 | train loss was: 0.130232 | val loss was: 0.263074\n",
      "epoch: 833/1500 | train loss was: 0.13024 | val loss was: 0.263027\n",
      "epoch: 834/1500 | train loss was: 0.130247 | val loss was: 0.262978\n",
      "epoch: 835/1500 | train loss was: 0.130255 | val loss was: 0.262931\n",
      "epoch: 836/1500 | train loss was: 0.130262 | val loss was: 0.262883\n",
      "epoch: 837/1500 | train loss was: 0.13027 | val loss was: 0.262835\n",
      "epoch: 838/1500 | train loss was: 0.130277 | val loss was: 0.262787\n",
      "epoch: 839/1500 | train loss was: 0.130285 | val loss was: 0.26274\n",
      "epoch: 840/1500 | train loss was: 0.130293 | val loss was: 0.262693\n",
      "epoch: 841/1500 | train loss was: 0.130301 | val loss was: 0.262645\n",
      "epoch: 842/1500 | train loss was: 0.130309 | val loss was: 0.262599\n",
      "epoch: 843/1500 | train loss was: 0.130317 | val loss was: 0.262551\n",
      "epoch: 844/1500 | train loss was: 0.130325 | val loss was: 0.262505\n",
      "epoch: 845/1500 | train loss was: 0.130333 | val loss was: 0.262458\n",
      "epoch: 846/1500 | train loss was: 0.130342 | val loss was: 0.262412\n",
      "epoch: 847/1500 | train loss was: 0.13035 | val loss was: 0.262365\n",
      "epoch: 848/1500 | train loss was: 0.130358 | val loss was: 0.262319\n",
      "epoch: 849/1500 | train loss was: 0.130367 | val loss was: 0.262273\n",
      "epoch: 850/1500 | train loss was: 0.130375 | val loss was: 0.262227\n",
      "epoch: 851/1500 | train loss was: 0.130384 | val loss was: 0.262181\n",
      "epoch: 852/1500 | train loss was: 0.130393 | val loss was: 0.262135\n",
      "epoch: 853/1500 | train loss was: 0.130402 | val loss was: 0.26209\n",
      "epoch: 854/1500 | train loss was: 0.13041 | val loss was: 0.262044\n",
      "epoch: 855/1500 | train loss was: 0.130419 | val loss was: 0.261998\n",
      "epoch: 856/1500 | train loss was: 0.130428 | val loss was: 0.261954\n",
      "epoch: 857/1500 | train loss was: 0.130437 | val loss was: 0.261908\n",
      "epoch: 858/1500 | train loss was: 0.130446 | val loss was: 0.261864\n",
      "epoch: 859/1500 | train loss was: 0.130456 | val loss was: 0.261819\n",
      "epoch: 860/1500 | train loss was: 0.130465 | val loss was: 0.261774\n",
      "epoch: 861/1500 | train loss was: 0.130474 | val loss was: 0.261729\n",
      "epoch: 862/1500 | train loss was: 0.130483 | val loss was: 0.261685\n",
      "epoch: 863/1500 | train loss was: 0.130493 | val loss was: 0.261641\n",
      "epoch: 864/1500 | train loss was: 0.130502 | val loss was: 0.261596\n",
      "epoch: 865/1500 | train loss was: 0.130512 | val loss was: 0.261553\n",
      "epoch: 866/1500 | train loss was: 0.130522 | val loss was: 0.261508\n",
      "epoch: 867/1500 | train loss was: 0.130531 | val loss was: 0.261464\n",
      "epoch: 868/1500 | train loss was: 0.130541 | val loss was: 0.261421\n",
      "epoch: 869/1500 | train loss was: 0.130551 | val loss was: 0.261377\n",
      "epoch: 870/1500 | train loss was: 0.130561 | val loss was: 0.261334\n",
      "epoch: 871/1500 | train loss was: 0.130571 | val loss was: 0.26129\n",
      "epoch: 872/1500 | train loss was: 0.130581 | val loss was: 0.261247\n",
      "epoch: 873/1500 | train loss was: 0.130591 | val loss was: 0.261204\n",
      "epoch: 874/1500 | train loss was: 0.130601 | val loss was: 0.261161\n",
      "epoch: 875/1500 | train loss was: 0.130611 | val loss was: 0.261119\n",
      "epoch: 876/1500 | train loss was: 0.130621 | val loss was: 0.261076\n",
      "epoch: 877/1500 | train loss was: 0.130631 | val loss was: 0.261033\n",
      "epoch: 878/1500 | train loss was: 0.130642 | val loss was: 0.260991\n",
      "epoch: 879/1500 | train loss was: 0.130652 | val loss was: 0.260948\n",
      "epoch: 880/1500 | train loss was: 0.130663 | val loss was: 0.260906\n",
      "epoch: 881/1500 | train loss was: 0.130673 | val loss was: 0.260864\n",
      "epoch: 882/1500 | train loss was: 0.130683 | val loss was: 0.260821\n",
      "epoch: 883/1500 | train loss was: 0.130694 | val loss was: 0.26078\n",
      "epoch: 884/1500 | train loss was: 0.130705 | val loss was: 0.260738\n",
      "epoch: 885/1500 | train loss was: 0.130716 | val loss was: 0.260697\n",
      "epoch: 886/1500 | train loss was: 0.130726 | val loss was: 0.260655\n",
      "epoch: 887/1500 | train loss was: 0.130737 | val loss was: 0.260613\n",
      "epoch: 888/1500 | train loss was: 0.130748 | val loss was: 0.260572\n",
      "epoch: 889/1500 | train loss was: 0.130758 | val loss was: 0.260531\n",
      "epoch: 890/1500 | train loss was: 0.13077 | val loss was: 0.26049\n",
      "epoch: 891/1500 | train loss was: 0.13078 | val loss was: 0.260449\n",
      "epoch: 892/1500 | train loss was: 0.130791 | val loss was: 0.260408\n",
      "epoch: 893/1500 | train loss was: 0.130802 | val loss was: 0.260368\n",
      "epoch: 894/1500 | train loss was: 0.130813 | val loss was: 0.260327\n",
      "epoch: 895/1500 | train loss was: 0.130824 | val loss was: 0.260286\n",
      "epoch: 896/1500 | train loss was: 0.130835 | val loss was: 0.260246\n",
      "epoch: 897/1500 | train loss was: 0.130846 | val loss was: 0.260205\n",
      "epoch: 898/1500 | train loss was: 0.130858 | val loss was: 0.260166\n",
      "epoch: 899/1500 | train loss was: 0.130869 | val loss was: 0.260125\n",
      "epoch: 900/1500 | train loss was: 0.13088 | val loss was: 0.260085\n",
      "epoch: 901/1500 | train loss was: 0.130891 | val loss was: 0.260046\n",
      "epoch: 902/1500 | train loss was: 0.130903 | val loss was: 0.260006\n",
      "epoch: 903/1500 | train loss was: 0.130914 | val loss was: 0.259966\n",
      "epoch: 904/1500 | train loss was: 0.130925 | val loss was: 0.259927\n",
      "epoch: 905/1500 | train loss was: 0.130937 | val loss was: 0.259887\n",
      "epoch: 906/1500 | train loss was: 0.130948 | val loss was: 0.259848\n",
      "epoch: 907/1500 | train loss was: 0.13096 | val loss was: 0.259809\n",
      "epoch: 908/1500 | train loss was: 0.130971 | val loss was: 0.25977\n",
      "epoch: 909/1500 | train loss was: 0.130983 | val loss was: 0.259731\n",
      "epoch: 910/1500 | train loss was: 0.130994 | val loss was: 0.259692\n",
      "epoch: 911/1500 | train loss was: 0.131005 | val loss was: 0.259653\n",
      "epoch: 912/1500 | train loss was: 0.131017 | val loss was: 0.259615\n",
      "epoch: 913/1500 | train loss was: 0.131029 | val loss was: 0.259576\n",
      "epoch: 914/1500 | train loss was: 0.13104 | val loss was: 0.259537\n",
      "epoch: 915/1500 | train loss was: 0.131052 | val loss was: 0.259499\n",
      "epoch: 916/1500 | train loss was: 0.131063 | val loss was: 0.259461\n",
      "epoch: 917/1500 | train loss was: 0.131075 | val loss was: 0.259423\n",
      "epoch: 918/1500 | train loss was: 0.131087 | val loss was: 0.259385\n",
      "epoch: 919/1500 | train loss was: 0.131098 | val loss was: 0.259347\n",
      "epoch: 920/1500 | train loss was: 0.13111 | val loss was: 0.259309\n",
      "epoch: 921/1500 | train loss was: 0.131122 | val loss was: 0.259271\n",
      "epoch: 922/1500 | train loss was: 0.131133 | val loss was: 0.259233\n",
      "epoch: 923/1500 | train loss was: 0.131145 | val loss was: 0.259196\n",
      "epoch: 924/1500 | train loss was: 0.131157 | val loss was: 0.259159\n",
      "epoch: 925/1500 | train loss was: 0.131168 | val loss was: 0.259121\n",
      "epoch: 926/1500 | train loss was: 0.13118 | val loss was: 0.259084\n",
      "epoch: 927/1500 | train loss was: 0.131192 | val loss was: 0.259047\n",
      "epoch: 928/1500 | train loss was: 0.131204 | val loss was: 0.25901\n",
      "epoch: 929/1500 | train loss was: 0.131216 | val loss was: 0.258973\n",
      "epoch: 930/1500 | train loss was: 0.131227 | val loss was: 0.258936\n",
      "epoch: 931/1500 | train loss was: 0.131239 | val loss was: 0.2589\n",
      "epoch: 932/1500 | train loss was: 0.131251 | val loss was: 0.258863\n",
      "epoch: 933/1500 | train loss was: 0.131263 | val loss was: 0.258826\n",
      "epoch: 934/1500 | train loss was: 0.131275 | val loss was: 0.25879\n",
      "epoch: 935/1500 | train loss was: 0.131286 | val loss was: 0.258753\n",
      "epoch: 936/1500 | train loss was: 0.131298 | val loss was: 0.258717\n",
      "epoch: 937/1500 | train loss was: 0.13131 | val loss was: 0.258681\n",
      "epoch: 938/1500 | train loss was: 0.131322 | val loss was: 0.258644\n",
      "epoch: 939/1500 | train loss was: 0.131333 | val loss was: 0.258608\n",
      "epoch: 940/1500 | train loss was: 0.131345 | val loss was: 0.258573\n",
      "epoch: 941/1500 | train loss was: 0.131357 | val loss was: 0.258536\n",
      "epoch: 942/1500 | train loss was: 0.131369 | val loss was: 0.2585\n",
      "epoch: 943/1500 | train loss was: 0.131381 | val loss was: 0.258465\n",
      "epoch: 944/1500 | train loss was: 0.131393 | val loss was: 0.258429\n",
      "epoch: 945/1500 | train loss was: 0.131404 | val loss was: 0.258393\n",
      "epoch: 946/1500 | train loss was: 0.131416 | val loss was: 0.258358\n",
      "epoch: 947/1500 | train loss was: 0.131428 | val loss was: 0.258323\n",
      "epoch: 948/1500 | train loss was: 0.131439 | val loss was: 0.258287\n",
      "epoch: 949/1500 | train loss was: 0.131451 | val loss was: 0.258253\n",
      "epoch: 950/1500 | train loss was: 0.131463 | val loss was: 0.258217\n",
      "epoch: 951/1500 | train loss was: 0.131475 | val loss was: 0.258182\n",
      "epoch: 952/1500 | train loss was: 0.131487 | val loss was: 0.258147\n",
      "epoch: 953/1500 | train loss was: 0.131498 | val loss was: 0.258112\n",
      "epoch: 954/1500 | train loss was: 0.13151 | val loss was: 0.258078\n",
      "epoch: 955/1500 | train loss was: 0.131522 | val loss was: 0.258043\n",
      "epoch: 956/1500 | train loss was: 0.131533 | val loss was: 0.258008\n",
      "epoch: 957/1500 | train loss was: 0.131545 | val loss was: 0.257974\n",
      "epoch: 958/1500 | train loss was: 0.131557 | val loss was: 0.257939\n",
      "epoch: 959/1500 | train loss was: 0.131568 | val loss was: 0.257904\n",
      "epoch: 960/1500 | train loss was: 0.13158 | val loss was: 0.25787\n",
      "epoch: 961/1500 | train loss was: 0.131592 | val loss was: 0.257835\n",
      "epoch: 962/1500 | train loss was: 0.131603 | val loss was: 0.257801\n",
      "epoch: 963/1500 | train loss was: 0.131615 | val loss was: 0.257767\n",
      "epoch: 964/1500 | train loss was: 0.131627 | val loss was: 0.257733\n",
      "epoch: 965/1500 | train loss was: 0.131638 | val loss was: 0.257699\n",
      "epoch: 966/1500 | train loss was: 0.13165 | val loss was: 0.257665\n",
      "epoch: 967/1500 | train loss was: 0.131661 | val loss was: 0.257631\n",
      "epoch: 968/1500 | train loss was: 0.131673 | val loss was: 0.257597\n",
      "epoch: 969/1500 | train loss was: 0.131684 | val loss was: 0.257563\n",
      "epoch: 970/1500 | train loss was: 0.131696 | val loss was: 0.257529\n",
      "epoch: 971/1500 | train loss was: 0.131707 | val loss was: 0.257496\n",
      "epoch: 972/1500 | train loss was: 0.131719 | val loss was: 0.257462\n",
      "epoch: 973/1500 | train loss was: 0.13173 | val loss was: 0.257429\n",
      "epoch: 974/1500 | train loss was: 0.131741 | val loss was: 0.257395\n",
      "epoch: 975/1500 | train loss was: 0.131753 | val loss was: 0.257362\n",
      "epoch: 976/1500 | train loss was: 0.131764 | val loss was: 0.257328\n",
      "epoch: 977/1500 | train loss was: 0.131775 | val loss was: 0.257295\n",
      "epoch: 978/1500 | train loss was: 0.131787 | val loss was: 0.257262\n",
      "epoch: 979/1500 | train loss was: 0.131798 | val loss was: 0.257228\n",
      "epoch: 980/1500 | train loss was: 0.131809 | val loss was: 0.257195\n",
      "epoch: 981/1500 | train loss was: 0.131821 | val loss was: 0.257162\n",
      "epoch: 982/1500 | train loss was: 0.131832 | val loss was: 0.257129\n",
      "epoch: 983/1500 | train loss was: 0.131843 | val loss was: 0.257096\n",
      "epoch: 984/1500 | train loss was: 0.131854 | val loss was: 0.257063\n",
      "epoch: 985/1500 | train loss was: 0.131865 | val loss was: 0.25703\n",
      "epoch: 986/1500 | train loss was: 0.131876 | val loss was: 0.256997\n",
      "epoch: 987/1500 | train loss was: 0.131887 | val loss was: 0.256965\n",
      "epoch: 988/1500 | train loss was: 0.131898 | val loss was: 0.256932\n",
      "epoch: 989/1500 | train loss was: 0.131909 | val loss was: 0.256899\n",
      "epoch: 990/1500 | train loss was: 0.131921 | val loss was: 0.256867\n",
      "epoch: 991/1500 | train loss was: 0.131931 | val loss was: 0.256834\n",
      "epoch: 992/1500 | train loss was: 0.131942 | val loss was: 0.256801\n",
      "epoch: 993/1500 | train loss was: 0.131953 | val loss was: 0.256769\n",
      "epoch: 994/1500 | train loss was: 0.131964 | val loss was: 0.256736\n",
      "epoch: 995/1500 | train loss was: 0.131975 | val loss was: 0.256704\n",
      "epoch: 996/1500 | train loss was: 0.131986 | val loss was: 0.256672\n",
      "epoch: 997/1500 | train loss was: 0.131997 | val loss was: 0.256639\n",
      "epoch: 998/1500 | train loss was: 0.132007 | val loss was: 0.256607\n",
      "epoch: 999/1500 | train loss was: 0.132018 | val loss was: 0.256575\n",
      "epoch: 1000/1500 | train loss was: 0.132029 | val loss was: 0.256543\n",
      "epoch: 1001/1500 | train loss was: 0.132039 | val loss was: 0.25651\n",
      "epoch: 1002/1500 | train loss was: 0.13205 | val loss was: 0.256479\n",
      "epoch: 1003/1500 | train loss was: 0.132061 | val loss was: 0.256446\n",
      "epoch: 1004/1500 | train loss was: 0.132072 | val loss was: 0.256415\n",
      "epoch: 1005/1500 | train loss was: 0.132082 | val loss was: 0.256383\n",
      "epoch: 1006/1500 | train loss was: 0.132093 | val loss was: 0.25635\n",
      "epoch: 1007/1500 | train loss was: 0.132103 | val loss was: 0.256319\n",
      "epoch: 1008/1500 | train loss was: 0.132114 | val loss was: 0.256287\n",
      "epoch: 1009/1500 | train loss was: 0.132124 | val loss was: 0.256255\n",
      "epoch: 1010/1500 | train loss was: 0.132135 | val loss was: 0.256223\n",
      "epoch: 1011/1500 | train loss was: 0.132145 | val loss was: 0.256191\n",
      "epoch: 1012/1500 | train loss was: 0.132155 | val loss was: 0.256159\n",
      "epoch: 1013/1500 | train loss was: 0.132166 | val loss was: 0.256128\n",
      "epoch: 1014/1500 | train loss was: 0.132176 | val loss was: 0.256096\n",
      "epoch: 1015/1500 | train loss was: 0.132186 | val loss was: 0.256064\n",
      "epoch: 1016/1500 | train loss was: 0.132196 | val loss was: 0.256033\n",
      "epoch: 1017/1500 | train loss was: 0.132207 | val loss was: 0.256001\n",
      "epoch: 1018/1500 | train loss was: 0.132217 | val loss was: 0.255969\n",
      "epoch: 1019/1500 | train loss was: 0.132227 | val loss was: 0.255938\n",
      "epoch: 1020/1500 | train loss was: 0.132237 | val loss was: 0.255907\n",
      "epoch: 1021/1500 | train loss was: 0.132247 | val loss was: 0.255875\n",
      "epoch: 1022/1500 | train loss was: 0.132257 | val loss was: 0.255844\n",
      "epoch: 1023/1500 | train loss was: 0.132267 | val loss was: 0.255812\n",
      "epoch: 1024/1500 | train loss was: 0.132277 | val loss was: 0.255781\n",
      "epoch: 1025/1500 | train loss was: 0.132287 | val loss was: 0.25575\n",
      "epoch: 1026/1500 | train loss was: 0.132297 | val loss was: 0.255718\n",
      "epoch: 1027/1500 | train loss was: 0.132306 | val loss was: 0.255687\n",
      "epoch: 1028/1500 | train loss was: 0.132316 | val loss was: 0.255656\n",
      "epoch: 1029/1500 | train loss was: 0.132326 | val loss was: 0.255624\n",
      "epoch: 1030/1500 | train loss was: 0.132336 | val loss was: 0.255593\n",
      "epoch: 1031/1500 | train loss was: 0.132345 | val loss was: 0.255562\n",
      "epoch: 1032/1500 | train loss was: 0.132355 | val loss was: 0.255531\n",
      "epoch: 1033/1500 | train loss was: 0.132365 | val loss was: 0.255499\n",
      "epoch: 1034/1500 | train loss was: 0.132374 | val loss was: 0.255469\n",
      "epoch: 1035/1500 | train loss was: 0.132384 | val loss was: 0.255437\n",
      "epoch: 1036/1500 | train loss was: 0.132394 | val loss was: 0.255407\n",
      "epoch: 1037/1500 | train loss was: 0.132403 | val loss was: 0.255375\n",
      "epoch: 1038/1500 | train loss was: 0.132412 | val loss was: 0.255344\n",
      "epoch: 1039/1500 | train loss was: 0.132422 | val loss was: 0.255313\n",
      "epoch: 1040/1500 | train loss was: 0.132431 | val loss was: 0.255282\n",
      "epoch: 1041/1500 | train loss was: 0.13244 | val loss was: 0.255251\n",
      "epoch: 1042/1500 | train loss was: 0.13245 | val loss was: 0.25522\n",
      "epoch: 1043/1500 | train loss was: 0.132459 | val loss was: 0.255189\n",
      "epoch: 1044/1500 | train loss was: 0.132468 | val loss was: 0.255158\n",
      "epoch: 1045/1500 | train loss was: 0.132478 | val loss was: 0.255127\n",
      "epoch: 1046/1500 | train loss was: 0.132487 | val loss was: 0.255096\n",
      "epoch: 1047/1500 | train loss was: 0.132496 | val loss was: 0.255065\n",
      "epoch: 1048/1500 | train loss was: 0.132505 | val loss was: 0.255035\n",
      "epoch: 1049/1500 | train loss was: 0.132514 | val loss was: 0.255004\n",
      "epoch: 1050/1500 | train loss was: 0.132523 | val loss was: 0.254973\n",
      "epoch: 1051/1500 | train loss was: 0.132532 | val loss was: 0.254942\n",
      "epoch: 1052/1500 | train loss was: 0.132541 | val loss was: 0.254911\n",
      "epoch: 1053/1500 | train loss was: 0.13255 | val loss was: 0.254881\n",
      "epoch: 1054/1500 | train loss was: 0.132559 | val loss was: 0.25485\n",
      "epoch: 1055/1500 | train loss was: 0.132567 | val loss was: 0.254819\n",
      "epoch: 1056/1500 | train loss was: 0.132576 | val loss was: 0.254788\n",
      "epoch: 1057/1500 | train loss was: 0.132585 | val loss was: 0.254757\n",
      "epoch: 1058/1500 | train loss was: 0.132594 | val loss was: 0.254727\n",
      "epoch: 1059/1500 | train loss was: 0.132602 | val loss was: 0.254696\n",
      "epoch: 1060/1500 | train loss was: 0.132611 | val loss was: 0.254665\n",
      "epoch: 1061/1500 | train loss was: 0.132619 | val loss was: 0.254634\n",
      "epoch: 1062/1500 | train loss was: 0.132628 | val loss was: 0.254604\n",
      "epoch: 1063/1500 | train loss was: 0.132637 | val loss was: 0.254573\n",
      "epoch: 1064/1500 | train loss was: 0.132645 | val loss was: 0.254542\n",
      "epoch: 1065/1500 | train loss was: 0.132654 | val loss was: 0.254512\n",
      "epoch: 1066/1500 | train loss was: 0.132662 | val loss was: 0.254481\n",
      "epoch: 1067/1500 | train loss was: 0.132671 | val loss was: 0.254451\n",
      "epoch: 1068/1500 | train loss was: 0.132679 | val loss was: 0.25442\n",
      "epoch: 1069/1500 | train loss was: 0.132687 | val loss was: 0.254389\n",
      "epoch: 1070/1500 | train loss was: 0.132695 | val loss was: 0.254359\n",
      "epoch: 1071/1500 | train loss was: 0.132704 | val loss was: 0.254328\n",
      "epoch: 1072/1500 | train loss was: 0.132712 | val loss was: 0.254298\n",
      "epoch: 1073/1500 | train loss was: 0.13272 | val loss was: 0.254267\n",
      "epoch: 1074/1500 | train loss was: 0.132728 | val loss was: 0.254237\n",
      "epoch: 1075/1500 | train loss was: 0.132736 | val loss was: 0.254206\n",
      "epoch: 1076/1500 | train loss was: 0.132744 | val loss was: 0.254176\n",
      "epoch: 1077/1500 | train loss was: 0.132752 | val loss was: 0.254145\n",
      "epoch: 1078/1500 | train loss was: 0.13276 | val loss was: 0.254115\n",
      "epoch: 1079/1500 | train loss was: 0.132768 | val loss was: 0.254084\n",
      "epoch: 1080/1500 | train loss was: 0.132776 | val loss was: 0.254053\n",
      "epoch: 1081/1500 | train loss was: 0.132784 | val loss was: 0.254023\n",
      "epoch: 1082/1500 | train loss was: 0.132792 | val loss was: 0.253992\n",
      "epoch: 1083/1500 | train loss was: 0.132799 | val loss was: 0.253962\n",
      "epoch: 1084/1500 | train loss was: 0.132807 | val loss was: 0.253932\n",
      "epoch: 1085/1500 | train loss was: 0.132815 | val loss was: 0.253901\n",
      "epoch: 1086/1500 | train loss was: 0.132823 | val loss was: 0.253871\n",
      "epoch: 1087/1500 | train loss was: 0.13283 | val loss was: 0.25384\n",
      "epoch: 1088/1500 | train loss was: 0.132838 | val loss was: 0.25381\n",
      "epoch: 1089/1500 | train loss was: 0.132846 | val loss was: 0.253779\n",
      "epoch: 1090/1500 | train loss was: 0.132853 | val loss was: 0.253749\n",
      "epoch: 1091/1500 | train loss was: 0.13286 | val loss was: 0.253718\n",
      "epoch: 1092/1500 | train loss was: 0.132868 | val loss was: 0.253688\n",
      "epoch: 1093/1500 | train loss was: 0.132875 | val loss was: 0.253657\n",
      "epoch: 1094/1500 | train loss was: 0.132883 | val loss was: 0.253627\n",
      "epoch: 1095/1500 | train loss was: 0.13289 | val loss was: 0.253597\n",
      "epoch: 1096/1500 | train loss was: 0.132897 | val loss was: 0.253566\n",
      "epoch: 1097/1500 | train loss was: 0.132905 | val loss was: 0.253536\n",
      "epoch: 1098/1500 | train loss was: 0.132912 | val loss was: 0.253505\n",
      "epoch: 1099/1500 | train loss was: 0.132919 | val loss was: 0.253475\n",
      "epoch: 1100/1500 | train loss was: 0.132926 | val loss was: 0.253445\n",
      "epoch: 1101/1500 | train loss was: 0.132933 | val loss was: 0.253414\n",
      "epoch: 1102/1500 | train loss was: 0.132941 | val loss was: 0.253384\n",
      "epoch: 1103/1500 | train loss was: 0.132948 | val loss was: 0.253354\n",
      "epoch: 1104/1500 | train loss was: 0.132955 | val loss was: 0.253323\n",
      "epoch: 1105/1500 | train loss was: 0.132962 | val loss was: 0.253293\n",
      "epoch: 1106/1500 | train loss was: 0.132969 | val loss was: 0.253262\n",
      "epoch: 1107/1500 | train loss was: 0.132976 | val loss was: 0.253232\n",
      "epoch: 1108/1500 | train loss was: 0.132982 | val loss was: 0.253202\n",
      "epoch: 1109/1500 | train loss was: 0.132989 | val loss was: 0.253171\n",
      "epoch: 1110/1500 | train loss was: 0.132996 | val loss was: 0.253141\n",
      "epoch: 1111/1500 | train loss was: 0.133003 | val loss was: 0.253111\n",
      "epoch: 1112/1500 | train loss was: 0.13301 | val loss was: 0.253081\n",
      "epoch: 1113/1500 | train loss was: 0.133016 | val loss was: 0.25305\n",
      "epoch: 1114/1500 | train loss was: 0.133023 | val loss was: 0.25302\n",
      "epoch: 1115/1500 | train loss was: 0.13303 | val loss was: 0.25299\n",
      "epoch: 1116/1500 | train loss was: 0.133036 | val loss was: 0.252959\n",
      "epoch: 1117/1500 | train loss was: 0.133043 | val loss was: 0.252929\n",
      "epoch: 1118/1500 | train loss was: 0.133049 | val loss was: 0.252899\n",
      "epoch: 1119/1500 | train loss was: 0.133056 | val loss was: 0.252868\n",
      "epoch: 1120/1500 | train loss was: 0.133062 | val loss was: 0.252838\n",
      "epoch: 1121/1500 | train loss was: 0.133069 | val loss was: 0.252808\n",
      "epoch: 1122/1500 | train loss was: 0.133075 | val loss was: 0.252777\n",
      "epoch: 1123/1500 | train loss was: 0.133082 | val loss was: 0.252747\n",
      "epoch: 1124/1500 | train loss was: 0.133088 | val loss was: 0.252717\n",
      "epoch: 1125/1500 | train loss was: 0.133094 | val loss was: 0.252687\n",
      "epoch: 1126/1500 | train loss was: 0.133101 | val loss was: 0.252656\n",
      "epoch: 1127/1500 | train loss was: 0.133107 | val loss was: 0.252626\n",
      "epoch: 1128/1500 | train loss was: 0.133113 | val loss was: 0.252596\n",
      "epoch: 1129/1500 | train loss was: 0.133119 | val loss was: 0.252565\n",
      "epoch: 1130/1500 | train loss was: 0.133125 | val loss was: 0.252535\n",
      "epoch: 1131/1500 | train loss was: 0.133131 | val loss was: 0.252505\n",
      "epoch: 1132/1500 | train loss was: 0.133137 | val loss was: 0.252474\n",
      "epoch: 1133/1500 | train loss was: 0.133143 | val loss was: 0.252444\n",
      "epoch: 1134/1500 | train loss was: 0.133149 | val loss was: 0.252414\n",
      "epoch: 1135/1500 | train loss was: 0.133155 | val loss was: 0.252384\n",
      "epoch: 1136/1500 | train loss was: 0.133161 | val loss was: 0.252353\n",
      "epoch: 1137/1500 | train loss was: 0.133167 | val loss was: 0.252323\n",
      "epoch: 1138/1500 | train loss was: 0.133173 | val loss was: 0.252293\n",
      "epoch: 1139/1500 | train loss was: 0.133179 | val loss was: 0.252263\n",
      "epoch: 1140/1500 | train loss was: 0.133185 | val loss was: 0.252233\n",
      "epoch: 1141/1500 | train loss was: 0.133191 | val loss was: 0.252202\n",
      "epoch: 1142/1500 | train loss was: 0.133196 | val loss was: 0.252172\n",
      "epoch: 1143/1500 | train loss was: 0.133202 | val loss was: 0.252142\n",
      "epoch: 1144/1500 | train loss was: 0.133208 | val loss was: 0.252111\n",
      "epoch: 1145/1500 | train loss was: 0.133213 | val loss was: 0.252081\n",
      "epoch: 1146/1500 | train loss was: 0.133219 | val loss was: 0.252051\n",
      "epoch: 1147/1500 | train loss was: 0.133225 | val loss was: 0.252021\n",
      "epoch: 1148/1500 | train loss was: 0.13323 | val loss was: 0.251991\n",
      "epoch: 1149/1500 | train loss was: 0.133236 | val loss was: 0.25196\n",
      "epoch: 1150/1500 | train loss was: 0.133241 | val loss was: 0.25193\n",
      "epoch: 1151/1500 | train loss was: 0.133247 | val loss was: 0.2519\n",
      "epoch: 1152/1500 | train loss was: 0.133252 | val loss was: 0.25187\n",
      "epoch: 1153/1500 | train loss was: 0.133257 | val loss was: 0.25184\n",
      "epoch: 1154/1500 | train loss was: 0.133263 | val loss was: 0.251809\n",
      "epoch: 1155/1500 | train loss was: 0.133268 | val loss was: 0.251779\n",
      "epoch: 1156/1500 | train loss was: 0.133273 | val loss was: 0.251749\n",
      "epoch: 1157/1500 | train loss was: 0.133279 | val loss was: 0.251719\n",
      "epoch: 1158/1500 | train loss was: 0.133284 | val loss was: 0.251689\n",
      "epoch: 1159/1500 | train loss was: 0.133289 | val loss was: 0.251659\n",
      "epoch: 1160/1500 | train loss was: 0.133294 | val loss was: 0.251628\n",
      "epoch: 1161/1500 | train loss was: 0.133299 | val loss was: 0.251598\n",
      "epoch: 1162/1500 | train loss was: 0.133305 | val loss was: 0.251568\n",
      "epoch: 1163/1500 | train loss was: 0.13331 | val loss was: 0.251538\n",
      "epoch: 1164/1500 | train loss was: 0.133315 | val loss was: 0.251508\n",
      "epoch: 1165/1500 | train loss was: 0.13332 | val loss was: 0.251478\n",
      "epoch: 1166/1500 | train loss was: 0.133325 | val loss was: 0.251447\n",
      "epoch: 1167/1500 | train loss was: 0.13333 | val loss was: 0.251417\n",
      "epoch: 1168/1500 | train loss was: 0.133335 | val loss was: 0.251387\n",
      "epoch: 1169/1500 | train loss was: 0.13334 | val loss was: 0.251357\n",
      "epoch: 1170/1500 | train loss was: 0.133345 | val loss was: 0.251327\n",
      "epoch: 1171/1500 | train loss was: 0.13335 | val loss was: 0.251297\n",
      "epoch: 1172/1500 | train loss was: 0.133354 | val loss was: 0.251267\n",
      "epoch: 1173/1500 | train loss was: 0.133359 | val loss was: 0.251237\n",
      "epoch: 1174/1500 | train loss was: 0.133364 | val loss was: 0.251207\n",
      "epoch: 1175/1500 | train loss was: 0.133369 | val loss was: 0.251176\n",
      "epoch: 1176/1500 | train loss was: 0.133373 | val loss was: 0.251147\n",
      "epoch: 1177/1500 | train loss was: 0.133378 | val loss was: 0.251116\n",
      "epoch: 1178/1500 | train loss was: 0.133383 | val loss was: 0.251086\n",
      "epoch: 1179/1500 | train loss was: 0.133387 | val loss was: 0.251056\n",
      "epoch: 1180/1500 | train loss was: 0.133392 | val loss was: 0.251026\n",
      "epoch: 1181/1500 | train loss was: 0.133397 | val loss was: 0.250996\n",
      "epoch: 1182/1500 | train loss was: 0.133401 | val loss was: 0.250966\n",
      "epoch: 1183/1500 | train loss was: 0.133406 | val loss was: 0.250936\n",
      "epoch: 1184/1500 | train loss was: 0.13341 | val loss was: 0.250906\n",
      "epoch: 1185/1500 | train loss was: 0.133415 | val loss was: 0.250876\n",
      "epoch: 1186/1500 | train loss was: 0.133419 | val loss was: 0.250846\n",
      "epoch: 1187/1500 | train loss was: 0.133424 | val loss was: 0.250816\n",
      "epoch: 1188/1500 | train loss was: 0.133428 | val loss was: 0.250786\n",
      "epoch: 1189/1500 | train loss was: 0.133432 | val loss was: 0.250756\n",
      "epoch: 1190/1500 | train loss was: 0.133437 | val loss was: 0.250726\n",
      "epoch: 1191/1500 | train loss was: 0.133441 | val loss was: 0.250696\n",
      "epoch: 1192/1500 | train loss was: 0.133445 | val loss was: 0.250666\n",
      "epoch: 1193/1500 | train loss was: 0.13345 | val loss was: 0.250636\n",
      "epoch: 1194/1500 | train loss was: 0.133454 | val loss was: 0.250606\n",
      "epoch: 1195/1500 | train loss was: 0.133458 | val loss was: 0.250576\n",
      "epoch: 1196/1500 | train loss was: 0.133462 | val loss was: 0.250546\n",
      "epoch: 1197/1500 | train loss was: 0.133466 | val loss was: 0.250516\n",
      "epoch: 1198/1500 | train loss was: 0.13347 | val loss was: 0.250486\n",
      "epoch: 1199/1500 | train loss was: 0.133475 | val loss was: 0.250456\n",
      "epoch: 1200/1500 | train loss was: 0.133479 | val loss was: 0.250426\n",
      "epoch: 1201/1500 | train loss was: 0.133483 | val loss was: 0.250396\n",
      "epoch: 1202/1500 | train loss was: 0.133487 | val loss was: 0.250366\n",
      "epoch: 1203/1500 | train loss was: 0.133491 | val loss was: 0.250337\n",
      "epoch: 1204/1500 | train loss was: 0.133495 | val loss was: 0.250307\n",
      "epoch: 1205/1500 | train loss was: 0.133499 | val loss was: 0.250276\n",
      "epoch: 1206/1500 | train loss was: 0.133503 | val loss was: 0.250247\n",
      "epoch: 1207/1500 | train loss was: 0.133507 | val loss was: 0.250217\n",
      "epoch: 1208/1500 | train loss was: 0.133511 | val loss was: 0.250187\n",
      "epoch: 1209/1500 | train loss was: 0.133514 | val loss was: 0.250157\n",
      "epoch: 1210/1500 | train loss was: 0.133518 | val loss was: 0.250127\n",
      "epoch: 1211/1500 | train loss was: 0.133522 | val loss was: 0.250097\n",
      "epoch: 1212/1500 | train loss was: 0.133526 | val loss was: 0.250068\n",
      "epoch: 1213/1500 | train loss was: 0.13353 | val loss was: 0.250038\n",
      "epoch: 1214/1500 | train loss was: 0.133534 | val loss was: 0.250008\n",
      "epoch: 1215/1500 | train loss was: 0.133537 | val loss was: 0.249978\n",
      "epoch: 1216/1500 | train loss was: 0.133541 | val loss was: 0.249948\n",
      "epoch: 1217/1500 | train loss was: 0.133545 | val loss was: 0.249919\n",
      "epoch: 1218/1500 | train loss was: 0.133548 | val loss was: 0.249889\n",
      "epoch: 1219/1500 | train loss was: 0.133552 | val loss was: 0.249859\n",
      "epoch: 1220/1500 | train loss was: 0.133555 | val loss was: 0.249829\n",
      "epoch: 1221/1500 | train loss was: 0.133559 | val loss was: 0.2498\n",
      "epoch: 1222/1500 | train loss was: 0.133563 | val loss was: 0.24977\n",
      "epoch: 1223/1500 | train loss was: 0.133566 | val loss was: 0.24974\n",
      "epoch: 1224/1500 | train loss was: 0.13357 | val loss was: 0.24971\n",
      "epoch: 1225/1500 | train loss was: 0.133573 | val loss was: 0.249681\n",
      "epoch: 1226/1500 | train loss was: 0.133577 | val loss was: 0.249651\n",
      "epoch: 1227/1500 | train loss was: 0.13358 | val loss was: 0.249621\n",
      "epoch: 1228/1500 | train loss was: 0.133584 | val loss was: 0.249591\n",
      "epoch: 1229/1500 | train loss was: 0.133587 | val loss was: 0.249562\n",
      "epoch: 1230/1500 | train loss was: 0.13359 | val loss was: 0.249532\n",
      "epoch: 1231/1500 | train loss was: 0.133594 | val loss was: 0.249502\n",
      "epoch: 1232/1500 | train loss was: 0.133597 | val loss was: 0.249473\n",
      "epoch: 1233/1500 | train loss was: 0.1336 | val loss was: 0.249443\n",
      "epoch: 1234/1500 | train loss was: 0.133604 | val loss was: 0.249414\n",
      "epoch: 1235/1500 | train loss was: 0.133607 | val loss was: 0.249384\n",
      "epoch: 1236/1500 | train loss was: 0.13361 | val loss was: 0.249354\n",
      "epoch: 1237/1500 | train loss was: 0.133614 | val loss was: 0.249325\n",
      "epoch: 1238/1500 | train loss was: 0.133617 | val loss was: 0.249295\n",
      "epoch: 1239/1500 | train loss was: 0.13362 | val loss was: 0.249266\n",
      "epoch: 1240/1500 | train loss was: 0.133623 | val loss was: 0.249236\n",
      "epoch: 1241/1500 | train loss was: 0.133626 | val loss was: 0.249206\n",
      "epoch: 1242/1500 | train loss was: 0.133629 | val loss was: 0.249177\n",
      "epoch: 1243/1500 | train loss was: 0.133633 | val loss was: 0.249147\n",
      "epoch: 1244/1500 | train loss was: 0.133636 | val loss was: 0.249118\n",
      "epoch: 1245/1500 | train loss was: 0.133639 | val loss was: 0.249088\n",
      "epoch: 1246/1500 | train loss was: 0.133642 | val loss was: 0.249059\n",
      "epoch: 1247/1500 | train loss was: 0.133645 | val loss was: 0.24903\n",
      "epoch: 1248/1500 | train loss was: 0.133648 | val loss was: 0.249\n",
      "epoch: 1249/1500 | train loss was: 0.133651 | val loss was: 0.248971\n",
      "epoch: 1250/1500 | train loss was: 0.133654 | val loss was: 0.248941\n",
      "epoch: 1251/1500 | train loss was: 0.133657 | val loss was: 0.248912\n",
      "epoch: 1252/1500 | train loss was: 0.13366 | val loss was: 0.248882\n",
      "epoch: 1253/1500 | train loss was: 0.133663 | val loss was: 0.248853\n",
      "epoch: 1254/1500 | train loss was: 0.133666 | val loss was: 0.248823\n",
      "epoch: 1255/1500 | train loss was: 0.133669 | val loss was: 0.248794\n",
      "epoch: 1256/1500 | train loss was: 0.133672 | val loss was: 0.248765\n",
      "epoch: 1257/1500 | train loss was: 0.133674 | val loss was: 0.248735\n",
      "epoch: 1258/1500 | train loss was: 0.133677 | val loss was: 0.248706\n",
      "epoch: 1259/1500 | train loss was: 0.13368 | val loss was: 0.248677\n",
      "epoch: 1260/1500 | train loss was: 0.133683 | val loss was: 0.248647\n",
      "epoch: 1261/1500 | train loss was: 0.133686 | val loss was: 0.248618\n",
      "epoch: 1262/1500 | train loss was: 0.133688 | val loss was: 0.248589\n",
      "epoch: 1263/1500 | train loss was: 0.133691 | val loss was: 0.248559\n",
      "epoch: 1264/1500 | train loss was: 0.133694 | val loss was: 0.24853\n",
      "epoch: 1265/1500 | train loss was: 0.133696 | val loss was: 0.248501\n",
      "epoch: 1266/1500 | train loss was: 0.133699 | val loss was: 0.248472\n",
      "epoch: 1267/1500 | train loss was: 0.133702 | val loss was: 0.248443\n",
      "epoch: 1268/1500 | train loss was: 0.133705 | val loss was: 0.248413\n",
      "epoch: 1269/1500 | train loss was: 0.133707 | val loss was: 0.248384\n",
      "epoch: 1270/1500 | train loss was: 0.13371 | val loss was: 0.248355\n",
      "epoch: 1271/1500 | train loss was: 0.133712 | val loss was: 0.248326\n",
      "epoch: 1272/1500 | train loss was: 0.133715 | val loss was: 0.248297\n",
      "epoch: 1273/1500 | train loss was: 0.133718 | val loss was: 0.248268\n",
      "epoch: 1274/1500 | train loss was: 0.13372 | val loss was: 0.248239\n",
      "epoch: 1275/1500 | train loss was: 0.133723 | val loss was: 0.248209\n",
      "epoch: 1276/1500 | train loss was: 0.133725 | val loss was: 0.24818\n",
      "epoch: 1277/1500 | train loss was: 0.133728 | val loss was: 0.248151\n",
      "epoch: 1278/1500 | train loss was: 0.13373 | val loss was: 0.248122\n",
      "epoch: 1279/1500 | train loss was: 0.133733 | val loss was: 0.248093\n",
      "epoch: 1280/1500 | train loss was: 0.133735 | val loss was: 0.248064\n",
      "epoch: 1281/1500 | train loss was: 0.133738 | val loss was: 0.248035\n",
      "epoch: 1282/1500 | train loss was: 0.13374 | val loss was: 0.248006\n",
      "epoch: 1283/1500 | train loss was: 0.133743 | val loss was: 0.247977\n",
      "epoch: 1284/1500 | train loss was: 0.133745 | val loss was: 0.247948\n",
      "epoch: 1285/1500 | train loss was: 0.133747 | val loss was: 0.247919\n",
      "epoch: 1286/1500 | train loss was: 0.13375 | val loss was: 0.24789\n",
      "epoch: 1287/1500 | train loss was: 0.133752 | val loss was: 0.247861\n",
      "epoch: 1288/1500 | train loss was: 0.133754 | val loss was: 0.247832\n",
      "epoch: 1289/1500 | train loss was: 0.133757 | val loss was: 0.247804\n",
      "epoch: 1290/1500 | train loss was: 0.133759 | val loss was: 0.247775\n",
      "epoch: 1291/1500 | train loss was: 0.133761 | val loss was: 0.247746\n",
      "epoch: 1292/1500 | train loss was: 0.133763 | val loss was: 0.247717\n",
      "epoch: 1293/1500 | train loss was: 0.133766 | val loss was: 0.247688\n",
      "epoch: 1294/1500 | train loss was: 0.133768 | val loss was: 0.247659\n",
      "epoch: 1295/1500 | train loss was: 0.13377 | val loss was: 0.247631\n",
      "epoch: 1296/1500 | train loss was: 0.133772 | val loss was: 0.247602\n",
      "epoch: 1297/1500 | train loss was: 0.133775 | val loss was: 0.247573\n",
      "epoch: 1298/1500 | train loss was: 0.133777 | val loss was: 0.247544\n",
      "epoch: 1299/1500 | train loss was: 0.133779 | val loss was: 0.247516\n",
      "epoch: 1300/1500 | train loss was: 0.133781 | val loss was: 0.247487\n",
      "epoch: 1301/1500 | train loss was: 0.133783 | val loss was: 0.247458\n",
      "epoch: 1302/1500 | train loss was: 0.133785 | val loss was: 0.247429\n",
      "epoch: 1303/1500 | train loss was: 0.133788 | val loss was: 0.247401\n",
      "epoch: 1304/1500 | train loss was: 0.13379 | val loss was: 0.247372\n",
      "epoch: 1305/1500 | train loss was: 0.133792 | val loss was: 0.247344\n",
      "epoch: 1306/1500 | train loss was: 0.133794 | val loss was: 0.247315\n",
      "epoch: 1307/1500 | train loss was: 0.133796 | val loss was: 0.247286\n",
      "epoch: 1308/1500 | train loss was: 0.133798 | val loss was: 0.247258\n",
      "epoch: 1309/1500 | train loss was: 0.1338 | val loss was: 0.247229\n",
      "epoch: 1310/1500 | train loss was: 0.133802 | val loss was: 0.247201\n",
      "epoch: 1311/1500 | train loss was: 0.133804 | val loss was: 0.247172\n",
      "epoch: 1312/1500 | train loss was: 0.133806 | val loss was: 0.247144\n",
      "epoch: 1313/1500 | train loss was: 0.133808 | val loss was: 0.247115\n",
      "epoch: 1314/1500 | train loss was: 0.13381 | val loss was: 0.247087\n",
      "epoch: 1315/1500 | train loss was: 0.133812 | val loss was: 0.247058\n",
      "epoch: 1316/1500 | train loss was: 0.133814 | val loss was: 0.24703\n",
      "epoch: 1317/1500 | train loss was: 0.133816 | val loss was: 0.247002\n",
      "epoch: 1318/1500 | train loss was: 0.133818 | val loss was: 0.246973\n",
      "epoch: 1319/1500 | train loss was: 0.13382 | val loss was: 0.246945\n",
      "epoch: 1320/1500 | train loss was: 0.133821 | val loss was: 0.246916\n",
      "epoch: 1321/1500 | train loss was: 0.133823 | val loss was: 0.246888\n",
      "epoch: 1322/1500 | train loss was: 0.133825 | val loss was: 0.24686\n",
      "epoch: 1323/1500 | train loss was: 0.133827 | val loss was: 0.246832\n",
      "epoch: 1324/1500 | train loss was: 0.133829 | val loss was: 0.246803\n",
      "epoch: 1325/1500 | train loss was: 0.133831 | val loss was: 0.246775\n",
      "epoch: 1326/1500 | train loss was: 0.133832 | val loss was: 0.246747\n",
      "epoch: 1327/1500 | train loss was: 0.133834 | val loss was: 0.246718\n",
      "epoch: 1328/1500 | train loss was: 0.133836 | val loss was: 0.24669\n",
      "epoch: 1329/1500 | train loss was: 0.133838 | val loss was: 0.246662\n",
      "epoch: 1330/1500 | train loss was: 0.13384 | val loss was: 0.246634\n",
      "epoch: 1331/1500 | train loss was: 0.133841 | val loss was: 0.246606\n",
      "epoch: 1332/1500 | train loss was: 0.133843 | val loss was: 0.246578\n",
      "epoch: 1333/1500 | train loss was: 0.133845 | val loss was: 0.246549\n",
      "epoch: 1334/1500 | train loss was: 0.133847 | val loss was: 0.246521\n",
      "epoch: 1335/1500 | train loss was: 0.133848 | val loss was: 0.246493\n",
      "epoch: 1336/1500 | train loss was: 0.13385 | val loss was: 0.246465\n",
      "epoch: 1337/1500 | train loss was: 0.133852 | val loss was: 0.246437\n",
      "epoch: 1338/1500 | train loss was: 0.133853 | val loss was: 0.246409\n",
      "epoch: 1339/1500 | train loss was: 0.133855 | val loss was: 0.246381\n",
      "epoch: 1340/1500 | train loss was: 0.133857 | val loss was: 0.246353\n",
      "epoch: 1341/1500 | train loss was: 0.133858 | val loss was: 0.246325\n",
      "epoch: 1342/1500 | train loss was: 0.13386 | val loss was: 0.246297\n",
      "epoch: 1343/1500 | train loss was: 0.133862 | val loss was: 0.246269\n",
      "epoch: 1344/1500 | train loss was: 0.133863 | val loss was: 0.246241\n",
      "epoch: 1345/1500 | train loss was: 0.133865 | val loss was: 0.246213\n",
      "epoch: 1346/1500 | train loss was: 0.133866 | val loss was: 0.246186\n",
      "epoch: 1347/1500 | train loss was: 0.133868 | val loss was: 0.246158\n",
      "epoch: 1348/1500 | train loss was: 0.13387 | val loss was: 0.24613\n",
      "epoch: 1349/1500 | train loss was: 0.133871 | val loss was: 0.246102\n",
      "epoch: 1350/1500 | train loss was: 0.133873 | val loss was: 0.246074\n",
      "epoch: 1351/1500 | train loss was: 0.133874 | val loss was: 0.246047\n",
      "epoch: 1352/1500 | train loss was: 0.133876 | val loss was: 0.246019\n",
      "epoch: 1353/1500 | train loss was: 0.133877 | val loss was: 0.245991\n",
      "epoch: 1354/1500 | train loss was: 0.133879 | val loss was: 0.245963\n",
      "epoch: 1355/1500 | train loss was: 0.13388 | val loss was: 0.245936\n",
      "epoch: 1356/1500 | train loss was: 0.133882 | val loss was: 0.245908\n",
      "epoch: 1357/1500 | train loss was: 0.133883 | val loss was: 0.24588\n",
      "epoch: 1358/1500 | train loss was: 0.133885 | val loss was: 0.245853\n",
      "epoch: 1359/1500 | train loss was: 0.133886 | val loss was: 0.245825\n",
      "epoch: 1360/1500 | train loss was: 0.133888 | val loss was: 0.245798\n",
      "epoch: 1361/1500 | train loss was: 0.133889 | val loss was: 0.24577\n",
      "epoch: 1362/1500 | train loss was: 0.13389 | val loss was: 0.245743\n",
      "epoch: 1363/1500 | train loss was: 0.133892 | val loss was: 0.245715\n",
      "epoch: 1364/1500 | train loss was: 0.133893 | val loss was: 0.245687\n",
      "epoch: 1365/1500 | train loss was: 0.133895 | val loss was: 0.24566\n",
      "epoch: 1366/1500 | train loss was: 0.133896 | val loss was: 0.245632\n",
      "epoch: 1367/1500 | train loss was: 0.133897 | val loss was: 0.245605\n",
      "epoch: 1368/1500 | train loss was: 0.133899 | val loss was: 0.245578\n",
      "epoch: 1369/1500 | train loss was: 0.1339 | val loss was: 0.24555\n",
      "epoch: 1370/1500 | train loss was: 0.133901 | val loss was: 0.245523\n",
      "epoch: 1371/1500 | train loss was: 0.133903 | val loss was: 0.245496\n",
      "epoch: 1372/1500 | train loss was: 0.133904 | val loss was: 0.245468\n",
      "epoch: 1373/1500 | train loss was: 0.133905 | val loss was: 0.245441\n",
      "epoch: 1374/1500 | train loss was: 0.133907 | val loss was: 0.245414\n",
      "epoch: 1375/1500 | train loss was: 0.133908 | val loss was: 0.245386\n",
      "epoch: 1376/1500 | train loss was: 0.133909 | val loss was: 0.245359\n",
      "epoch: 1377/1500 | train loss was: 0.133911 | val loss was: 0.245332\n",
      "epoch: 1378/1500 | train loss was: 0.133912 | val loss was: 0.245305\n",
      "epoch: 1379/1500 | train loss was: 0.133913 | val loss was: 0.245277\n",
      "epoch: 1380/1500 | train loss was: 0.133915 | val loss was: 0.24525\n",
      "epoch: 1381/1500 | train loss was: 0.133916 | val loss was: 0.245223\n",
      "epoch: 1382/1500 | train loss was: 0.133917 | val loss was: 0.245196\n",
      "epoch: 1383/1500 | train loss was: 0.133918 | val loss was: 0.245169\n",
      "epoch: 1384/1500 | train loss was: 0.133919 | val loss was: 0.245142\n",
      "epoch: 1385/1500 | train loss was: 0.133921 | val loss was: 0.245115\n",
      "epoch: 1386/1500 | train loss was: 0.133922 | val loss was: 0.245088\n",
      "epoch: 1387/1500 | train loss was: 0.133923 | val loss was: 0.245061\n",
      "epoch: 1388/1500 | train loss was: 0.133924 | val loss was: 0.245034\n",
      "epoch: 1389/1500 | train loss was: 0.133925 | val loss was: 0.245007\n",
      "epoch: 1390/1500 | train loss was: 0.133927 | val loss was: 0.24498\n",
      "epoch: 1391/1500 | train loss was: 0.133928 | val loss was: 0.244953\n",
      "epoch: 1392/1500 | train loss was: 0.133929 | val loss was: 0.244926\n",
      "epoch: 1393/1500 | train loss was: 0.13393 | val loss was: 0.244899\n",
      "epoch: 1394/1500 | train loss was: 0.133931 | val loss was: 0.244872\n",
      "epoch: 1395/1500 | train loss was: 0.133932 | val loss was: 0.244845\n",
      "epoch: 1396/1500 | train loss was: 0.133934 | val loss was: 0.244819\n",
      "epoch: 1397/1500 | train loss was: 0.133935 | val loss was: 0.244792\n",
      "epoch: 1398/1500 | train loss was: 0.133936 | val loss was: 0.244765\n",
      "epoch: 1399/1500 | train loss was: 0.133937 | val loss was: 0.244738\n",
      "epoch: 1400/1500 | train loss was: 0.133938 | val loss was: 0.244711\n",
      "epoch: 1401/1500 | train loss was: 0.133939 | val loss was: 0.244685\n",
      "epoch: 1402/1500 | train loss was: 0.13394 | val loss was: 0.244658\n",
      "epoch: 1403/1500 | train loss was: 0.133941 | val loss was: 0.244631\n",
      "epoch: 1404/1500 | train loss was: 0.133942 | val loss was: 0.244605\n",
      "epoch: 1405/1500 | train loss was: 0.133943 | val loss was: 0.244578\n",
      "epoch: 1406/1500 | train loss was: 0.133945 | val loss was: 0.244552\n",
      "epoch: 1407/1500 | train loss was: 0.133946 | val loss was: 0.244525\n",
      "epoch: 1408/1500 | train loss was: 0.133947 | val loss was: 0.244498\n",
      "epoch: 1409/1500 | train loss was: 0.133948 | val loss was: 0.244472\n",
      "epoch: 1410/1500 | train loss was: 0.133949 | val loss was: 0.244445\n",
      "epoch: 1411/1500 | train loss was: 0.13395 | val loss was: 0.244419\n",
      "epoch: 1412/1500 | train loss was: 0.133951 | val loss was: 0.244392\n",
      "epoch: 1413/1500 | train loss was: 0.133952 | val loss was: 0.244366\n",
      "epoch: 1414/1500 | train loss was: 0.133953 | val loss was: 0.24434\n",
      "epoch: 1415/1500 | train loss was: 0.133954 | val loss was: 0.244313\n",
      "epoch: 1416/1500 | train loss was: 0.133955 | val loss was: 0.244287\n",
      "epoch: 1417/1500 | train loss was: 0.133956 | val loss was: 0.24426\n",
      "epoch: 1418/1500 | train loss was: 0.133957 | val loss was: 0.244234\n",
      "epoch: 1419/1500 | train loss was: 0.133958 | val loss was: 0.244208\n",
      "epoch: 1420/1500 | train loss was: 0.133959 | val loss was: 0.244181\n",
      "epoch: 1421/1500 | train loss was: 0.13396 | val loss was: 0.244155\n",
      "epoch: 1422/1500 | train loss was: 0.13396 | val loss was: 0.244129\n",
      "epoch: 1423/1500 | train loss was: 0.133961 | val loss was: 0.244103\n",
      "epoch: 1424/1500 | train loss was: 0.133962 | val loss was: 0.244076\n",
      "epoch: 1425/1500 | train loss was: 0.133963 | val loss was: 0.24405\n",
      "epoch: 1426/1500 | train loss was: 0.133964 | val loss was: 0.244024\n",
      "epoch: 1427/1500 | train loss was: 0.133965 | val loss was: 0.243998\n",
      "epoch: 1428/1500 | train loss was: 0.133966 | val loss was: 0.243972\n",
      "epoch: 1429/1500 | train loss was: 0.133967 | val loss was: 0.243946\n",
      "epoch: 1430/1500 | train loss was: 0.133968 | val loss was: 0.24392\n",
      "epoch: 1431/1500 | train loss was: 0.133969 | val loss was: 0.243894\n",
      "epoch: 1432/1500 | train loss was: 0.13397 | val loss was: 0.243868\n",
      "epoch: 1433/1500 | train loss was: 0.13397 | val loss was: 0.243842\n",
      "epoch: 1434/1500 | train loss was: 0.133971 | val loss was: 0.243816\n",
      "epoch: 1435/1500 | train loss was: 0.133972 | val loss was: 0.24379\n",
      "epoch: 1436/1500 | train loss was: 0.133973 | val loss was: 0.243764\n",
      "epoch: 1437/1500 | train loss was: 0.133974 | val loss was: 0.243738\n",
      "epoch: 1438/1500 | train loss was: 0.133975 | val loss was: 0.243712\n",
      "epoch: 1439/1500 | train loss was: 0.133976 | val loss was: 0.243686\n",
      "epoch: 1440/1500 | train loss was: 0.133977 | val loss was: 0.24366\n",
      "epoch: 1441/1500 | train loss was: 0.133977 | val loss was: 0.243635\n",
      "epoch: 1442/1500 | train loss was: 0.133978 | val loss was: 0.243609\n",
      "epoch: 1443/1500 | train loss was: 0.133979 | val loss was: 0.243583\n",
      "epoch: 1444/1500 | train loss was: 0.13398 | val loss was: 0.243557\n",
      "epoch: 1445/1500 | train loss was: 0.133981 | val loss was: 0.243531\n",
      "epoch: 1446/1500 | train loss was: 0.133981 | val loss was: 0.243506\n",
      "epoch: 1447/1500 | train loss was: 0.133982 | val loss was: 0.24348\n",
      "epoch: 1448/1500 | train loss was: 0.133983 | val loss was: 0.243455\n",
      "epoch: 1449/1500 | train loss was: 0.133984 | val loss was: 0.243429\n",
      "epoch: 1450/1500 | train loss was: 0.133985 | val loss was: 0.243403\n",
      "epoch: 1451/1500 | train loss was: 0.133985 | val loss was: 0.243378\n",
      "epoch: 1452/1500 | train loss was: 0.133986 | val loss was: 0.243352\n",
      "epoch: 1453/1500 | train loss was: 0.133987 | val loss was: 0.243326\n",
      "epoch: 1454/1500 | train loss was: 0.133988 | val loss was: 0.243301\n",
      "epoch: 1455/1500 | train loss was: 0.133988 | val loss was: 0.243275\n",
      "epoch: 1456/1500 | train loss was: 0.133989 | val loss was: 0.24325\n",
      "epoch: 1457/1500 | train loss was: 0.13399 | val loss was: 0.243225\n",
      "epoch: 1458/1500 | train loss was: 0.133991 | val loss was: 0.243199\n",
      "epoch: 1459/1500 | train loss was: 0.133992 | val loss was: 0.243174\n",
      "epoch: 1460/1500 | train loss was: 0.133992 | val loss was: 0.243148\n",
      "epoch: 1461/1500 | train loss was: 0.133993 | val loss was: 0.243123\n",
      "epoch: 1462/1500 | train loss was: 0.133994 | val loss was: 0.243098\n",
      "epoch: 1463/1500 | train loss was: 0.133994 | val loss was: 0.243072\n",
      "epoch: 1464/1500 | train loss was: 0.133995 | val loss was: 0.243047\n",
      "epoch: 1465/1500 | train loss was: 0.133996 | val loss was: 0.243022\n",
      "epoch: 1466/1500 | train loss was: 0.133997 | val loss was: 0.242997\n",
      "epoch: 1467/1500 | train loss was: 0.133997 | val loss was: 0.242971\n",
      "epoch: 1468/1500 | train loss was: 0.133998 | val loss was: 0.242946\n",
      "epoch: 1469/1500 | train loss was: 0.133999 | val loss was: 0.242921\n",
      "epoch: 1470/1500 | train loss was: 0.133999 | val loss was: 0.242896\n",
      "epoch: 1471/1500 | train loss was: 0.134 | val loss was: 0.242871\n",
      "epoch: 1472/1500 | train loss was: 0.134001 | val loss was: 0.242846\n",
      "epoch: 1473/1500 | train loss was: 0.134002 | val loss was: 0.242821\n",
      "epoch: 1474/1500 | train loss was: 0.134002 | val loss was: 0.242795\n",
      "epoch: 1475/1500 | train loss was: 0.134003 | val loss was: 0.24277\n",
      "epoch: 1476/1500 | train loss was: 0.134003 | val loss was: 0.242745\n",
      "epoch: 1477/1500 | train loss was: 0.134004 | val loss was: 0.24272\n",
      "epoch: 1478/1500 | train loss was: 0.134005 | val loss was: 0.242695\n",
      "epoch: 1479/1500 | train loss was: 0.134005 | val loss was: 0.242671\n",
      "epoch: 1480/1500 | train loss was: 0.134006 | val loss was: 0.242645\n",
      "epoch: 1481/1500 | train loss was: 0.134007 | val loss was: 0.242621\n",
      "epoch: 1482/1500 | train loss was: 0.134007 | val loss was: 0.242596\n",
      "epoch: 1483/1500 | train loss was: 0.134008 | val loss was: 0.242571\n",
      "epoch: 1484/1500 | train loss was: 0.134009 | val loss was: 0.242546\n",
      "epoch: 1485/1500 | train loss was: 0.134009 | val loss was: 0.242521\n",
      "epoch: 1486/1500 | train loss was: 0.13401 | val loss was: 0.242497\n",
      "epoch: 1487/1500 | train loss was: 0.13401 | val loss was: 0.242472\n",
      "epoch: 1488/1500 | train loss was: 0.134011 | val loss was: 0.242447\n",
      "epoch: 1489/1500 | train loss was: 0.134012 | val loss was: 0.242422\n",
      "epoch: 1490/1500 | train loss was: 0.134012 | val loss was: 0.242398\n",
      "epoch: 1491/1500 | train loss was: 0.134013 | val loss was: 0.242373\n",
      "epoch: 1492/1500 | train loss was: 0.134013 | val loss was: 0.242348\n",
      "epoch: 1493/1500 | train loss was: 0.134014 | val loss was: 0.242324\n",
      "epoch: 1494/1500 | train loss was: 0.134015 | val loss was: 0.242299\n",
      "epoch: 1495/1500 | train loss was: 0.134015 | val loss was: 0.242275\n",
      "epoch: 1496/1500 | train loss was: 0.134016 | val loss was: 0.24225\n",
      "epoch: 1497/1500 | train loss was: 0.134016 | val loss was: 0.242225\n",
      "epoch: 1498/1500 | train loss was: 0.134017 | val loss was: 0.242201\n",
      "epoch: 1499/1500 | train loss was: 0.134018 | val loss was: 0.242176\n",
      "epoch: 1500/1500 | train loss was: 0.134018 | val loss was: 0.242152\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, val_loss_history = nn.train(x_train, y_train, x_val, y_val, epochs = 1000, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mean training loss per sample as a function of the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting stuff:\n",
    "\n",
    "epochs = np.arange(1, nn.epochs + 1, 1).reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "\n",
    "# plot train\n",
    "plt.plot(epochs, train_loss_history, label = 'training')\n",
    "plt.plot(epochs, val_loss_history, label = 'validation')\n",
    "plt.title('loss vs. epochs')\n",
    "plt.ylabel('average binary crossentropy loss per sample')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.xlim([1, nn.epochs])\n",
    "plt.grid('both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing the Model**:\n",
    "\n",
    "Now that the model has been sufficiently trained, we can test its accuracy on the test set of the data that was partitioned previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [0]\n",
      "predicted: 0 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [0]\n",
      "accuracy of model is: 88.372\n"
     ]
    }
   ],
   "source": [
    "# take the model that is already trained and use it to predict the class of tumour based on data:\n",
    "\n",
    "predictions, targets, accuracy = nn.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if accuracy >= 96:\n",
    "#     with open('initial_weights.csv', 'w', newline='') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerows(nn.initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
