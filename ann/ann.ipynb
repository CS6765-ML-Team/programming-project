{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS 6735 ANN - Intro**\n",
    "\n",
    "This is the notebook for the development of an artificial neural network from scratch for CS 6735. The purpose of this network is to perform a binary classification between malignant and benign tumours within a dataset of medical data about breast cancer.\n",
    "\n",
    "This ANN is to be developed from scratch, and must include:\n",
    "\n",
    "* a typical ANN structure (input, some number of hidden, output)\n",
    "* back propagation as the method of updating weights\n",
    "\n",
    "This code was developed by Matthew Tidd.\n",
    "\n",
    "**DATE CREATED: 13/11/2024**\n",
    "\n",
    "**DATE MODIFIED: 14/11/2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importing Packages**\n",
    "\n",
    "Must first import the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "from ucimlrepo import fetch_ucirepo                     # used to fetch dataset from remote repo\n",
    "import numpy as np                                      # used for mathematical operations\n",
    "import pandas as pd                                     # used to manipulate the data frame\n",
    "from sklearn.model_selection import train_test_split    # used to split the data\n",
    "from sklearn.metrics import confusion_matrix            # used to visualize the efficacy of model\n",
    "from sklearn.preprocessing import StandardScaler        # used to standardize the data\n",
    "import csv                                              # used to load CSVs\n",
    "\n",
    "import seaborn as sns                 # used for visualization of confusion matrix\n",
    "import matplotlib.pyplot as plt       # used for visualization of training loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load the Dataset**\n",
    "\n",
    "Need to load the dataset. This dataset can be found on the UC Irvine Machine Learning Repository, and was donated from the University of Wisconsin-Madison in 1995. It is considered a multivariate dataset, and consists of several features pertaining to breast mass that were computed from digitized images of fine needle aspirate. These features describe the characteristics of the cell nuclei present in the image. \n",
    "\n",
    "There are 10 real-valued features that are of importance for this dataset:\n",
    "\n",
    "* radius\n",
    "* texture\n",
    "* perimeter\n",
    "* area\n",
    "* smoothness\n",
    "* compactness\n",
    "* concavity\n",
    "* concave points\n",
    "* symmetry\n",
    "* fractal dimension\n",
    "\n",
    "For each of these real-valued features there are 3 versions, representing the mean, standard deviation, and worst case value. \n",
    "\n",
    "In total there are 30 features and 569 instances. The target variable in this dataset is a diagnosis, either malignant or benign. This dataset can be found at:\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the dataset is from the UCI ML repo, we can use their functions to fetch from their website:\n",
    "breast_data = fetch_ucirepo(id = 17)\n",
    "\n",
    "# can now access the data:\n",
    "breast_x = breast_data.data.features    # extract the features from the dict into a pandas data frame\n",
    "breast_y = breast_data.data.targets     # extract the labels from the dict into a pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Examine Dataset Properties**:\n",
    "\n",
    "Going to examine the properties of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 569 examples in the dataset\n",
      "there are 30 distinct features to train on\n",
      "the available diagnoses are: ['M' 'B']\n",
      "the available features are: \n",
      "\n",
      "radius1\n",
      "texture1\n",
      "perimeter1\n",
      "area1\n",
      "smoothness1\n",
      "compactness1\n",
      "concavity1\n",
      "concave_points1\n",
      "symmetry1\n",
      "fractal_dimension1\n",
      "radius2\n",
      "texture2\n",
      "perimeter2\n",
      "area2\n",
      "smoothness2\n",
      "compactness2\n",
      "concavity2\n",
      "concave_points2\n",
      "symmetry2\n",
      "fractal_dimension2\n",
      "radius3\n",
      "texture3\n",
      "perimeter3\n",
      "area3\n",
      "smoothness3\n",
      "compactness3\n",
      "concavity3\n",
      "concave_points3\n",
      "symmetry3\n",
      "fractal_dimension3\n"
     ]
    }
   ],
   "source": [
    "# get the total number of instances:\n",
    "print(f\"there are {breast_x.shape[0]} examples in the dataset\")\n",
    "\n",
    "# get number of features:\n",
    "print(f\"there are {breast_x.shape[1]} distinct features to train on\")\n",
    "\n",
    "# get the number of unique target variables:\n",
    "print(f\"the available diagnoses are: {breast_y['Diagnosis'].unique()}\")\n",
    "\n",
    "# get the names of the features:\n",
    "print(f\"the available features are: \\n\")\n",
    "for col in breast_x.columns:    # for every column in the data frame, \n",
    "    print(col)                  # print the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Pre-Processing**:\n",
    "\n",
    "Going to pre-process the dataset by performing the following:\n",
    "\n",
    "* remove the null values\n",
    "* scale the feature values to within a common range\n",
    "* encode the categorical labels into binary 0 or 1\n",
    "* split into training, validation, and testing data\n",
    "\n",
    "This is done to help realize the best model performance given the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no null values detected in features\n",
      "no null values detected in labels\n"
     ]
    }
   ],
   "source": [
    "# check for null values in the features:\n",
    "null = breast_x.isnull().values.any()   \n",
    "\n",
    "# if any null values exist, drop them, else pass\n",
    "if null == True:\n",
    "    breast_x = breast_x.dropna() \n",
    "    print(f'null values removed from features')\n",
    "else:\n",
    "    print(f'no null values detected in features')\n",
    "\n",
    "# check for null values in the labels, same process as above\n",
    "null = breast_y.isnull().values.any()\n",
    "if null == True:\n",
    "    breast_y = breast_y.dropna()\n",
    "    print(f'null values removed from labels')\n",
    "else:\n",
    "    print(f'no null values detected in labels')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize values to improve ANN performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features scaled\n"
     ]
    }
   ],
   "source": [
    "# standardize values by defining a scaler and fitting data to scaler:\n",
    "scaler = StandardScaler()\n",
    "x_scaled = pd.DataFrame(scaler.fit_transform(breast_x))\n",
    "print('features scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to encode the categorical variables into binary values to be used in the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels encoded: M = 1, B = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis\n",
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1"
      ]
     },
     "execution_count": 1353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode benign to 0, and malignant to 1\n",
    "breast_y =  pd.DataFrame(breast_y['Diagnosis'].map(lambda row: 1 if row == 'M' else 0))\n",
    "print('labels encoded: M = 1, B = 0')\n",
    "breast_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally need to create train/validation/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has form: (398, 30), labels are: (398, 1)\n",
      "validation data has form: (85, 30), labels are: (85, 1)\n",
      "test data has form: (86, 30), labels are: (86, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition data -> want 70% train, 15% validation, 15% testing\n",
    "x_train, dummy_x, y_train, dummy_y = train_test_split(x_scaled, breast_y, train_size = 0.7, test_size = 0.3)\n",
    "x_val, x_test, y_val, y_test = train_test_split(dummy_x, dummy_y, train_size = 0.5, test_size = 0.5)\n",
    "\n",
    "print(f\"training data has form: {x_train.shape}, labels are: {y_train.shape}\")\n",
    "print(f\"validation data has form: {x_val.shape}, labels are: {y_val.shape}\")\n",
    "print(f\"test data has form: {x_test.shape}, labels are: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANN Function Definition**\n",
    "\n",
    "Going to be using a class-based approach to defining the ANN, which will allow for instantiating, training, and querying of the network. Need to define the class itself as well as functions that will be used often, like the \n",
    "sigmoid function, relu, and their derivatives:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define useful functions:\n",
    "\n",
    "# logistic sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# rectified linear unit (basically a straight line)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# derivative function of relu (accepts np.arrays)\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# binary cross entropy -> for binary classification between 0 & 1\n",
    "def binary_crossentropy_loss(target, output):\n",
    "    output = np.clip(output, 1e-10, 1 - 1e-10)                              # clip to prevent log 0\n",
    "    loss = - (target * np.log(output) + (1 - target) * np.log(1 - output))  # BCE formula\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network class:\n",
    "\"\"\"\n",
    "this class accepts:\n",
    "\n",
    "input_size: number of input neurons\n",
    "hidden_size: number of hidden neurons\n",
    "output_size: expected number of output neurons\n",
    "load: whether or not to load initialized weights\n",
    "\n",
    "\"\"\"\n",
    "class NeuralNetwork:\n",
    "    # constructor:\n",
    "    def __init__(self, input_size, hidden_size, output_size, load):\n",
    "        # assign function inputs as instance variables:\n",
    "        self.input_size = input_size        # assign input neurons\n",
    "        self.hidden_size = hidden_size      # assign hidden neurons\n",
    "        self.output_size = output_size      # assign output neurons\n",
    "\n",
    "        # if the user wants to load the pre-initialized weights and biases:\n",
    "        if load == True:\n",
    "            # open the CSV and read it:\n",
    "            with open('initial_weights.csv', 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "\n",
    "                # for every value in each row, return as float to np.array:\n",
    "                w1_flat = np.array([float(val) for val in next(reader)])\n",
    "                b1_flat = np.array([float(val) for val in next(reader)])\n",
    "                w2_flat = np.array([float(val) for val in next(reader)])\n",
    "                b2_flat = np.array([float(val) for val in next(reader)])\n",
    "\n",
    "                # ensure that weights are in the correct dimensions:\n",
    "                self.w1 = w1_flat.reshape(hidden_size, input_size)\n",
    "                self.b1 = b1_flat.reshape(hidden_size, 1)\n",
    "                self.w2 = w2_flat.reshape(output_size, hidden_size)\n",
    "                self.b2 = b2_flat.reshape(output_size, 1)\n",
    "\n",
    "                # print to user that it was a success:\n",
    "                print('weights loaded')\n",
    "                print(\"w1 shape:\", self.w1.shape, 'w1 type:', type(self.w1))\n",
    "                print(\"b1 shape:\", self.b1.shape, 'b1 type:', type(self.b1))\n",
    "                print(\"w2 shape:\", self.w2.shape, 'w2 type:', type(self.w2))\n",
    "                print(\"b2 shape:\", self.b2.shape, 'b2 type:', type(self.b2))\n",
    "\n",
    "        # if not, randomly initialize weights and biases for the layers:\n",
    "        else:\n",
    "\n",
    "            # from input to hidden:\n",
    "            self.w1 = np.random.randn(hidden_size, input_size)\n",
    "            self.b1 = np.random.randn(hidden_size, 1)\n",
    "\n",
    "            # from hidden to output:\n",
    "            self.w2 = np.random.randn(output_size, hidden_size)\n",
    "            self.b2 = np.random.randn(output_size, 1)\n",
    "\n",
    "            # print to user that it was a success:\n",
    "            print('weights randomly initialized')\n",
    "            print(\"w1 shape:\", self.w1.shape, 'w1 type:', type(self.w1))\n",
    "            print(\"b1 shape:\", self.b1.shape, 'b1 type:', type(self.b1))\n",
    "            print(\"w2 shape:\", self.w2.shape, 'w2 type:', type(self.w2))\n",
    "            print(\"b2 shape:\", self.b2.shape, 'b2 type:', type(self.b2))\n",
    "\n",
    "\n",
    "    # feedforward function:\n",
    "    def forward_pass(self, x):\n",
    "        # make sure x is a column vector:\n",
    "        x = x.reshape((self.input_size, 1))\n",
    "\n",
    "        # from input to hidden:\n",
    "        self.net1 = np.dot(self.w1, x) + self.b1    # calculate net1\n",
    "        self.h1 = relu(self.net1)                   # calculate output of hidden layer\n",
    "\n",
    "        # from hidden to output:\n",
    "        self.net2 = np.dot(self.w2, self.h1) + self.b2  # calculate net2\n",
    "        self.o = sigmoid(self.net2)                     # calculate output of network\n",
    "\n",
    "        return self.o       # return value to user\n",
    "    \n",
    "    # backpropagation:\n",
    "    def backward_pass(self, x, y, learning_rate):\n",
    "        # make sure x is a column vector:\n",
    "        x = x.reshape((self.input_size, 1))\n",
    "\n",
    "        # get o - t:\n",
    "        o_error = self.o - y\n",
    "\n",
    "        # get gradients for weights and biases at the output layer:\n",
    "        de_dw2 = np.dot((o_error * sigmoid_derivative(self.net2)), self.h1.T)   # this is the partial derivative of E wrt. W2\n",
    "        de_db2 = o_error * sigmoid_derivative(self.net2)                        # this is the partial derivative of E wrt. B2\n",
    "\n",
    "        # get gradients for weights and biases at the input layer:\n",
    "\n",
    "        # this is an intermediary value because I was getting lost in the matrix dimensions\n",
    "        delta_1 = np.dot(self.w2.T, o_error * sigmoid_derivative(self.net2)) * relu_derivative(self.net1)  \n",
    "        de_dw1 = np.dot(delta_1, x.T)   # this is the partial derivative of E wrt. W1\n",
    "        de_db1 = delta_1                # this is the partial derivative of E wrt. B1\n",
    "\n",
    "        # update weights and biases:\n",
    "        self.w1 -= learning_rate  * de_dw1  # update w1\n",
    "        self.b1 -= learning_rate  * de_db1  # update b1\n",
    "        self.w2 -= learning_rate  * de_dw2  # update w2\n",
    "        self.b2 -= learning_rate  * de_db2  # update b2\n",
    "\n",
    "    # training:\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs, learning_rate):\n",
    "        # used in the plotting:\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # initialize lists for appending train and val history to:\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        # for every epoch:\n",
    "        for epoch in range(epochs):\n",
    "            total_train_loss = 0    # reset train loss for new epoch\n",
    "            total_val_loss = 0      # reset val loss for new epoch\n",
    "\n",
    "            # training loop:\n",
    "            for i in range(x_train.shape[0]):\n",
    "                # extract example:\n",
    "                x = x_train.iloc[i].values\n",
    "\n",
    "                # get target for that example:\n",
    "                target = y_train.iloc[i].values\n",
    "\n",
    "                # compute forward pass:\n",
    "                output = self.forward_pass(x)\n",
    "\n",
    "                # backpropagate:\n",
    "                self.backward_pass(x, target, learning_rate)\n",
    "\n",
    "                # get loss:\n",
    "                loss = binary_crossentropy_loss(target, output)\n",
    "                total_train_loss += loss    # add to total loss  \n",
    "\n",
    "            # get average BCE for train for that epoch:\n",
    "            average_train_loss_per_epoch = total_train_loss / x_train.shape[0]\n",
    "            train_loss_history.append(average_train_loss_per_epoch)\n",
    "\n",
    "            # validation loop:\n",
    "            for i in range(x_val.shape[0]):\n",
    "                # extract example:\n",
    "                x = x_val.iloc[i].values\n",
    "\n",
    "                # get target for that example:\n",
    "                target = y_val.iloc[i].values\n",
    "\n",
    "                # compute forward pass:\n",
    "                output = self.forward_pass(x)\n",
    "\n",
    "                # get loss:\n",
    "                loss = binary_crossentropy_loss(target, output)\n",
    "                total_val_loss += loss    # add to total loss \n",
    "            \n",
    "            # get average BCE for val:\n",
    "            average_val_loss_per_epoch = total_val_loss / x_val.shape[0]\n",
    "            val_loss_history.append(average_val_loss_per_epoch)\n",
    "\n",
    "            # print values to user:\n",
    "            print(f\"epoch: {epoch + 1}/{epochs} \n",
    "                  | train loss was: {round(float(average_train_loss_per_epoch), 6)} \n",
    "                  | val loss was: {round(float(average_val_loss_per_epoch), 6)}\")  \n",
    "\n",
    "        return np.array(train_loss_history).reshape(-1, 1), np.array(val_loss_history).reshape(-1, 1)\n",
    "\n",
    "    # testing:\n",
    "    def test(self, x_test, y_test):\n",
    "        # need to get the output for each value of x_test, compare against y_test:\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # initialize lists for appending values to:\n",
    "        targets = []\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(x_test.shape[0]):\n",
    "            # extract example:\n",
    "            x = x_test.iloc[i].values\n",
    "\n",
    "            # extract target:\n",
    "            target = y_test.iloc[i].values\n",
    "\n",
    "            # compute forward pass:\n",
    "            output = self.forward_pass(x)\n",
    "\n",
    "            # get class value from sigmoid value if over threshold:\n",
    "            prediction = 1 if output >= 0.5 else 0\n",
    "\n",
    "            print(f\"predicted: {prediction} | true: {target}\")\n",
    "\n",
    "            # append to lists, these are used for confusion matrix:\n",
    "            predictions.append(prediction)\n",
    "            targets.append(target)\n",
    "\n",
    "            # if correct, add to successes:\n",
    "            if prediction == target:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        # print accuracy:\n",
    "        accuracy = round((correct_predictions / x_test.shape[0]) * 100, 3)\n",
    "        print(f\"accuracy of model is: {accuracy}\")\n",
    "\n",
    "        return predictions, targets, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training & Testing the Model:**\n",
    "\n",
    "Following the definition of the neural network class and functions, a network can be instantiated and trained, and following this, tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights loaded\n",
      "w1 shape: (15, 30) w1 type: <class 'numpy.ndarray'>\n",
      "b1 shape: (15, 1) b1 type: <class 'numpy.ndarray'>\n",
      "w2 shape: (1, 15) w2 type: <class 'numpy.ndarray'>\n",
      "b2 shape: (1, 1) b2 type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# instantiate a network:\n",
    "nn = NeuralNetwork(input_size = 30, hidden_size = 15, output_size = 1, load = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000 | train loss was: 3.087491 | val loss was: 3.796667\n",
      "epoch: 2/1000 | train loss was: 2.916933 | val loss was: 3.624145\n",
      "epoch: 3/1000 | train loss was: 2.755326 | val loss was: 3.458374\n",
      "epoch: 4/1000 | train loss was: 2.600332 | val loss was: 3.297672\n",
      "epoch: 5/1000 | train loss was: 2.449741 | val loss was: 3.142076\n",
      "epoch: 6/1000 | train loss was: 2.301705 | val loss was: 2.987977\n",
      "epoch: 7/1000 | train loss was: 2.159463 | val loss was: 2.834597\n",
      "epoch: 8/1000 | train loss was: 2.022937 | val loss was: 2.678956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mtidd2\\AppData\\Local\\Temp\\ipykernel_8680\\749847774.py:146: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"epoch: {epoch + 1}/{epochs} | train loss was: {round(float(average_train_loss_per_epoch), 6)} | val loss was: {round(float(average_val_loss_per_epoch), 6)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/1000 | train loss was: 1.892865 | val loss was: 2.530971\n",
      "epoch: 10/1000 | train loss was: 1.774127 | val loss was: 2.400947\n",
      "epoch: 11/1000 | train loss was: 1.667927 | val loss was: 2.278703\n",
      "epoch: 12/1000 | train loss was: 1.571481 | val loss was: 2.160365\n",
      "epoch: 13/1000 | train loss was: 1.484101 | val loss was: 2.0494\n",
      "epoch: 14/1000 | train loss was: 1.405036 | val loss was: 1.950074\n",
      "epoch: 15/1000 | train loss was: 1.336355 | val loss was: 1.854419\n",
      "epoch: 16/1000 | train loss was: 1.273971 | val loss was: 1.761963\n",
      "epoch: 17/1000 | train loss was: 1.216736 | val loss was: 1.681596\n",
      "epoch: 18/1000 | train loss was: 1.166465 | val loss was: 1.614229\n",
      "epoch: 19/1000 | train loss was: 1.121304 | val loss was: 1.554612\n",
      "epoch: 20/1000 | train loss was: 1.07942 | val loss was: 1.500059\n",
      "epoch: 21/1000 | train loss was: 1.039976 | val loss was: 1.448535\n",
      "epoch: 22/1000 | train loss was: 1.002253 | val loss was: 1.399844\n",
      "epoch: 23/1000 | train loss was: 0.966322 | val loss was: 1.355617\n",
      "epoch: 24/1000 | train loss was: 0.933638 | val loss was: 1.317495\n",
      "epoch: 25/1000 | train loss was: 0.905062 | val loss was: 1.28339\n",
      "epoch: 26/1000 | train loss was: 0.879664 | val loss was: 1.252351\n",
      "epoch: 27/1000 | train loss was: 0.855969 | val loss was: 1.223229\n",
      "epoch: 28/1000 | train loss was: 0.833896 | val loss was: 1.19663\n",
      "epoch: 29/1000 | train loss was: 0.813024 | val loss was: 1.17196\n",
      "epoch: 30/1000 | train loss was: 0.793249 | val loss was: 1.149131\n",
      "epoch: 31/1000 | train loss was: 0.774984 | val loss was: 1.128036\n",
      "epoch: 32/1000 | train loss was: 0.75799 | val loss was: 1.108399\n",
      "epoch: 33/1000 | train loss was: 0.74233 | val loss was: 1.090749\n",
      "epoch: 34/1000 | train loss was: 0.727796 | val loss was: 1.073879\n",
      "epoch: 35/1000 | train loss was: 0.713709 | val loss was: 1.057514\n",
      "epoch: 36/1000 | train loss was: 0.699881 | val loss was: 1.041359\n",
      "epoch: 37/1000 | train loss was: 0.685997 | val loss was: 1.025027\n",
      "epoch: 38/1000 | train loss was: 0.671752 | val loss was: 1.008362\n",
      "epoch: 39/1000 | train loss was: 0.656989 | val loss was: 0.990964\n",
      "epoch: 40/1000 | train loss was: 0.641648 | val loss was: 0.973296\n",
      "epoch: 41/1000 | train loss was: 0.625913 | val loss was: 0.95628\n",
      "epoch: 42/1000 | train loss was: 0.610511 | val loss was: 0.940424\n",
      "epoch: 43/1000 | train loss was: 0.595339 | val loss was: 0.925271\n",
      "epoch: 44/1000 | train loss was: 0.579989 | val loss was: 0.910369\n",
      "epoch: 45/1000 | train loss was: 0.563626 | val loss was: 0.895076\n",
      "epoch: 46/1000 | train loss was: 0.544956 | val loss was: 0.87903\n",
      "epoch: 47/1000 | train loss was: 0.524616 | val loss was: 0.863219\n",
      "epoch: 48/1000 | train loss was: 0.506661 | val loss was: 0.84825\n",
      "epoch: 49/1000 | train loss was: 0.49154 | val loss was: 0.833093\n",
      "epoch: 50/1000 | train loss was: 0.476798 | val loss was: 0.81761\n",
      "epoch: 51/1000 | train loss was: 0.461121 | val loss was: 0.801762\n",
      "epoch: 52/1000 | train loss was: 0.446139 | val loss was: 0.78622\n",
      "epoch: 53/1000 | train loss was: 0.432212 | val loss was: 0.771424\n",
      "epoch: 54/1000 | train loss was: 0.419296 | val loss was: 0.757314\n",
      "epoch: 55/1000 | train loss was: 0.407129 | val loss was: 0.743601\n",
      "epoch: 56/1000 | train loss was: 0.395809 | val loss was: 0.730055\n",
      "epoch: 57/1000 | train loss was: 0.38569 | val loss was: 0.716649\n",
      "epoch: 58/1000 | train loss was: 0.376692 | val loss was: 0.703299\n",
      "epoch: 59/1000 | train loss was: 0.367852 | val loss was: 0.690595\n",
      "epoch: 60/1000 | train loss was: 0.359221 | val loss was: 0.678722\n",
      "epoch: 61/1000 | train loss was: 0.351668 | val loss was: 0.667743\n",
      "epoch: 62/1000 | train loss was: 0.345069 | val loss was: 0.657637\n",
      "epoch: 63/1000 | train loss was: 0.339298 | val loss was: 0.64834\n",
      "epoch: 64/1000 | train loss was: 0.334224 | val loss was: 0.639777\n",
      "epoch: 65/1000 | train loss was: 0.32973 | val loss was: 0.631871\n",
      "epoch: 66/1000 | train loss was: 0.32572 | val loss was: 0.624513\n",
      "epoch: 67/1000 | train loss was: 0.322103 | val loss was: 0.617638\n",
      "epoch: 68/1000 | train loss was: 0.318809 | val loss was: 0.611186\n",
      "epoch: 69/1000 | train loss was: 0.315784 | val loss was: 0.605105\n",
      "epoch: 70/1000 | train loss was: 0.312986 | val loss was: 0.599349\n",
      "epoch: 71/1000 | train loss was: 0.310382 | val loss was: 0.593884\n",
      "epoch: 72/1000 | train loss was: 0.307945 | val loss was: 0.588678\n",
      "epoch: 73/1000 | train loss was: 0.305652 | val loss was: 0.583706\n",
      "epoch: 74/1000 | train loss was: 0.303486 | val loss was: 0.57895\n",
      "epoch: 75/1000 | train loss was: 0.301431 | val loss was: 0.574391\n",
      "epoch: 76/1000 | train loss was: 0.299474 | val loss was: 0.570016\n",
      "epoch: 77/1000 | train loss was: 0.297605 | val loss was: 0.565813\n",
      "epoch: 78/1000 | train loss was: 0.295815 | val loss was: 0.561774\n",
      "epoch: 79/1000 | train loss was: 0.294096 | val loss was: 0.557909\n",
      "epoch: 80/1000 | train loss was: 0.29244 | val loss was: 0.554194\n",
      "epoch: 81/1000 | train loss was: 0.290841 | val loss was: 0.550624\n",
      "epoch: 82/1000 | train loss was: 0.289295 | val loss was: 0.547194\n",
      "epoch: 83/1000 | train loss was: 0.287796 | val loss was: 0.543901\n",
      "epoch: 84/1000 | train loss was: 0.286327 | val loss was: 0.54071\n",
      "epoch: 85/1000 | train loss was: 0.284866 | val loss was: 0.537705\n",
      "epoch: 86/1000 | train loss was: 0.283476 | val loss was: 0.53483\n",
      "epoch: 87/1000 | train loss was: 0.282119 | val loss was: 0.532085\n",
      "epoch: 88/1000 | train loss was: 0.280791 | val loss was: 0.529466\n",
      "epoch: 89/1000 | train loss was: 0.279491 | val loss was: 0.526971\n",
      "epoch: 90/1000 | train loss was: 0.278216 | val loss was: 0.524596\n",
      "epoch: 91/1000 | train loss was: 0.276965 | val loss was: 0.522338\n",
      "epoch: 92/1000 | train loss was: 0.275737 | val loss was: 0.520195\n",
      "epoch: 93/1000 | train loss was: 0.27453 | val loss was: 0.518164\n",
      "epoch: 94/1000 | train loss was: 0.273346 | val loss was: 0.516242\n",
      "epoch: 95/1000 | train loss was: 0.272186 | val loss was: 0.514425\n",
      "epoch: 96/1000 | train loss was: 0.271047 | val loss was: 0.512709\n",
      "epoch: 97/1000 | train loss was: 0.269928 | val loss was: 0.511092\n",
      "epoch: 98/1000 | train loss was: 0.268819 | val loss was: 0.509561\n",
      "epoch: 99/1000 | train loss was: 0.267734 | val loss was: 0.508125\n",
      "epoch: 100/1000 | train loss was: 0.266678 | val loss was: 0.506782\n",
      "epoch: 101/1000 | train loss was: 0.265655 | val loss was: 0.505527\n",
      "epoch: 102/1000 | train loss was: 0.26467 | val loss was: 0.50436\n",
      "epoch: 103/1000 | train loss was: 0.263727 | val loss was: 0.503273\n",
      "epoch: 104/1000 | train loss was: 0.262829 | val loss was: 0.502261\n",
      "epoch: 105/1000 | train loss was: 0.262067 | val loss was: 0.501438\n",
      "epoch: 106/1000 | train loss was: 0.261346 | val loss was: 0.500651\n",
      "epoch: 107/1000 | train loss was: 0.260658 | val loss was: 0.499872\n",
      "epoch: 108/1000 | train loss was: 0.259984 | val loss was: 0.499067\n",
      "epoch: 109/1000 | train loss was: 0.259297 | val loss was: 0.498198\n",
      "epoch: 110/1000 | train loss was: 0.258562 | val loss was: 0.497232\n",
      "epoch: 111/1000 | train loss was: 0.257744 | val loss was: 0.496148\n",
      "epoch: 112/1000 | train loss was: 0.256832 | val loss was: 0.494924\n",
      "epoch: 113/1000 | train loss was: 0.255798 | val loss was: 0.493576\n",
      "epoch: 114/1000 | train loss was: 0.254621 | val loss was: 0.492109\n",
      "epoch: 115/1000 | train loss was: 0.253307 | val loss was: 0.490547\n",
      "epoch: 116/1000 | train loss was: 0.251848 | val loss was: 0.488905\n",
      "epoch: 117/1000 | train loss was: 0.25025 | val loss was: 0.487201\n",
      "epoch: 118/1000 | train loss was: 0.248511 | val loss was: 0.485445\n",
      "epoch: 119/1000 | train loss was: 0.24663 | val loss was: 0.483648\n",
      "epoch: 120/1000 | train loss was: 0.244602 | val loss was: 0.481818\n",
      "epoch: 121/1000 | train loss was: 0.242414 | val loss was: 0.479958\n",
      "epoch: 122/1000 | train loss was: 0.240049 | val loss was: 0.47807\n",
      "epoch: 123/1000 | train loss was: 0.23749 | val loss was: 0.476175\n",
      "epoch: 124/1000 | train loss was: 0.23472 | val loss was: 0.474293\n",
      "epoch: 125/1000 | train loss was: 0.231741 | val loss was: 0.472463\n",
      "epoch: 126/1000 | train loss was: 0.228595 | val loss was: 0.470745\n",
      "epoch: 127/1000 | train loss was: 0.225369 | val loss was: 0.469214\n",
      "epoch: 128/1000 | train loss was: 0.222216 | val loss was: 0.467944\n",
      "epoch: 129/1000 | train loss was: 0.219307 | val loss was: 0.466967\n",
      "epoch: 130/1000 | train loss was: 0.216776 | val loss was: 0.466265\n",
      "epoch: 131/1000 | train loss was: 0.21465 | val loss was: 0.465765\n",
      "epoch: 132/1000 | train loss was: 0.212896 | val loss was: 0.465415\n",
      "epoch: 133/1000 | train loss was: 0.211423 | val loss was: 0.465157\n",
      "epoch: 134/1000 | train loss was: 0.210167 | val loss was: 0.464956\n",
      "epoch: 135/1000 | train loss was: 0.209076 | val loss was: 0.464799\n",
      "epoch: 136/1000 | train loss was: 0.208094 | val loss was: 0.464664\n",
      "epoch: 137/1000 | train loss was: 0.207204 | val loss was: 0.464553\n",
      "epoch: 138/1000 | train loss was: 0.206358 | val loss was: 0.464472\n",
      "epoch: 139/1000 | train loss was: 0.205555 | val loss was: 0.464398\n",
      "epoch: 140/1000 | train loss was: 0.204792 | val loss was: 0.464336\n",
      "epoch: 141/1000 | train loss was: 0.20405 | val loss was: 0.464276\n",
      "epoch: 142/1000 | train loss was: 0.203333 | val loss was: 0.464227\n",
      "epoch: 143/1000 | train loss was: 0.202624 | val loss was: 0.464177\n",
      "epoch: 144/1000 | train loss was: 0.201931 | val loss was: 0.464135\n",
      "epoch: 145/1000 | train loss was: 0.201239 | val loss was: 0.464097\n",
      "epoch: 146/1000 | train loss was: 0.200553 | val loss was: 0.464057\n",
      "epoch: 147/1000 | train loss was: 0.199874 | val loss was: 0.464024\n",
      "epoch: 148/1000 | train loss was: 0.19919 | val loss was: 0.463989\n",
      "epoch: 149/1000 | train loss was: 0.198512 | val loss was: 0.463961\n",
      "epoch: 150/1000 | train loss was: 0.197826 | val loss was: 0.46393\n",
      "epoch: 151/1000 | train loss was: 0.197143 | val loss was: 0.463905\n",
      "epoch: 152/1000 | train loss was: 0.196451 | val loss was: 0.463877\n",
      "epoch: 153/1000 | train loss was: 0.19576 | val loss was: 0.463855\n",
      "epoch: 154/1000 | train loss was: 0.195057 | val loss was: 0.463831\n",
      "epoch: 155/1000 | train loss was: 0.194353 | val loss was: 0.463812\n",
      "epoch: 156/1000 | train loss was: 0.193636 | val loss was: 0.46379\n",
      "epoch: 157/1000 | train loss was: 0.192916 | val loss was: 0.463775\n",
      "epoch: 158/1000 | train loss was: 0.192182 | val loss was: 0.463757\n",
      "epoch: 159/1000 | train loss was: 0.191441 | val loss was: 0.463744\n",
      "epoch: 160/1000 | train loss was: 0.190683 | val loss was: 0.46373\n",
      "epoch: 161/1000 | train loss was: 0.189917 | val loss was: 0.463721\n",
      "epoch: 162/1000 | train loss was: 0.189131 | val loss was: 0.46371\n",
      "epoch: 163/1000 | train loss was: 0.188334 | val loss was: 0.463705\n",
      "epoch: 164/1000 | train loss was: 0.187515 | val loss was: 0.463699\n",
      "epoch: 165/1000 | train loss was: 0.186681 | val loss was: 0.463695\n",
      "epoch: 166/1000 | train loss was: 0.185827 | val loss was: 0.463698\n",
      "epoch: 167/1000 | train loss was: 0.184946 | val loss was: 0.4637\n",
      "epoch: 168/1000 | train loss was: 0.184044 | val loss was: 0.463706\n",
      "epoch: 169/1000 | train loss was: 0.183115 | val loss was: 0.46372\n",
      "epoch: 170/1000 | train loss was: 0.182151 | val loss was: 0.463734\n",
      "epoch: 171/1000 | train loss was: 0.181159 | val loss was: 0.463754\n",
      "epoch: 172/1000 | train loss was: 0.180131 | val loss was: 0.463779\n",
      "epoch: 173/1000 | train loss was: 0.179063 | val loss was: 0.463811\n",
      "epoch: 174/1000 | train loss was: 0.177951 | val loss was: 0.46385\n",
      "epoch: 175/1000 | train loss was: 0.176789 | val loss was: 0.463898\n",
      "epoch: 176/1000 | train loss was: 0.175572 | val loss was: 0.463955\n",
      "epoch: 177/1000 | train loss was: 0.174295 | val loss was: 0.464024\n",
      "epoch: 178/1000 | train loss was: 0.172948 | val loss was: 0.464106\n",
      "epoch: 179/1000 | train loss was: 0.171525 | val loss was: 0.464204\n",
      "epoch: 180/1000 | train loss was: 0.170014 | val loss was: 0.464319\n",
      "epoch: 181/1000 | train loss was: 0.168405 | val loss was: 0.464455\n",
      "epoch: 182/1000 | train loss was: 0.166685 | val loss was: 0.464616\n",
      "epoch: 183/1000 | train loss was: 0.164838 | val loss was: 0.464807\n",
      "epoch: 184/1000 | train loss was: 0.162849 | val loss was: 0.465033\n",
      "epoch: 185/1000 | train loss was: 0.160698 | val loss was: 0.465302\n",
      "epoch: 186/1000 | train loss was: 0.158368 | val loss was: 0.46562\n",
      "epoch: 187/1000 | train loss was: 0.155841 | val loss was: 0.465997\n",
      "epoch: 188/1000 | train loss was: 0.153105 | val loss was: 0.466442\n",
      "epoch: 189/1000 | train loss was: 0.150159 | val loss was: 0.466967\n",
      "epoch: 190/1000 | train loss was: 0.147017 | val loss was: 0.46758\n",
      "epoch: 191/1000 | train loss was: 0.14371 | val loss was: 0.468292\n",
      "epoch: 192/1000 | train loss was: 0.14027 | val loss was: 0.469122\n",
      "epoch: 193/1000 | train loss was: 0.136695 | val loss was: 0.47011\n",
      "epoch: 194/1000 | train loss was: 0.1329 | val loss was: 0.471348\n",
      "epoch: 195/1000 | train loss was: 0.128691 | val loss was: 0.472978\n",
      "epoch: 196/1000 | train loss was: 0.123963 | val loss was: 0.474894\n",
      "epoch: 197/1000 | train loss was: 0.119509 | val loss was: 0.476149\n",
      "epoch: 198/1000 | train loss was: 0.116903 | val loss was: 0.476697\n",
      "epoch: 199/1000 | train loss was: 0.115562 | val loss was: 0.476988\n",
      "epoch: 200/1000 | train loss was: 0.114651 | val loss was: 0.477169\n",
      "epoch: 201/1000 | train loss was: 0.113933 | val loss was: 0.477296\n",
      "epoch: 202/1000 | train loss was: 0.113322 | val loss was: 0.477397\n",
      "epoch: 203/1000 | train loss was: 0.112776 | val loss was: 0.477486\n",
      "epoch: 204/1000 | train loss was: 0.112272 | val loss was: 0.47757\n",
      "epoch: 205/1000 | train loss was: 0.111797 | val loss was: 0.477655\n",
      "epoch: 206/1000 | train loss was: 0.111343 | val loss was: 0.477743\n",
      "epoch: 207/1000 | train loss was: 0.110904 | val loss was: 0.477835\n",
      "epoch: 208/1000 | train loss was: 0.110475 | val loss was: 0.477935\n",
      "epoch: 209/1000 | train loss was: 0.110052 | val loss was: 0.478041\n",
      "epoch: 210/1000 | train loss was: 0.109635 | val loss was: 0.478155\n",
      "epoch: 211/1000 | train loss was: 0.109221 | val loss was: 0.478277\n",
      "epoch: 212/1000 | train loss was: 0.108808 | val loss was: 0.478407\n",
      "epoch: 213/1000 | train loss was: 0.108395 | val loss was: 0.478546\n",
      "epoch: 214/1000 | train loss was: 0.107981 | val loss was: 0.478693\n",
      "epoch: 215/1000 | train loss was: 0.107566 | val loss was: 0.478849\n",
      "epoch: 216/1000 | train loss was: 0.107148 | val loss was: 0.479013\n",
      "epoch: 217/1000 | train loss was: 0.106726 | val loss was: 0.479187\n",
      "epoch: 218/1000 | train loss was: 0.1063 | val loss was: 0.47937\n",
      "epoch: 219/1000 | train loss was: 0.10587 | val loss was: 0.479563\n",
      "epoch: 220/1000 | train loss was: 0.105434 | val loss was: 0.479765\n",
      "epoch: 221/1000 | train loss was: 0.104991 | val loss was: 0.479976\n",
      "epoch: 222/1000 | train loss was: 0.104542 | val loss was: 0.480198\n",
      "epoch: 223/1000 | train loss was: 0.104085 | val loss was: 0.48043\n",
      "epoch: 224/1000 | train loss was: 0.10362 | val loss was: 0.480673\n",
      "epoch: 225/1000 | train loss was: 0.103145 | val loss was: 0.480927\n",
      "epoch: 226/1000 | train loss was: 0.102659 | val loss was: 0.481193\n",
      "epoch: 227/1000 | train loss was: 0.102161 | val loss was: 0.481472\n",
      "epoch: 228/1000 | train loss was: 0.101651 | val loss was: 0.481765\n",
      "epoch: 229/1000 | train loss was: 0.101124 | val loss was: 0.482073\n",
      "epoch: 230/1000 | train loss was: 0.100581 | val loss was: 0.482398\n",
      "epoch: 231/1000 | train loss was: 0.100018 | val loss was: 0.482741\n",
      "epoch: 232/1000 | train loss was: 0.099433 | val loss was: 0.483104\n",
      "epoch: 233/1000 | train loss was: 0.09882 | val loss was: 0.483492\n",
      "epoch: 234/1000 | train loss was: 0.098176 | val loss was: 0.483908\n",
      "epoch: 235/1000 | train loss was: 0.097495 | val loss was: 0.484358\n",
      "epoch: 236/1000 | train loss was: 0.096768 | val loss was: 0.48485\n",
      "epoch: 237/1000 | train loss was: 0.095984 | val loss was: 0.485394\n",
      "epoch: 238/1000 | train loss was: 0.095129 | val loss was: 0.486007\n",
      "epoch: 239/1000 | train loss was: 0.094183 | val loss was: 0.486714\n",
      "epoch: 240/1000 | train loss was: 0.093116 | val loss was: 0.487554\n",
      "epoch: 241/1000 | train loss was: 0.091892 | val loss was: 0.488589\n",
      "epoch: 242/1000 | train loss was: 0.090473 | val loss was: 0.489912\n",
      "epoch: 243/1000 | train loss was: 0.088845 | val loss was: 0.491575\n",
      "epoch: 244/1000 | train loss was: 0.087121 | val loss was: 0.493545\n",
      "epoch: 245/1000 | train loss was: 0.085639 | val loss was: 0.495261\n",
      "epoch: 246/1000 | train loss was: 0.084677 | val loss was: 0.496233\n",
      "epoch: 247/1000 | train loss was: 0.084102 | val loss was: 0.496652\n",
      "epoch: 248/1000 | train loss was: 0.083704 | val loss was: 0.496782\n",
      "epoch: 249/1000 | train loss was: 0.083388 | val loss was: 0.496772\n",
      "epoch: 250/1000 | train loss was: 0.083117 | val loss was: 0.496698\n",
      "epoch: 251/1000 | train loss was: 0.082873 | val loss was: 0.4966\n",
      "epoch: 252/1000 | train loss was: 0.082649 | val loss was: 0.496499\n",
      "epoch: 253/1000 | train loss was: 0.082438 | val loss was: 0.496405\n",
      "epoch: 254/1000 | train loss was: 0.082237 | val loss was: 0.496321\n",
      "epoch: 255/1000 | train loss was: 0.082043 | val loss was: 0.49625\n",
      "epoch: 256/1000 | train loss was: 0.081856 | val loss was: 0.496193\n",
      "epoch: 257/1000 | train loss was: 0.081674 | val loss was: 0.496149\n",
      "epoch: 258/1000 | train loss was: 0.081495 | val loss was: 0.496117\n",
      "epoch: 259/1000 | train loss was: 0.08132 | val loss was: 0.496097\n",
      "epoch: 260/1000 | train loss was: 0.081151 | val loss was: 0.496082\n",
      "epoch: 261/1000 | train loss was: 0.080985 | val loss was: 0.496077\n",
      "epoch: 262/1000 | train loss was: 0.080821 | val loss was: 0.496081\n",
      "epoch: 263/1000 | train loss was: 0.080657 | val loss was: 0.496093\n",
      "epoch: 264/1000 | train loss was: 0.080495 | val loss was: 0.496113\n",
      "epoch: 265/1000 | train loss was: 0.080334 | val loss was: 0.49614\n",
      "epoch: 266/1000 | train loss was: 0.080173 | val loss was: 0.496174\n",
      "epoch: 267/1000 | train loss was: 0.080012 | val loss was: 0.496213\n",
      "epoch: 268/1000 | train loss was: 0.079852 | val loss was: 0.496258\n",
      "epoch: 269/1000 | train loss was: 0.079691 | val loss was: 0.496309\n",
      "epoch: 270/1000 | train loss was: 0.079531 | val loss was: 0.496365\n",
      "epoch: 271/1000 | train loss was: 0.079369 | val loss was: 0.496426\n",
      "epoch: 272/1000 | train loss was: 0.079208 | val loss was: 0.496492\n",
      "epoch: 273/1000 | train loss was: 0.079045 | val loss was: 0.496563\n",
      "epoch: 274/1000 | train loss was: 0.078882 | val loss was: 0.496638\n",
      "epoch: 275/1000 | train loss was: 0.078713 | val loss was: 0.496734\n",
      "epoch: 276/1000 | train loss was: 0.078537 | val loss was: 0.496832\n",
      "epoch: 277/1000 | train loss was: 0.078359 | val loss was: 0.496933\n",
      "epoch: 278/1000 | train loss was: 0.078181 | val loss was: 0.497038\n",
      "epoch: 279/1000 | train loss was: 0.078002 | val loss was: 0.497147\n",
      "epoch: 280/1000 | train loss was: 0.077821 | val loss was: 0.49726\n",
      "epoch: 281/1000 | train loss was: 0.077639 | val loss was: 0.497377\n",
      "epoch: 282/1000 | train loss was: 0.077455 | val loss was: 0.497499\n",
      "epoch: 283/1000 | train loss was: 0.077269 | val loss was: 0.497626\n",
      "epoch: 284/1000 | train loss was: 0.077081 | val loss was: 0.497759\n",
      "epoch: 285/1000 | train loss was: 0.076891 | val loss was: 0.497898\n",
      "epoch: 286/1000 | train loss was: 0.076698 | val loss was: 0.498043\n",
      "epoch: 287/1000 | train loss was: 0.076503 | val loss was: 0.498195\n",
      "epoch: 288/1000 | train loss was: 0.076305 | val loss was: 0.498356\n",
      "epoch: 289/1000 | train loss was: 0.076105 | val loss was: 0.498525\n",
      "epoch: 290/1000 | train loss was: 0.075903 | val loss was: 0.498703\n",
      "epoch: 291/1000 | train loss was: 0.075697 | val loss was: 0.49889\n",
      "epoch: 292/1000 | train loss was: 0.075488 | val loss was: 0.499086\n",
      "epoch: 293/1000 | train loss was: 0.075277 | val loss was: 0.499293\n",
      "epoch: 294/1000 | train loss was: 0.075063 | val loss was: 0.49951\n",
      "epoch: 295/1000 | train loss was: 0.074847 | val loss was: 0.499738\n",
      "epoch: 296/1000 | train loss was: 0.074629 | val loss was: 0.499977\n",
      "epoch: 297/1000 | train loss was: 0.074366 | val loss was: 0.500143\n",
      "epoch: 298/1000 | train loss was: 0.074037 | val loss was: 0.500324\n",
      "epoch: 299/1000 | train loss was: 0.073711 | val loss was: 0.500518\n",
      "epoch: 300/1000 | train loss was: 0.073389 | val loss was: 0.500724\n",
      "epoch: 301/1000 | train loss was: 0.073074 | val loss was: 0.500937\n",
      "epoch: 302/1000 | train loss was: 0.072766 | val loss was: 0.501153\n",
      "epoch: 303/1000 | train loss was: 0.072468 | val loss was: 0.501367\n",
      "epoch: 304/1000 | train loss was: 0.072181 | val loss was: 0.501574\n",
      "epoch: 305/1000 | train loss was: 0.071907 | val loss was: 0.501768\n",
      "epoch: 306/1000 | train loss was: 0.071647 | val loss was: 0.501944\n",
      "epoch: 307/1000 | train loss was: 0.071402 | val loss was: 0.502099\n",
      "epoch: 308/1000 | train loss was: 0.071171 | val loss was: 0.502231\n",
      "epoch: 309/1000 | train loss was: 0.070954 | val loss was: 0.502337\n",
      "epoch: 310/1000 | train loss was: 0.070751 | val loss was: 0.502418\n",
      "epoch: 311/1000 | train loss was: 0.07056 | val loss was: 0.502474\n",
      "epoch: 312/1000 | train loss was: 0.070381 | val loss was: 0.502509\n",
      "epoch: 313/1000 | train loss was: 0.070213 | val loss was: 0.502523\n",
      "epoch: 314/1000 | train loss was: 0.070054 | val loss was: 0.50252\n",
      "epoch: 315/1000 | train loss was: 0.069903 | val loss was: 0.502502\n",
      "epoch: 316/1000 | train loss was: 0.069761 | val loss was: 0.502473\n",
      "epoch: 317/1000 | train loss was: 0.069626 | val loss was: 0.502451\n",
      "epoch: 318/1000 | train loss was: 0.069494 | val loss was: 0.502421\n",
      "epoch: 319/1000 | train loss was: 0.069367 | val loss was: 0.502385\n",
      "epoch: 320/1000 | train loss was: 0.069246 | val loss was: 0.502345\n",
      "epoch: 321/1000 | train loss was: 0.06913 | val loss was: 0.502301\n",
      "epoch: 322/1000 | train loss was: 0.069018 | val loss was: 0.502255\n",
      "epoch: 323/1000 | train loss was: 0.068911 | val loss was: 0.502207\n",
      "epoch: 324/1000 | train loss was: 0.068807 | val loss was: 0.502159\n",
      "epoch: 325/1000 | train loss was: 0.068706 | val loss was: 0.502112\n",
      "epoch: 326/1000 | train loss was: 0.068609 | val loss was: 0.502065\n",
      "epoch: 327/1000 | train loss was: 0.068515 | val loss was: 0.502019\n",
      "epoch: 328/1000 | train loss was: 0.068424 | val loss was: 0.501976\n",
      "epoch: 329/1000 | train loss was: 0.068335 | val loss was: 0.501945\n",
      "epoch: 330/1000 | train loss was: 0.068249 | val loss was: 0.501916\n",
      "epoch: 331/1000 | train loss was: 0.068165 | val loss was: 0.501888\n",
      "epoch: 332/1000 | train loss was: 0.068083 | val loss was: 0.501862\n",
      "epoch: 333/1000 | train loss was: 0.068003 | val loss was: 0.501839\n",
      "epoch: 334/1000 | train loss was: 0.06792 | val loss was: 0.501793\n",
      "epoch: 335/1000 | train loss was: 0.067842 | val loss was: 0.50175\n",
      "epoch: 336/1000 | train loss was: 0.067772 | val loss was: 0.501732\n",
      "epoch: 337/1000 | train loss was: 0.067695 | val loss was: 0.501693\n",
      "epoch: 338/1000 | train loss was: 0.067628 | val loss was: 0.501654\n",
      "epoch: 339/1000 | train loss was: 0.067564 | val loss was: 0.501616\n",
      "epoch: 340/1000 | train loss was: 0.0675 | val loss was: 0.50158\n",
      "epoch: 341/1000 | train loss was: 0.067444 | val loss was: 0.501568\n",
      "epoch: 342/1000 | train loss was: 0.067381 | val loss was: 0.501536\n",
      "epoch: 343/1000 | train loss was: 0.067321 | val loss was: 0.501507\n",
      "epoch: 344/1000 | train loss was: 0.067261 | val loss was: 0.501479\n",
      "epoch: 345/1000 | train loss was: 0.067203 | val loss was: 0.501454\n",
      "epoch: 346/1000 | train loss was: 0.067146 | val loss was: 0.50143\n",
      "epoch: 347/1000 | train loss was: 0.067089 | val loss was: 0.501408\n",
      "epoch: 348/1000 | train loss was: 0.067034 | val loss was: 0.501388\n",
      "epoch: 349/1000 | train loss was: 0.066984 | val loss was: 0.501389\n",
      "epoch: 350/1000 | train loss was: 0.066928 | val loss was: 0.501372\n",
      "epoch: 351/1000 | train loss was: 0.066874 | val loss was: 0.501357\n",
      "epoch: 352/1000 | train loss was: 0.066822 | val loss was: 0.501344\n",
      "epoch: 353/1000 | train loss was: 0.066769 | val loss was: 0.501331\n",
      "epoch: 354/1000 | train loss was: 0.066718 | val loss was: 0.501321\n",
      "epoch: 355/1000 | train loss was: 0.066667 | val loss was: 0.501307\n",
      "epoch: 356/1000 | train loss was: 0.066618 | val loss was: 0.501295\n",
      "epoch: 357/1000 | train loss was: 0.066569 | val loss was: 0.501284\n",
      "epoch: 358/1000 | train loss was: 0.066521 | val loss was: 0.501272\n",
      "epoch: 359/1000 | train loss was: 0.066473 | val loss was: 0.501262\n",
      "epoch: 360/1000 | train loss was: 0.066426 | val loss was: 0.501253\n",
      "epoch: 361/1000 | train loss was: 0.066379 | val loss was: 0.501245\n",
      "epoch: 362/1000 | train loss was: 0.066333 | val loss was: 0.501238\n",
      "epoch: 363/1000 | train loss was: 0.066287 | val loss was: 0.501232\n",
      "epoch: 364/1000 | train loss was: 0.066242 | val loss was: 0.501227\n",
      "epoch: 365/1000 | train loss was: 0.066197 | val loss was: 0.501224\n",
      "epoch: 366/1000 | train loss was: 0.066152 | val loss was: 0.501221\n",
      "epoch: 367/1000 | train loss was: 0.066108 | val loss was: 0.501219\n",
      "epoch: 368/1000 | train loss was: 0.066064 | val loss was: 0.501218\n",
      "epoch: 369/1000 | train loss was: 0.06602 | val loss was: 0.501218\n",
      "epoch: 370/1000 | train loss was: 0.065977 | val loss was: 0.501218\n",
      "epoch: 371/1000 | train loss was: 0.065934 | val loss was: 0.50122\n",
      "epoch: 372/1000 | train loss was: 0.065891 | val loss was: 0.501222\n",
      "epoch: 373/1000 | train loss was: 0.065849 | val loss was: 0.501225\n",
      "epoch: 374/1000 | train loss was: 0.065807 | val loss was: 0.501228\n",
      "epoch: 375/1000 | train loss was: 0.065766 | val loss was: 0.501232\n",
      "epoch: 376/1000 | train loss was: 0.065724 | val loss was: 0.501237\n",
      "epoch: 377/1000 | train loss was: 0.065683 | val loss was: 0.501243\n",
      "epoch: 378/1000 | train loss was: 0.065642 | val loss was: 0.501249\n",
      "epoch: 379/1000 | train loss was: 0.065602 | val loss was: 0.501255\n",
      "epoch: 380/1000 | train loss was: 0.065561 | val loss was: 0.501262\n",
      "epoch: 381/1000 | train loss was: 0.065521 | val loss was: 0.501269\n",
      "epoch: 382/1000 | train loss was: 0.065482 | val loss was: 0.501277\n",
      "epoch: 383/1000 | train loss was: 0.065442 | val loss was: 0.501286\n",
      "epoch: 384/1000 | train loss was: 0.065403 | val loss was: 0.501295\n",
      "epoch: 385/1000 | train loss was: 0.065364 | val loss was: 0.501304\n",
      "epoch: 386/1000 | train loss was: 0.065325 | val loss was: 0.501313\n",
      "epoch: 387/1000 | train loss was: 0.065286 | val loss was: 0.501323\n",
      "epoch: 388/1000 | train loss was: 0.065248 | val loss was: 0.501334\n",
      "epoch: 389/1000 | train loss was: 0.06521 | val loss was: 0.501344\n",
      "epoch: 390/1000 | train loss was: 0.065172 | val loss was: 0.501355\n",
      "epoch: 391/1000 | train loss was: 0.065134 | val loss was: 0.501366\n",
      "epoch: 392/1000 | train loss was: 0.065097 | val loss was: 0.501378\n",
      "epoch: 393/1000 | train loss was: 0.065059 | val loss was: 0.501389\n",
      "epoch: 394/1000 | train loss was: 0.065022 | val loss was: 0.501401\n",
      "epoch: 395/1000 | train loss was: 0.064986 | val loss was: 0.501413\n",
      "epoch: 396/1000 | train loss was: 0.064949 | val loss was: 0.501425\n",
      "epoch: 397/1000 | train loss was: 0.064912 | val loss was: 0.501438\n",
      "epoch: 398/1000 | train loss was: 0.064876 | val loss was: 0.501451\n",
      "epoch: 399/1000 | train loss was: 0.06484 | val loss was: 0.501463\n",
      "epoch: 400/1000 | train loss was: 0.064804 | val loss was: 0.501477\n",
      "epoch: 401/1000 | train loss was: 0.064768 | val loss was: 0.50149\n",
      "epoch: 402/1000 | train loss was: 0.064733 | val loss was: 0.501504\n",
      "epoch: 403/1000 | train loss was: 0.064697 | val loss was: 0.501517\n",
      "epoch: 404/1000 | train loss was: 0.064662 | val loss was: 0.50153\n",
      "epoch: 405/1000 | train loss was: 0.064626 | val loss was: 0.501544\n",
      "epoch: 406/1000 | train loss was: 0.064591 | val loss was: 0.501557\n",
      "epoch: 407/1000 | train loss was: 0.064556 | val loss was: 0.501572\n",
      "epoch: 408/1000 | train loss was: 0.064521 | val loss was: 0.501586\n",
      "epoch: 409/1000 | train loss was: 0.064486 | val loss was: 0.5016\n",
      "epoch: 410/1000 | train loss was: 0.064451 | val loss was: 0.501614\n",
      "epoch: 411/1000 | train loss was: 0.064416 | val loss was: 0.501628\n",
      "epoch: 412/1000 | train loss was: 0.064382 | val loss was: 0.501643\n",
      "epoch: 413/1000 | train loss was: 0.064347 | val loss was: 0.501657\n",
      "epoch: 414/1000 | train loss was: 0.064313 | val loss was: 0.501672\n",
      "epoch: 415/1000 | train loss was: 0.064279 | val loss was: 0.501687\n",
      "epoch: 416/1000 | train loss was: 0.064245 | val loss was: 0.501702\n",
      "epoch: 417/1000 | train loss was: 0.064212 | val loss was: 0.501716\n",
      "epoch: 418/1000 | train loss was: 0.064178 | val loss was: 0.501731\n",
      "epoch: 419/1000 | train loss was: 0.064144 | val loss was: 0.501746\n",
      "epoch: 420/1000 | train loss was: 0.064111 | val loss was: 0.501761\n",
      "epoch: 421/1000 | train loss was: 0.064078 | val loss was: 0.501776\n",
      "epoch: 422/1000 | train loss was: 0.064045 | val loss was: 0.501791\n",
      "epoch: 423/1000 | train loss was: 0.064012 | val loss was: 0.501806\n",
      "epoch: 424/1000 | train loss was: 0.063979 | val loss was: 0.501821\n",
      "epoch: 425/1000 | train loss was: 0.063946 | val loss was: 0.501838\n",
      "epoch: 426/1000 | train loss was: 0.063914 | val loss was: 0.501854\n",
      "epoch: 427/1000 | train loss was: 0.063882 | val loss was: 0.501871\n",
      "epoch: 428/1000 | train loss was: 0.06385 | val loss was: 0.501887\n",
      "epoch: 429/1000 | train loss was: 0.063818 | val loss was: 0.501904\n",
      "epoch: 430/1000 | train loss was: 0.063787 | val loss was: 0.501921\n",
      "epoch: 431/1000 | train loss was: 0.063755 | val loss was: 0.501937\n",
      "epoch: 432/1000 | train loss was: 0.063723 | val loss was: 0.501953\n",
      "epoch: 433/1000 | train loss was: 0.063692 | val loss was: 0.50197\n",
      "epoch: 434/1000 | train loss was: 0.063661 | val loss was: 0.501994\n",
      "epoch: 435/1000 | train loss was: 0.063629 | val loss was: 0.502018\n",
      "epoch: 436/1000 | train loss was: 0.063598 | val loss was: 0.502041\n",
      "epoch: 437/1000 | train loss was: 0.063567 | val loss was: 0.502064\n",
      "epoch: 438/1000 | train loss was: 0.063536 | val loss was: 0.502088\n",
      "epoch: 439/1000 | train loss was: 0.063505 | val loss was: 0.502111\n",
      "epoch: 440/1000 | train loss was: 0.063474 | val loss was: 0.502134\n",
      "epoch: 441/1000 | train loss was: 0.063444 | val loss was: 0.502157\n",
      "epoch: 442/1000 | train loss was: 0.063413 | val loss was: 0.50218\n",
      "epoch: 443/1000 | train loss was: 0.063383 | val loss was: 0.502203\n",
      "epoch: 444/1000 | train loss was: 0.063352 | val loss was: 0.502225\n",
      "epoch: 445/1000 | train loss was: 0.063322 | val loss was: 0.502248\n",
      "epoch: 446/1000 | train loss was: 0.063292 | val loss was: 0.50227\n",
      "epoch: 447/1000 | train loss was: 0.063262 | val loss was: 0.502293\n",
      "epoch: 448/1000 | train loss was: 0.063232 | val loss was: 0.502314\n",
      "epoch: 449/1000 | train loss was: 0.063202 | val loss was: 0.502336\n",
      "epoch: 450/1000 | train loss was: 0.063172 | val loss was: 0.502358\n",
      "epoch: 451/1000 | train loss was: 0.063142 | val loss was: 0.50238\n",
      "epoch: 452/1000 | train loss was: 0.063113 | val loss was: 0.502401\n",
      "epoch: 453/1000 | train loss was: 0.063083 | val loss was: 0.502423\n",
      "epoch: 454/1000 | train loss was: 0.063054 | val loss was: 0.502444\n",
      "epoch: 455/1000 | train loss was: 0.063024 | val loss was: 0.502466\n",
      "epoch: 456/1000 | train loss was: 0.062995 | val loss was: 0.502487\n",
      "epoch: 457/1000 | train loss was: 0.062966 | val loss was: 0.502507\n",
      "epoch: 458/1000 | train loss was: 0.062937 | val loss was: 0.502528\n",
      "epoch: 459/1000 | train loss was: 0.062908 | val loss was: 0.502549\n",
      "epoch: 460/1000 | train loss was: 0.062879 | val loss was: 0.50257\n",
      "epoch: 461/1000 | train loss was: 0.06285 | val loss was: 0.50259\n",
      "epoch: 462/1000 | train loss was: 0.062821 | val loss was: 0.50261\n",
      "epoch: 463/1000 | train loss was: 0.062793 | val loss was: 0.50263\n",
      "epoch: 464/1000 | train loss was: 0.062764 | val loss was: 0.50265\n",
      "epoch: 465/1000 | train loss was: 0.062736 | val loss was: 0.50267\n",
      "epoch: 466/1000 | train loss was: 0.062707 | val loss was: 0.50269\n",
      "epoch: 467/1000 | train loss was: 0.062679 | val loss was: 0.502709\n",
      "epoch: 468/1000 | train loss was: 0.062651 | val loss was: 0.502729\n",
      "epoch: 469/1000 | train loss was: 0.062622 | val loss was: 0.502748\n",
      "epoch: 470/1000 | train loss was: 0.062594 | val loss was: 0.502768\n",
      "epoch: 471/1000 | train loss was: 0.062566 | val loss was: 0.502786\n",
      "epoch: 472/1000 | train loss was: 0.062538 | val loss was: 0.502805\n",
      "epoch: 473/1000 | train loss was: 0.06251 | val loss was: 0.502824\n",
      "epoch: 474/1000 | train loss was: 0.062483 | val loss was: 0.502843\n",
      "epoch: 475/1000 | train loss was: 0.062455 | val loss was: 0.502861\n",
      "epoch: 476/1000 | train loss was: 0.062427 | val loss was: 0.502879\n",
      "epoch: 477/1000 | train loss was: 0.0624 | val loss was: 0.502897\n",
      "epoch: 478/1000 | train loss was: 0.062372 | val loss was: 0.502916\n",
      "epoch: 479/1000 | train loss was: 0.062344 | val loss was: 0.502934\n",
      "epoch: 480/1000 | train loss was: 0.062317 | val loss was: 0.502952\n",
      "epoch: 481/1000 | train loss was: 0.06229 | val loss was: 0.502969\n",
      "epoch: 482/1000 | train loss was: 0.062263 | val loss was: 0.502986\n",
      "epoch: 483/1000 | train loss was: 0.062235 | val loss was: 0.503004\n",
      "epoch: 484/1000 | train loss was: 0.062208 | val loss was: 0.503021\n",
      "epoch: 485/1000 | train loss was: 0.062181 | val loss was: 0.503039\n",
      "epoch: 486/1000 | train loss was: 0.062154 | val loss was: 0.503055\n",
      "epoch: 487/1000 | train loss was: 0.062127 | val loss was: 0.503072\n",
      "epoch: 488/1000 | train loss was: 0.0621 | val loss was: 0.503089\n",
      "epoch: 489/1000 | train loss was: 0.062074 | val loss was: 0.503106\n",
      "epoch: 490/1000 | train loss was: 0.062047 | val loss was: 0.503105\n",
      "epoch: 491/1000 | train loss was: 0.062023 | val loss was: 0.503121\n",
      "epoch: 492/1000 | train loss was: 0.061997 | val loss was: 0.503138\n",
      "epoch: 493/1000 | train loss was: 0.06197 | val loss was: 0.503137\n",
      "epoch: 494/1000 | train loss was: 0.061947 | val loss was: 0.503154\n",
      "epoch: 495/1000 | train loss was: 0.06192 | val loss was: 0.503171\n",
      "epoch: 496/1000 | train loss was: 0.061894 | val loss was: 0.50317\n",
      "epoch: 497/1000 | train loss was: 0.06187 | val loss was: 0.503186\n",
      "epoch: 498/1000 | train loss was: 0.061844 | val loss was: 0.503203\n",
      "epoch: 499/1000 | train loss was: 0.061818 | val loss was: 0.503202\n",
      "epoch: 500/1000 | train loss was: 0.061794 | val loss was: 0.503219\n",
      "epoch: 501/1000 | train loss was: 0.061768 | val loss was: 0.503218\n",
      "epoch: 502/1000 | train loss was: 0.061744 | val loss was: 0.503235\n",
      "epoch: 503/1000 | train loss was: 0.061718 | val loss was: 0.503251\n",
      "epoch: 504/1000 | train loss was: 0.061692 | val loss was: 0.50325\n",
      "epoch: 505/1000 | train loss was: 0.061668 | val loss was: 0.503266\n",
      "epoch: 506/1000 | train loss was: 0.061642 | val loss was: 0.503266\n",
      "epoch: 507/1000 | train loss was: 0.061619 | val loss was: 0.503282\n",
      "epoch: 508/1000 | train loss was: 0.061593 | val loss was: 0.503298\n",
      "epoch: 509/1000 | train loss was: 0.061567 | val loss was: 0.503301\n",
      "epoch: 510/1000 | train loss was: 0.061542 | val loss was: 0.50332\n",
      "epoch: 511/1000 | train loss was: 0.061516 | val loss was: 0.503323\n",
      "epoch: 512/1000 | train loss was: 0.061492 | val loss was: 0.503343\n",
      "epoch: 513/1000 | train loss was: 0.061465 | val loss was: 0.503362\n",
      "epoch: 514/1000 | train loss was: 0.061439 | val loss was: 0.503365\n",
      "epoch: 515/1000 | train loss was: 0.061415 | val loss was: 0.503384\n",
      "epoch: 516/1000 | train loss was: 0.061388 | val loss was: 0.503387\n",
      "epoch: 517/1000 | train loss was: 0.061364 | val loss was: 0.503406\n",
      "epoch: 518/1000 | train loss was: 0.061338 | val loss was: 0.503426\n",
      "epoch: 519/1000 | train loss was: 0.061312 | val loss was: 0.503429\n",
      "epoch: 520/1000 | train loss was: 0.061288 | val loss was: 0.503448\n",
      "epoch: 521/1000 | train loss was: 0.061262 | val loss was: 0.503451\n",
      "epoch: 522/1000 | train loss was: 0.061238 | val loss was: 0.50347\n",
      "epoch: 523/1000 | train loss was: 0.061211 | val loss was: 0.503473\n",
      "epoch: 524/1000 | train loss was: 0.061188 | val loss was: 0.503493\n",
      "epoch: 525/1000 | train loss was: 0.061161 | val loss was: 0.503512\n",
      "epoch: 526/1000 | train loss was: 0.061136 | val loss was: 0.503515\n",
      "epoch: 527/1000 | train loss was: 0.061112 | val loss was: 0.503534\n",
      "epoch: 528/1000 | train loss was: 0.061086 | val loss was: 0.503537\n",
      "epoch: 529/1000 | train loss was: 0.061062 | val loss was: 0.503556\n",
      "epoch: 530/1000 | train loss was: 0.061037 | val loss was: 0.503559\n",
      "epoch: 531/1000 | train loss was: 0.061013 | val loss was: 0.503578\n",
      "epoch: 532/1000 | train loss was: 0.060987 | val loss was: 0.503581\n",
      "epoch: 533/1000 | train loss was: 0.060964 | val loss was: 0.5036\n",
      "epoch: 534/1000 | train loss was: 0.060938 | val loss was: 0.503603\n",
      "epoch: 535/1000 | train loss was: 0.060914 | val loss was: 0.503622\n",
      "epoch: 536/1000 | train loss was: 0.060888 | val loss was: 0.503641\n",
      "epoch: 537/1000 | train loss was: 0.060863 | val loss was: 0.503644\n",
      "epoch: 538/1000 | train loss was: 0.060839 | val loss was: 0.503662\n",
      "epoch: 539/1000 | train loss was: 0.060814 | val loss was: 0.503665\n",
      "epoch: 540/1000 | train loss was: 0.060791 | val loss was: 0.503684\n",
      "epoch: 541/1000 | train loss was: 0.060765 | val loss was: 0.503687\n",
      "epoch: 542/1000 | train loss was: 0.060742 | val loss was: 0.503705\n",
      "epoch: 543/1000 | train loss was: 0.060717 | val loss was: 0.503708\n",
      "epoch: 544/1000 | train loss was: 0.060693 | val loss was: 0.503726\n",
      "epoch: 545/1000 | train loss was: 0.060668 | val loss was: 0.503729\n",
      "epoch: 546/1000 | train loss was: 0.060645 | val loss was: 0.503748\n",
      "epoch: 547/1000 | train loss was: 0.06062 | val loss was: 0.50375\n",
      "epoch: 548/1000 | train loss was: 0.060597 | val loss was: 0.503769\n",
      "epoch: 549/1000 | train loss was: 0.060572 | val loss was: 0.503771\n",
      "epoch: 550/1000 | train loss was: 0.060548 | val loss was: 0.503789\n",
      "epoch: 551/1000 | train loss was: 0.060523 | val loss was: 0.503792\n",
      "epoch: 552/1000 | train loss was: 0.0605 | val loss was: 0.50381\n",
      "epoch: 553/1000 | train loss was: 0.060475 | val loss was: 0.503813\n",
      "epoch: 554/1000 | train loss was: 0.060452 | val loss was: 0.503831\n",
      "epoch: 555/1000 | train loss was: 0.060428 | val loss was: 0.503833\n",
      "epoch: 556/1000 | train loss was: 0.060405 | val loss was: 0.503851\n",
      "epoch: 557/1000 | train loss was: 0.06038 | val loss was: 0.503854\n",
      "epoch: 558/1000 | train loss was: 0.060357 | val loss was: 0.503871\n",
      "epoch: 559/1000 | train loss was: 0.060332 | val loss was: 0.503874\n",
      "epoch: 560/1000 | train loss was: 0.060309 | val loss was: 0.503891\n",
      "epoch: 561/1000 | train loss was: 0.060285 | val loss was: 0.503894\n",
      "epoch: 562/1000 | train loss was: 0.060262 | val loss was: 0.503911\n",
      "epoch: 563/1000 | train loss was: 0.060238 | val loss was: 0.503913\n",
      "epoch: 564/1000 | train loss was: 0.060215 | val loss was: 0.503931\n",
      "epoch: 565/1000 | train loss was: 0.060191 | val loss was: 0.503933\n",
      "epoch: 566/1000 | train loss was: 0.060168 | val loss was: 0.50395\n",
      "epoch: 567/1000 | train loss was: 0.060144 | val loss was: 0.503952\n",
      "epoch: 568/1000 | train loss was: 0.060121 | val loss was: 0.503954\n",
      "epoch: 569/1000 | train loss was: 0.060099 | val loss was: 0.503971\n",
      "epoch: 570/1000 | train loss was: 0.060074 | val loss was: 0.503974\n",
      "epoch: 571/1000 | train loss was: 0.060052 | val loss was: 0.50399\n",
      "epoch: 572/1000 | train loss was: 0.060027 | val loss was: 0.503993\n",
      "epoch: 573/1000 | train loss was: 0.060005 | val loss was: 0.504009\n",
      "epoch: 574/1000 | train loss was: 0.059981 | val loss was: 0.504012\n",
      "epoch: 575/1000 | train loss was: 0.059959 | val loss was: 0.504028\n",
      "epoch: 576/1000 | train loss was: 0.059935 | val loss was: 0.50403\n",
      "epoch: 577/1000 | train loss was: 0.059912 | val loss was: 0.504046\n",
      "epoch: 578/1000 | train loss was: 0.059889 | val loss was: 0.504048\n",
      "epoch: 579/1000 | train loss was: 0.059866 | val loss was: 0.504065\n",
      "epoch: 580/1000 | train loss was: 0.059843 | val loss was: 0.504066\n",
      "epoch: 581/1000 | train loss was: 0.05982 | val loss was: 0.504068\n",
      "epoch: 582/1000 | train loss was: 0.059798 | val loss was: 0.504085\n",
      "epoch: 583/1000 | train loss was: 0.059774 | val loss was: 0.504087\n",
      "epoch: 584/1000 | train loss was: 0.059753 | val loss was: 0.504102\n",
      "epoch: 585/1000 | train loss was: 0.059729 | val loss was: 0.504104\n",
      "epoch: 586/1000 | train loss was: 0.059707 | val loss was: 0.50412\n",
      "epoch: 587/1000 | train loss was: 0.059683 | val loss was: 0.504122\n",
      "epoch: 588/1000 | train loss was: 0.059661 | val loss was: 0.504124\n",
      "epoch: 589/1000 | train loss was: 0.05964 | val loss was: 0.504139\n",
      "epoch: 590/1000 | train loss was: 0.059616 | val loss was: 0.504141\n",
      "epoch: 591/1000 | train loss was: 0.059594 | val loss was: 0.504157\n",
      "epoch: 592/1000 | train loss was: 0.059571 | val loss was: 0.504158\n",
      "epoch: 593/1000 | train loss was: 0.059549 | val loss was: 0.504174\n",
      "epoch: 594/1000 | train loss was: 0.059526 | val loss was: 0.504175\n",
      "epoch: 595/1000 | train loss was: 0.059504 | val loss was: 0.504177\n",
      "epoch: 596/1000 | train loss was: 0.059482 | val loss was: 0.504192\n",
      "epoch: 597/1000 | train loss was: 0.059459 | val loss was: 0.504193\n",
      "epoch: 598/1000 | train loss was: 0.059437 | val loss was: 0.504208\n",
      "epoch: 599/1000 | train loss was: 0.059414 | val loss was: 0.50421\n",
      "epoch: 600/1000 | train loss was: 0.059393 | val loss was: 0.504225\n",
      "epoch: 601/1000 | train loss was: 0.05937 | val loss was: 0.504226\n",
      "epoch: 602/1000 | train loss was: 0.059348 | val loss was: 0.504227\n",
      "epoch: 603/1000 | train loss was: 0.059327 | val loss was: 0.504242\n",
      "epoch: 604/1000 | train loss was: 0.059304 | val loss was: 0.504243\n",
      "epoch: 605/1000 | train loss was: 0.059282 | val loss was: 0.504257\n",
      "epoch: 606/1000 | train loss was: 0.059259 | val loss was: 0.504258\n",
      "epoch: 607/1000 | train loss was: 0.059238 | val loss was: 0.504259\n",
      "epoch: 608/1000 | train loss was: 0.059217 | val loss was: 0.504274\n",
      "epoch: 609/1000 | train loss was: 0.059194 | val loss was: 0.504275\n",
      "epoch: 610/1000 | train loss was: 0.059173 | val loss was: 0.504289\n",
      "epoch: 611/1000 | train loss was: 0.05915 | val loss was: 0.50429\n",
      "epoch: 612/1000 | train loss was: 0.059129 | val loss was: 0.504291\n",
      "epoch: 613/1000 | train loss was: 0.059108 | val loss was: 0.504305\n",
      "epoch: 614/1000 | train loss was: 0.059085 | val loss was: 0.504305\n",
      "epoch: 615/1000 | train loss was: 0.059064 | val loss was: 0.504319\n",
      "epoch: 616/1000 | train loss was: 0.059042 | val loss was: 0.50432\n",
      "epoch: 617/1000 | train loss was: 0.05902 | val loss was: 0.50432\n",
      "epoch: 618/1000 | train loss was: 0.059 | val loss was: 0.504334\n",
      "epoch: 619/1000 | train loss was: 0.058977 | val loss was: 0.504335\n",
      "epoch: 620/1000 | train loss was: 0.058956 | val loss was: 0.504335\n",
      "epoch: 621/1000 | train loss was: 0.058936 | val loss was: 0.504349\n",
      "epoch: 622/1000 | train loss was: 0.058913 | val loss was: 0.504349\n",
      "epoch: 623/1000 | train loss was: 0.058892 | val loss was: 0.504362\n",
      "epoch: 624/1000 | train loss was: 0.05887 | val loss was: 0.504363\n",
      "epoch: 625/1000 | train loss was: 0.058849 | val loss was: 0.504363\n",
      "epoch: 626/1000 | train loss was: 0.058829 | val loss was: 0.504376\n",
      "epoch: 627/1000 | train loss was: 0.058806 | val loss was: 0.504376\n",
      "epoch: 628/1000 | train loss was: 0.058785 | val loss was: 0.504377\n",
      "epoch: 629/1000 | train loss was: 0.058765 | val loss was: 0.50439\n",
      "epoch: 630/1000 | train loss was: 0.058743 | val loss was: 0.50439\n",
      "epoch: 631/1000 | train loss was: 0.058722 | val loss was: 0.50439\n",
      "epoch: 632/1000 | train loss was: 0.058702 | val loss was: 0.504403\n",
      "epoch: 633/1000 | train loss was: 0.05868 | val loss was: 0.504403\n",
      "epoch: 634/1000 | train loss was: 0.058659 | val loss was: 0.504415\n",
      "epoch: 635/1000 | train loss was: 0.058638 | val loss was: 0.504415\n",
      "epoch: 636/1000 | train loss was: 0.058617 | val loss was: 0.504415\n",
      "epoch: 637/1000 | train loss was: 0.058597 | val loss was: 0.504427\n",
      "epoch: 638/1000 | train loss was: 0.058575 | val loss was: 0.504427\n",
      "epoch: 639/1000 | train loss was: 0.058554 | val loss was: 0.504427\n",
      "epoch: 640/1000 | train loss was: 0.058534 | val loss was: 0.504439\n",
      "epoch: 641/1000 | train loss was: 0.058513 | val loss was: 0.504438\n",
      "epoch: 642/1000 | train loss was: 0.058492 | val loss was: 0.504438\n",
      "epoch: 643/1000 | train loss was: 0.058472 | val loss was: 0.50445\n",
      "epoch: 644/1000 | train loss was: 0.058451 | val loss was: 0.50445\n",
      "epoch: 645/1000 | train loss was: 0.05843 | val loss was: 0.504449\n",
      "epoch: 646/1000 | train loss was: 0.05841 | val loss was: 0.504461\n",
      "epoch: 647/1000 | train loss was: 0.058389 | val loss was: 0.50446\n",
      "epoch: 648/1000 | train loss was: 0.058369 | val loss was: 0.50446\n",
      "epoch: 649/1000 | train loss was: 0.058349 | val loss was: 0.504471\n",
      "epoch: 650/1000 | train loss was: 0.058327 | val loss was: 0.50447\n",
      "epoch: 651/1000 | train loss was: 0.058307 | val loss was: 0.50447\n",
      "epoch: 652/1000 | train loss was: 0.058288 | val loss was: 0.504481\n",
      "epoch: 653/1000 | train loss was: 0.058266 | val loss was: 0.50448\n",
      "epoch: 654/1000 | train loss was: 0.058246 | val loss was: 0.504478\n",
      "epoch: 655/1000 | train loss was: 0.058227 | val loss was: 0.504488\n",
      "epoch: 656/1000 | train loss was: 0.058205 | val loss was: 0.504486\n",
      "epoch: 657/1000 | train loss was: 0.058185 | val loss was: 0.504484\n",
      "epoch: 658/1000 | train loss was: 0.058166 | val loss was: 0.504494\n",
      "epoch: 659/1000 | train loss was: 0.058145 | val loss was: 0.504491\n",
      "epoch: 660/1000 | train loss was: 0.058125 | val loss was: 0.504489\n",
      "epoch: 661/1000 | train loss was: 0.058105 | val loss was: 0.504487\n",
      "epoch: 662/1000 | train loss was: 0.058086 | val loss was: 0.504496\n",
      "epoch: 663/1000 | train loss was: 0.058065 | val loss was: 0.504494\n",
      "epoch: 664/1000 | train loss was: 0.058045 | val loss was: 0.504492\n",
      "epoch: 665/1000 | train loss was: 0.058026 | val loss was: 0.504501\n",
      "epoch: 666/1000 | train loss was: 0.058005 | val loss was: 0.504498\n",
      "epoch: 667/1000 | train loss was: 0.057985 | val loss was: 0.504496\n",
      "epoch: 668/1000 | train loss was: 0.057966 | val loss was: 0.504505\n",
      "epoch: 669/1000 | train loss was: 0.057945 | val loss was: 0.504502\n",
      "epoch: 670/1000 | train loss was: 0.057926 | val loss was: 0.5045\n",
      "epoch: 671/1000 | train loss was: 0.057906 | val loss was: 0.504497\n",
      "epoch: 672/1000 | train loss was: 0.057887 | val loss was: 0.504506\n",
      "epoch: 673/1000 | train loss was: 0.057866 | val loss was: 0.504503\n",
      "epoch: 674/1000 | train loss was: 0.057847 | val loss was: 0.5045\n",
      "epoch: 675/1000 | train loss was: 0.057828 | val loss was: 0.504509\n",
      "epoch: 676/1000 | train loss was: 0.057808 | val loss was: 0.504506\n",
      "epoch: 677/1000 | train loss was: 0.057788 | val loss was: 0.504503\n",
      "epoch: 678/1000 | train loss was: 0.057769 | val loss was: 0.5045\n",
      "epoch: 679/1000 | train loss was: 0.05775 | val loss was: 0.504509\n",
      "epoch: 680/1000 | train loss was: 0.05773 | val loss was: 0.504506\n",
      "epoch: 681/1000 | train loss was: 0.05771 | val loss was: 0.504502\n",
      "epoch: 682/1000 | train loss was: 0.057691 | val loss was: 0.504499\n",
      "epoch: 683/1000 | train loss was: 0.057672 | val loss was: 0.504507\n",
      "epoch: 684/1000 | train loss was: 0.057651 | val loss was: 0.504503\n",
      "epoch: 685/1000 | train loss was: 0.057631 | val loss was: 0.504499\n",
      "epoch: 686/1000 | train loss was: 0.057611 | val loss was: 0.504495\n",
      "epoch: 687/1000 | train loss was: 0.057592 | val loss was: 0.504502\n",
      "epoch: 688/1000 | train loss was: 0.057571 | val loss was: 0.504498\n",
      "epoch: 689/1000 | train loss was: 0.057551 | val loss was: 0.504493\n",
      "epoch: 690/1000 | train loss was: 0.057531 | val loss was: 0.504489\n",
      "epoch: 691/1000 | train loss was: 0.057512 | val loss was: 0.504496\n",
      "epoch: 692/1000 | train loss was: 0.057491 | val loss was: 0.504492\n",
      "epoch: 693/1000 | train loss was: 0.057472 | val loss was: 0.504488\n",
      "epoch: 694/1000 | train loss was: 0.057452 | val loss was: 0.504484\n",
      "epoch: 695/1000 | train loss was: 0.057433 | val loss was: 0.504491\n",
      "epoch: 696/1000 | train loss was: 0.057412 | val loss was: 0.504486\n",
      "epoch: 697/1000 | train loss was: 0.057393 | val loss was: 0.504482\n",
      "epoch: 698/1000 | train loss was: 0.057373 | val loss was: 0.504478\n",
      "epoch: 699/1000 | train loss was: 0.057354 | val loss was: 0.504483\n",
      "epoch: 700/1000 | train loss was: 0.057334 | val loss was: 0.504478\n",
      "epoch: 701/1000 | train loss was: 0.057314 | val loss was: 0.504473\n",
      "epoch: 702/1000 | train loss was: 0.057294 | val loss was: 0.504468\n",
      "epoch: 703/1000 | train loss was: 0.057275 | val loss was: 0.504463\n",
      "epoch: 704/1000 | train loss was: 0.057256 | val loss was: 0.504468\n",
      "epoch: 705/1000 | train loss was: 0.057235 | val loss was: 0.504464\n",
      "epoch: 706/1000 | train loss was: 0.057216 | val loss was: 0.504458\n",
      "epoch: 707/1000 | train loss was: 0.057196 | val loss was: 0.504453\n",
      "epoch: 708/1000 | train loss was: 0.057177 | val loss was: 0.504447\n",
      "epoch: 709/1000 | train loss was: 0.057158 | val loss was: 0.504451\n",
      "epoch: 710/1000 | train loss was: 0.057138 | val loss was: 0.504444\n",
      "epoch: 711/1000 | train loss was: 0.057118 | val loss was: 0.504439\n",
      "epoch: 712/1000 | train loss was: 0.057099 | val loss was: 0.504432\n",
      "epoch: 713/1000 | train loss was: 0.05708 | val loss was: 0.504426\n",
      "epoch: 714/1000 | train loss was: 0.057061 | val loss was: 0.50443\n",
      "epoch: 715/1000 | train loss was: 0.057041 | val loss was: 0.504423\n",
      "epoch: 716/1000 | train loss was: 0.057022 | val loss was: 0.504416\n",
      "epoch: 717/1000 | train loss was: 0.057003 | val loss was: 0.50441\n",
      "epoch: 718/1000 | train loss was: 0.056983 | val loss was: 0.504403\n",
      "epoch: 719/1000 | train loss was: 0.056965 | val loss was: 0.504408\n",
      "epoch: 720/1000 | train loss was: 0.056945 | val loss was: 0.504401\n",
      "epoch: 721/1000 | train loss was: 0.056926 | val loss was: 0.504394\n",
      "epoch: 722/1000 | train loss was: 0.056907 | val loss was: 0.504388\n",
      "epoch: 723/1000 | train loss was: 0.056888 | val loss was: 0.504381\n",
      "epoch: 724/1000 | train loss was: 0.056869 | val loss was: 0.504374\n",
      "epoch: 725/1000 | train loss was: 0.05685 | val loss was: 0.504378\n",
      "epoch: 726/1000 | train loss was: 0.056831 | val loss was: 0.504371\n",
      "epoch: 727/1000 | train loss was: 0.056812 | val loss was: 0.504364\n",
      "epoch: 728/1000 | train loss was: 0.056793 | val loss was: 0.504357\n",
      "epoch: 729/1000 | train loss was: 0.056775 | val loss was: 0.504352\n",
      "epoch: 730/1000 | train loss was: 0.056756 | val loss was: 0.504347\n",
      "epoch: 731/1000 | train loss was: 0.056737 | val loss was: 0.504342\n",
      "epoch: 732/1000 | train loss was: 0.056718 | val loss was: 0.504336\n",
      "epoch: 733/1000 | train loss was: 0.0567 | val loss was: 0.504331\n",
      "epoch: 734/1000 | train loss was: 0.056681 | val loss was: 0.504326\n",
      "epoch: 735/1000 | train loss was: 0.056663 | val loss was: 0.50432\n",
      "epoch: 736/1000 | train loss was: 0.056644 | val loss was: 0.504315\n",
      "epoch: 737/1000 | train loss was: 0.056625 | val loss was: 0.504309\n",
      "epoch: 738/1000 | train loss was: 0.056607 | val loss was: 0.504302\n",
      "epoch: 739/1000 | train loss was: 0.056589 | val loss was: 0.504296\n",
      "epoch: 740/1000 | train loss was: 0.05657 | val loss was: 0.504291\n",
      "epoch: 741/1000 | train loss was: 0.056552 | val loss was: 0.504285\n",
      "epoch: 742/1000 | train loss was: 0.056534 | val loss was: 0.504279\n",
      "epoch: 743/1000 | train loss was: 0.056515 | val loss was: 0.504273\n",
      "epoch: 744/1000 | train loss was: 0.056497 | val loss was: 0.504268\n",
      "epoch: 745/1000 | train loss was: 0.056479 | val loss was: 0.504262\n",
      "epoch: 746/1000 | train loss was: 0.056461 | val loss was: 0.504256\n",
      "epoch: 747/1000 | train loss was: 0.056442 | val loss was: 0.50425\n",
      "epoch: 748/1000 | train loss was: 0.056424 | val loss was: 0.504244\n",
      "epoch: 749/1000 | train loss was: 0.056406 | val loss was: 0.504237\n",
      "epoch: 750/1000 | train loss was: 0.056388 | val loss was: 0.504231\n",
      "epoch: 751/1000 | train loss was: 0.05637 | val loss was: 0.504225\n",
      "epoch: 752/1000 | train loss was: 0.056352 | val loss was: 0.504219\n",
      "epoch: 753/1000 | train loss was: 0.056334 | val loss was: 0.504213\n",
      "epoch: 754/1000 | train loss was: 0.056317 | val loss was: 0.504207\n",
      "epoch: 755/1000 | train loss was: 0.056299 | val loss was: 0.504201\n",
      "epoch: 756/1000 | train loss was: 0.056281 | val loss was: 0.504195\n",
      "epoch: 757/1000 | train loss was: 0.056263 | val loss was: 0.504189\n",
      "epoch: 758/1000 | train loss was: 0.056245 | val loss was: 0.504183\n",
      "epoch: 759/1000 | train loss was: 0.056227 | val loss was: 0.504176\n",
      "epoch: 760/1000 | train loss was: 0.05621 | val loss was: 0.50417\n",
      "epoch: 761/1000 | train loss was: 0.056192 | val loss was: 0.504162\n",
      "epoch: 762/1000 | train loss was: 0.056175 | val loss was: 0.504156\n",
      "epoch: 763/1000 | train loss was: 0.056157 | val loss was: 0.504149\n",
      "epoch: 764/1000 | train loss was: 0.056139 | val loss was: 0.504143\n",
      "epoch: 765/1000 | train loss was: 0.056122 | val loss was: 0.504137\n",
      "epoch: 766/1000 | train loss was: 0.056104 | val loss was: 0.504131\n",
      "epoch: 767/1000 | train loss was: 0.056087 | val loss was: 0.504124\n",
      "epoch: 768/1000 | train loss was: 0.05607 | val loss was: 0.504118\n",
      "epoch: 769/1000 | train loss was: 0.056052 | val loss was: 0.504112\n",
      "epoch: 770/1000 | train loss was: 0.056035 | val loss was: 0.504105\n",
      "epoch: 771/1000 | train loss was: 0.056017 | val loss was: 0.504099\n",
      "epoch: 772/1000 | train loss was: 0.056 | val loss was: 0.504093\n",
      "epoch: 773/1000 | train loss was: 0.055983 | val loss was: 0.504086\n",
      "epoch: 774/1000 | train loss was: 0.055965 | val loss was: 0.504078\n",
      "epoch: 775/1000 | train loss was: 0.055949 | val loss was: 0.504072\n",
      "epoch: 776/1000 | train loss was: 0.055931 | val loss was: 0.504066\n",
      "epoch: 777/1000 | train loss was: 0.055914 | val loss was: 0.504059\n",
      "epoch: 778/1000 | train loss was: 0.055897 | val loss was: 0.504053\n",
      "epoch: 779/1000 | train loss was: 0.05588 | val loss was: 0.504047\n",
      "epoch: 780/1000 | train loss was: 0.055863 | val loss was: 0.50404\n",
      "epoch: 781/1000 | train loss was: 0.055846 | val loss was: 0.504034\n",
      "epoch: 782/1000 | train loss was: 0.055829 | val loss was: 0.504028\n",
      "epoch: 783/1000 | train loss was: 0.055812 | val loss was: 0.504021\n",
      "epoch: 784/1000 | train loss was: 0.055795 | val loss was: 0.504015\n",
      "epoch: 785/1000 | train loss was: 0.055779 | val loss was: 0.504009\n",
      "epoch: 786/1000 | train loss was: 0.055762 | val loss was: 0.504002\n",
      "epoch: 787/1000 | train loss was: 0.055745 | val loss was: 0.503996\n",
      "epoch: 788/1000 | train loss was: 0.055728 | val loss was: 0.50399\n",
      "epoch: 789/1000 | train loss was: 0.055712 | val loss was: 0.503983\n",
      "epoch: 790/1000 | train loss was: 0.055694 | val loss was: 0.503975\n",
      "epoch: 791/1000 | train loss was: 0.055678 | val loss was: 0.503969\n",
      "epoch: 792/1000 | train loss was: 0.055662 | val loss was: 0.503963\n",
      "epoch: 793/1000 | train loss was: 0.055645 | val loss was: 0.503956\n",
      "epoch: 794/1000 | train loss was: 0.055629 | val loss was: 0.50395\n",
      "epoch: 795/1000 | train loss was: 0.055612 | val loss was: 0.503944\n",
      "epoch: 796/1000 | train loss was: 0.055596 | val loss was: 0.503937\n",
      "epoch: 797/1000 | train loss was: 0.055579 | val loss was: 0.503931\n",
      "epoch: 798/1000 | train loss was: 0.055563 | val loss was: 0.503925\n",
      "epoch: 799/1000 | train loss was: 0.055546 | val loss was: 0.503918\n",
      "epoch: 800/1000 | train loss was: 0.05553 | val loss was: 0.503912\n",
      "epoch: 801/1000 | train loss was: 0.055514 | val loss was: 0.503906\n",
      "epoch: 802/1000 | train loss was: 0.055498 | val loss was: 0.503899\n",
      "epoch: 803/1000 | train loss was: 0.055481 | val loss was: 0.503893\n",
      "epoch: 804/1000 | train loss was: 0.055465 | val loss was: 0.503887\n",
      "epoch: 805/1000 | train loss was: 0.055449 | val loss was: 0.50388\n",
      "epoch: 806/1000 | train loss was: 0.055433 | val loss was: 0.503874\n",
      "epoch: 807/1000 | train loss was: 0.055417 | val loss was: 0.503868\n",
      "epoch: 808/1000 | train loss was: 0.055401 | val loss was: 0.503861\n",
      "epoch: 809/1000 | train loss was: 0.055384 | val loss was: 0.503853\n",
      "epoch: 810/1000 | train loss was: 0.055369 | val loss was: 0.503847\n",
      "epoch: 811/1000 | train loss was: 0.055353 | val loss was: 0.503841\n",
      "epoch: 812/1000 | train loss was: 0.055337 | val loss was: 0.503835\n",
      "epoch: 813/1000 | train loss was: 0.055321 | val loss was: 0.503829\n",
      "epoch: 814/1000 | train loss was: 0.055305 | val loss was: 0.503822\n",
      "epoch: 815/1000 | train loss was: 0.055289 | val loss was: 0.503816\n",
      "epoch: 816/1000 | train loss was: 0.055274 | val loss was: 0.50381\n",
      "epoch: 817/1000 | train loss was: 0.055258 | val loss was: 0.503804\n",
      "epoch: 818/1000 | train loss was: 0.055242 | val loss was: 0.503798\n",
      "epoch: 819/1000 | train loss was: 0.055226 | val loss was: 0.503791\n",
      "epoch: 820/1000 | train loss was: 0.055211 | val loss was: 0.503785\n",
      "epoch: 821/1000 | train loss was: 0.055195 | val loss was: 0.503779\n",
      "epoch: 822/1000 | train loss was: 0.05518 | val loss was: 0.503773\n",
      "epoch: 823/1000 | train loss was: 0.055164 | val loss was: 0.503767\n",
      "epoch: 824/1000 | train loss was: 0.055149 | val loss was: 0.503761\n",
      "epoch: 825/1000 | train loss was: 0.055133 | val loss was: 0.503755\n",
      "epoch: 826/1000 | train loss was: 0.055118 | val loss was: 0.503749\n",
      "epoch: 827/1000 | train loss was: 0.055102 | val loss was: 0.503742\n",
      "epoch: 828/1000 | train loss was: 0.055087 | val loss was: 0.503736\n",
      "epoch: 829/1000 | train loss was: 0.055072 | val loss was: 0.50373\n",
      "epoch: 830/1000 | train loss was: 0.055056 | val loss was: 0.503724\n",
      "epoch: 831/1000 | train loss was: 0.055041 | val loss was: 0.503718\n",
      "epoch: 832/1000 | train loss was: 0.055026 | val loss was: 0.503712\n",
      "epoch: 833/1000 | train loss was: 0.055011 | val loss was: 0.503706\n",
      "epoch: 834/1000 | train loss was: 0.054996 | val loss was: 0.5037\n",
      "epoch: 835/1000 | train loss was: 0.054981 | val loss was: 0.503694\n",
      "epoch: 836/1000 | train loss was: 0.054965 | val loss was: 0.503688\n",
      "epoch: 837/1000 | train loss was: 0.05495 | val loss was: 0.503682\n",
      "epoch: 838/1000 | train loss was: 0.054935 | val loss was: 0.503676\n",
      "epoch: 839/1000 | train loss was: 0.05492 | val loss was: 0.503671\n",
      "epoch: 840/1000 | train loss was: 0.054906 | val loss was: 0.503665\n",
      "epoch: 841/1000 | train loss was: 0.054891 | val loss was: 0.503659\n",
      "epoch: 842/1000 | train loss was: 0.054876 | val loss was: 0.503653\n",
      "epoch: 843/1000 | train loss was: 0.054861 | val loss was: 0.503647\n",
      "epoch: 844/1000 | train loss was: 0.054846 | val loss was: 0.503641\n",
      "epoch: 845/1000 | train loss was: 0.054831 | val loss was: 0.503636\n",
      "epoch: 846/1000 | train loss was: 0.054817 | val loss was: 0.50363\n",
      "epoch: 847/1000 | train loss was: 0.054802 | val loss was: 0.503624\n",
      "epoch: 848/1000 | train loss was: 0.054787 | val loss was: 0.503617\n",
      "epoch: 849/1000 | train loss was: 0.054772 | val loss was: 0.503609\n",
      "epoch: 850/1000 | train loss was: 0.054758 | val loss was: 0.503602\n",
      "epoch: 851/1000 | train loss was: 0.054743 | val loss was: 0.503594\n",
      "epoch: 852/1000 | train loss was: 0.054728 | val loss was: 0.503587\n",
      "epoch: 853/1000 | train loss was: 0.054714 | val loss was: 0.50358\n",
      "epoch: 854/1000 | train loss was: 0.054699 | val loss was: 0.503573\n",
      "epoch: 855/1000 | train loss was: 0.054685 | val loss was: 0.503565\n",
      "epoch: 856/1000 | train loss was: 0.05467 | val loss was: 0.503559\n",
      "epoch: 857/1000 | train loss was: 0.054656 | val loss was: 0.503552\n",
      "epoch: 858/1000 | train loss was: 0.054642 | val loss was: 0.503545\n",
      "epoch: 859/1000 | train loss was: 0.054627 | val loss was: 0.503538\n",
      "epoch: 860/1000 | train loss was: 0.054613 | val loss was: 0.503532\n",
      "epoch: 861/1000 | train loss was: 0.054599 | val loss was: 0.503525\n",
      "epoch: 862/1000 | train loss was: 0.054584 | val loss was: 0.503519\n",
      "epoch: 863/1000 | train loss was: 0.05457 | val loss was: 0.503512\n",
      "epoch: 864/1000 | train loss was: 0.054556 | val loss was: 0.503506\n",
      "epoch: 865/1000 | train loss was: 0.054542 | val loss was: 0.5035\n",
      "epoch: 866/1000 | train loss was: 0.054528 | val loss was: 0.503493\n",
      "epoch: 867/1000 | train loss was: 0.054514 | val loss was: 0.503487\n",
      "epoch: 868/1000 | train loss was: 0.0545 | val loss was: 0.503481\n",
      "epoch: 869/1000 | train loss was: 0.054486 | val loss was: 0.503475\n",
      "epoch: 870/1000 | train loss was: 0.054472 | val loss was: 0.503469\n",
      "epoch: 871/1000 | train loss was: 0.054458 | val loss was: 0.503464\n",
      "epoch: 872/1000 | train loss was: 0.054444 | val loss was: 0.503458\n",
      "epoch: 873/1000 | train loss was: 0.05443 | val loss was: 0.503452\n",
      "epoch: 874/1000 | train loss was: 0.054416 | val loss was: 0.503447\n",
      "epoch: 875/1000 | train loss was: 0.054402 | val loss was: 0.503441\n",
      "epoch: 876/1000 | train loss was: 0.054388 | val loss was: 0.503436\n",
      "epoch: 877/1000 | train loss was: 0.054375 | val loss was: 0.503431\n",
      "epoch: 878/1000 | train loss was: 0.054361 | val loss was: 0.503425\n",
      "epoch: 879/1000 | train loss was: 0.054347 | val loss was: 0.50342\n",
      "epoch: 880/1000 | train loss was: 0.054334 | val loss was: 0.503415\n",
      "epoch: 881/1000 | train loss was: 0.05432 | val loss was: 0.50341\n",
      "epoch: 882/1000 | train loss was: 0.054306 | val loss was: 0.503405\n",
      "epoch: 883/1000 | train loss was: 0.054293 | val loss was: 0.5034\n",
      "epoch: 884/1000 | train loss was: 0.054279 | val loss was: 0.503395\n",
      "epoch: 885/1000 | train loss was: 0.054266 | val loss was: 0.50339\n",
      "epoch: 886/1000 | train loss was: 0.054252 | val loss was: 0.503386\n",
      "epoch: 887/1000 | train loss was: 0.054239 | val loss was: 0.503381\n",
      "epoch: 888/1000 | train loss was: 0.054226 | val loss was: 0.503376\n",
      "epoch: 889/1000 | train loss was: 0.054212 | val loss was: 0.503372\n",
      "epoch: 890/1000 | train loss was: 0.054199 | val loss was: 0.503367\n",
      "epoch: 891/1000 | train loss was: 0.054186 | val loss was: 0.503363\n",
      "epoch: 892/1000 | train loss was: 0.054172 | val loss was: 0.503358\n",
      "epoch: 893/1000 | train loss was: 0.054159 | val loss was: 0.503354\n",
      "epoch: 894/1000 | train loss was: 0.054146 | val loss was: 0.50335\n",
      "epoch: 895/1000 | train loss was: 0.054133 | val loss was: 0.503345\n",
      "epoch: 896/1000 | train loss was: 0.05412 | val loss was: 0.503341\n",
      "epoch: 897/1000 | train loss was: 0.054107 | val loss was: 0.503337\n",
      "epoch: 898/1000 | train loss was: 0.054094 | val loss was: 0.503333\n",
      "epoch: 899/1000 | train loss was: 0.054081 | val loss was: 0.503329\n",
      "epoch: 900/1000 | train loss was: 0.054068 | val loss was: 0.503325\n",
      "epoch: 901/1000 | train loss was: 0.054055 | val loss was: 0.503322\n",
      "epoch: 902/1000 | train loss was: 0.054042 | val loss was: 0.503318\n",
      "epoch: 903/1000 | train loss was: 0.054029 | val loss was: 0.503314\n",
      "epoch: 904/1000 | train loss was: 0.054016 | val loss was: 0.503311\n",
      "epoch: 905/1000 | train loss was: 0.054003 | val loss was: 0.503307\n",
      "epoch: 906/1000 | train loss was: 0.05399 | val loss was: 0.503303\n",
      "epoch: 907/1000 | train loss was: 0.053977 | val loss was: 0.5033\n",
      "epoch: 908/1000 | train loss was: 0.053965 | val loss was: 0.503297\n",
      "epoch: 909/1000 | train loss was: 0.053952 | val loss was: 0.503293\n",
      "epoch: 910/1000 | train loss was: 0.053939 | val loss was: 0.50329\n",
      "epoch: 911/1000 | train loss was: 0.053927 | val loss was: 0.503287\n",
      "epoch: 912/1000 | train loss was: 0.053914 | val loss was: 0.503283\n",
      "epoch: 913/1000 | train loss was: 0.053901 | val loss was: 0.50328\n",
      "epoch: 914/1000 | train loss was: 0.053889 | val loss was: 0.503277\n",
      "epoch: 915/1000 | train loss was: 0.053876 | val loss was: 0.503274\n",
      "epoch: 916/1000 | train loss was: 0.053864 | val loss was: 0.503271\n",
      "epoch: 917/1000 | train loss was: 0.053851 | val loss was: 0.503268\n",
      "epoch: 918/1000 | train loss was: 0.053839 | val loss was: 0.503265\n",
      "epoch: 919/1000 | train loss was: 0.053827 | val loss was: 0.503263\n",
      "epoch: 920/1000 | train loss was: 0.053814 | val loss was: 0.50326\n",
      "epoch: 921/1000 | train loss was: 0.053802 | val loss was: 0.503257\n",
      "epoch: 922/1000 | train loss was: 0.05379 | val loss was: 0.503255\n",
      "epoch: 923/1000 | train loss was: 0.053777 | val loss was: 0.503252\n",
      "epoch: 924/1000 | train loss was: 0.053765 | val loss was: 0.503249\n",
      "epoch: 925/1000 | train loss was: 0.053753 | val loss was: 0.503247\n",
      "epoch: 926/1000 | train loss was: 0.053741 | val loss was: 0.503244\n",
      "epoch: 927/1000 | train loss was: 0.053729 | val loss was: 0.503242\n",
      "epoch: 928/1000 | train loss was: 0.053717 | val loss was: 0.50324\n",
      "epoch: 929/1000 | train loss was: 0.053705 | val loss was: 0.503237\n",
      "epoch: 930/1000 | train loss was: 0.053692 | val loss was: 0.503235\n",
      "epoch: 931/1000 | train loss was: 0.05368 | val loss was: 0.503233\n",
      "epoch: 932/1000 | train loss was: 0.053669 | val loss was: 0.503231\n",
      "epoch: 933/1000 | train loss was: 0.053657 | val loss was: 0.503229\n",
      "epoch: 934/1000 | train loss was: 0.053645 | val loss was: 0.503227\n",
      "epoch: 935/1000 | train loss was: 0.053633 | val loss was: 0.503225\n",
      "epoch: 936/1000 | train loss was: 0.053621 | val loss was: 0.503223\n",
      "epoch: 937/1000 | train loss was: 0.053609 | val loss was: 0.503221\n",
      "epoch: 938/1000 | train loss was: 0.053597 | val loss was: 0.503219\n",
      "epoch: 939/1000 | train loss was: 0.053586 | val loss was: 0.503217\n",
      "epoch: 940/1000 | train loss was: 0.053574 | val loss was: 0.503216\n",
      "epoch: 941/1000 | train loss was: 0.053562 | val loss was: 0.503214\n",
      "epoch: 942/1000 | train loss was: 0.053551 | val loss was: 0.503212\n",
      "epoch: 943/1000 | train loss was: 0.053539 | val loss was: 0.503211\n",
      "epoch: 944/1000 | train loss was: 0.053527 | val loss was: 0.503209\n",
      "epoch: 945/1000 | train loss was: 0.053516 | val loss was: 0.503207\n",
      "epoch: 946/1000 | train loss was: 0.053504 | val loss was: 0.503206\n",
      "epoch: 947/1000 | train loss was: 0.053493 | val loss was: 0.503205\n",
      "epoch: 948/1000 | train loss was: 0.053481 | val loss was: 0.503203\n",
      "epoch: 949/1000 | train loss was: 0.05347 | val loss was: 0.503202\n",
      "epoch: 950/1000 | train loss was: 0.053458 | val loss was: 0.503201\n",
      "epoch: 951/1000 | train loss was: 0.053447 | val loss was: 0.503199\n",
      "epoch: 952/1000 | train loss was: 0.053436 | val loss was: 0.503198\n",
      "epoch: 953/1000 | train loss was: 0.053424 | val loss was: 0.503197\n",
      "epoch: 954/1000 | train loss was: 0.053413 | val loss was: 0.503196\n",
      "epoch: 955/1000 | train loss was: 0.053402 | val loss was: 0.503195\n",
      "epoch: 956/1000 | train loss was: 0.05339 | val loss was: 0.503194\n",
      "epoch: 957/1000 | train loss was: 0.053379 | val loss was: 0.503192\n",
      "epoch: 958/1000 | train loss was: 0.053368 | val loss was: 0.503191\n",
      "epoch: 959/1000 | train loss was: 0.053357 | val loss was: 0.50319\n",
      "epoch: 960/1000 | train loss was: 0.053345 | val loss was: 0.503189\n",
      "epoch: 961/1000 | train loss was: 0.053334 | val loss was: 0.503188\n",
      "epoch: 962/1000 | train loss was: 0.053323 | val loss was: 0.503187\n",
      "epoch: 963/1000 | train loss was: 0.053312 | val loss was: 0.503186\n",
      "epoch: 964/1000 | train loss was: 0.053301 | val loss was: 0.503185\n",
      "epoch: 965/1000 | train loss was: 0.053289 | val loss was: 0.503184\n",
      "epoch: 966/1000 | train loss was: 0.053278 | val loss was: 0.503183\n",
      "epoch: 967/1000 | train loss was: 0.053267 | val loss was: 0.503182\n",
      "epoch: 968/1000 | train loss was: 0.053256 | val loss was: 0.503181\n",
      "epoch: 969/1000 | train loss was: 0.053245 | val loss was: 0.50318\n",
      "epoch: 970/1000 | train loss was: 0.053234 | val loss was: 0.50318\n",
      "epoch: 971/1000 | train loss was: 0.053223 | val loss was: 0.503179\n",
      "epoch: 972/1000 | train loss was: 0.053213 | val loss was: 0.503178\n",
      "epoch: 973/1000 | train loss was: 0.053202 | val loss was: 0.503178\n",
      "epoch: 974/1000 | train loss was: 0.053191 | val loss was: 0.503177\n",
      "epoch: 975/1000 | train loss was: 0.05318 | val loss was: 0.503177\n",
      "epoch: 976/1000 | train loss was: 0.053169 | val loss was: 0.503176\n",
      "epoch: 977/1000 | train loss was: 0.053159 | val loss was: 0.503176\n",
      "epoch: 978/1000 | train loss was: 0.053148 | val loss was: 0.503176\n",
      "epoch: 979/1000 | train loss was: 0.053137 | val loss was: 0.503175\n",
      "epoch: 980/1000 | train loss was: 0.053126 | val loss was: 0.503175\n",
      "epoch: 981/1000 | train loss was: 0.053116 | val loss was: 0.503175\n",
      "epoch: 982/1000 | train loss was: 0.053105 | val loss was: 0.503175\n",
      "epoch: 983/1000 | train loss was: 0.053095 | val loss was: 0.503174\n",
      "epoch: 984/1000 | train loss was: 0.053084 | val loss was: 0.503174\n",
      "epoch: 985/1000 | train loss was: 0.053073 | val loss was: 0.503174\n",
      "epoch: 986/1000 | train loss was: 0.053063 | val loss was: 0.503174\n",
      "epoch: 987/1000 | train loss was: 0.053052 | val loss was: 0.503174\n",
      "epoch: 988/1000 | train loss was: 0.053042 | val loss was: 0.503174\n",
      "epoch: 989/1000 | train loss was: 0.053032 | val loss was: 0.503174\n",
      "epoch: 990/1000 | train loss was: 0.053021 | val loss was: 0.503174\n",
      "epoch: 991/1000 | train loss was: 0.053011 | val loss was: 0.503174\n",
      "epoch: 992/1000 | train loss was: 0.053 | val loss was: 0.503175\n",
      "epoch: 993/1000 | train loss was: 0.05299 | val loss was: 0.503175\n",
      "epoch: 994/1000 | train loss was: 0.05298 | val loss was: 0.503175\n",
      "epoch: 995/1000 | train loss was: 0.052969 | val loss was: 0.503175\n",
      "epoch: 996/1000 | train loss was: 0.052959 | val loss was: 0.503176\n",
      "epoch: 997/1000 | train loss was: 0.052949 | val loss was: 0.503176\n",
      "epoch: 998/1000 | train loss was: 0.052939 | val loss was: 0.503176\n",
      "epoch: 999/1000 | train loss was: 0.052929 | val loss was: 0.503177\n",
      "epoch: 1000/1000 | train loss was: 0.052918 | val loss was: 0.503177\n"
     ]
    }
   ],
   "source": [
    "# run the training with the hyperparameters you want, return losses for plotting:\n",
    "train_loss_history, val_loss_history = nn.train(x_train, y_train, x_val, y_val, epochs = 1000, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the mean training loss per sample as a function of the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting stuff:\n",
    "epochs = np.arange(1, nn.epochs + 1, 1).reshape(-1,1)\n",
    "\n",
    "# plot training & validation loss:\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "plt.plot(epochs, train_loss_history, label = 'training')\n",
    "plt.plot(epochs, val_loss_history, label = 'validation')\n",
    "plt.title('loss vs. epochs')\n",
    "plt.ylabel('average binary crossentropy loss per sample')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.xlim([1, nn.epochs])\n",
    "plt.grid('both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing the Model**:\n",
    "\n",
    "Now that the model has been sufficiently trained, we can test its accuracy on the test set of the data that was partitioned previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 0 | true: [0]\n",
      "predicted: 1 | true: [1]\n",
      "predicted: 0 | true: [0]\n",
      "accuracy of model is: 98.837\n"
     ]
    }
   ],
   "source": [
    "# take the model that is already trained and use it to predict the class of tumour based on data:\n",
    "predictions, targets, accuracy = nn.test(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate a confusion matrix:\n",
    "cm = confusion_matrix(targets, predictions)\n",
    "\n",
    "# plot the confusion matrix:\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
